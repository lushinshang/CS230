1
00:00:05,300 --> 00:00:10,730
I'm Kian Katanforoosh and I am
the co-creator and collector

2
00:00:10,730 --> 00:00:15,140
with Andrew for
this class, CS230.

3
00:00:15,140 --> 00:00:19,550
And I will teach about half
of the in-person lectures

4
00:00:19,550 --> 00:00:21,050
this quarter.

5
00:00:21,050 --> 00:00:24,300
Outside of Stanford,
I work in industry.

6
00:00:24,300 --> 00:00:28,820
I lead a company called Workera
which uses AI to measure skills.

7
00:00:28,820 --> 00:00:32,210
And with the history
of CS230 students that

8
00:00:32,210 --> 00:00:35,750
have started AI startups and
companies, what I try to do,

9
00:00:35,750 --> 00:00:38,490
usually, is to bring a lot
of examples from industry.

10
00:00:38,490 --> 00:00:42,080
So what you should expect
from these in-class lectures

11
00:00:42,080 --> 00:00:45,260
is not as much of the
academic side of things, which

12
00:00:45,260 --> 00:00:48,750
we learn anyway in
the online videos,

13
00:00:48,750 --> 00:00:53,550
but also the industry
specific input.

14
00:00:53,550 --> 00:00:58,970
And some of the topics that
we'll cover this year together

15
00:00:58,970 --> 00:01:02,820
includes decision-making
in AI projects,

16
00:01:02,820 --> 00:01:04,620
which we're going to see today.

17
00:01:04,620 --> 00:01:07,130
I want you to come out of
today's lecture feeling

18
00:01:07,130 --> 00:01:10,380
like you had some fun,
it was interactive.

19
00:01:10,380 --> 00:01:14,450
And also you have a better way
to make decisions in AI projects

20
00:01:14,450 --> 00:01:18,420
because you've seen how
deep-learning researchers,

21
00:01:18,420 --> 00:01:22,070
engineers, and scientists make
their decisions solving problems

22
00:01:22,070 --> 00:01:23,750
in industry.

23
00:01:23,750 --> 00:01:27,680
Other topics later in the
quarter for in classroom time

24
00:01:27,680 --> 00:01:32,340
include things like adversarial
attacks and defenses.

25
00:01:32,340 --> 00:01:34,940
We might have some
time to cover it today.

26
00:01:34,940 --> 00:01:36,680
Deep reinforcement
learning, which

27
00:01:36,680 --> 00:01:38,430
is really hot in the
market right now,

28
00:01:38,430 --> 00:01:40,820
and I think it's very
important to about it.

29
00:01:40,820 --> 00:01:43,350
And then all the stuff
that is very practical,

30
00:01:43,350 --> 00:01:46,860
like retrieval
augmented generation,

31
00:01:46,860 --> 00:01:49,260
AI agents, multi-agent system.

32
00:01:49,260 --> 00:01:52,130
As we go deeper into
the class, and you

33
00:01:52,130 --> 00:01:54,110
get the baggage of
neural networks,

34
00:01:54,110 --> 00:01:58,564
we'll be able to cover
even more fun topics.

35
00:01:58,564 --> 00:02:02,840
So today's lecture is
going to be structured

36
00:02:02,840 --> 00:02:06,080
in three parts,
maybe four depending

37
00:02:06,080 --> 00:02:07,980
on whether we have time.

38
00:02:07,980 --> 00:02:11,990
We'll start with a
little recap of the week.

39
00:02:11,990 --> 00:02:14,360
What you've learned online
about neurons and layers

40
00:02:14,360 --> 00:02:15,690
and deep neural networks.

41
00:02:15,690 --> 00:02:20,120
Then we'll get into a set of
supervised learning projects,

42
00:02:20,120 --> 00:02:25,200
including a day and night
simple vanilla classification,

43
00:02:25,200 --> 00:02:27,878
the trigger word detection,
which is actually

44
00:02:27,878 --> 00:02:30,170
a project you're going to
build at the end of the class

45
00:02:30,170 --> 00:02:31,520
yourself.

46
00:02:31,520 --> 00:02:33,920
And then face
verification, which

47
00:02:33,920 --> 00:02:36,770
we'll see also variation
of how face verification

48
00:02:36,770 --> 00:02:38,070
algorithms work.

49
00:02:38,070 --> 00:02:40,310
In the third
section, we'll focus

50
00:02:40,310 --> 00:02:43,518
on self-supervised learning
and weakly supervised learning.

51
00:02:43,518 --> 00:02:45,060
Don't worry if you
don't these terms.

52
00:02:45,060 --> 00:02:46,820
We're going to
learn them together.

53
00:02:46,820 --> 00:02:49,190
And we'll talk a
lot about embeddings

54
00:02:49,190 --> 00:02:54,730
because embeddings are the
connective tissue of many AI

55
00:02:54,730 --> 00:02:58,010
systems online today, and it's
important to know about them.

56
00:02:58,010 --> 00:02:59,890
And finally, if we
have time, we'll

57
00:02:59,890 --> 00:03:03,430
also talk about adversarial
attacks and defenses.

58
00:03:03,430 --> 00:03:06,920
With more and more AI
systems in the wild,

59
00:03:06,920 --> 00:03:09,605
knowing how to defend
them is very important,

60
00:03:09,605 --> 00:03:11,230
and knowing how to
attack them can also

61
00:03:11,230 --> 00:03:12,500
teach you how to defend them.

62
00:03:12,500 --> 00:03:14,980
So we'll cover that as well.

63
00:03:14,980 --> 00:03:16,840
Sounds good?

64
00:03:16,840 --> 00:03:19,460
Please interrupt me as we
go through the lecture.

65
00:03:19,460 --> 00:03:23,890
We want this to be very
conversational as much as

66
00:03:23,890 --> 00:03:24,590
possible.

67
00:03:24,590 --> 00:03:30,280
So recap of the
week is the core way

68
00:03:30,280 --> 00:03:35,830
that AI learns from data in a
traditional supervised learning

69
00:03:35,830 --> 00:03:36,740
setup.

70
00:03:36,740 --> 00:03:40,750
You can think of it as an
input, such as this little image

71
00:03:40,750 --> 00:03:44,240
of the confused
cat and an output,

72
00:03:44,240 --> 00:03:46,690
in this case, a
number between 0 and 1

73
00:03:46,690 --> 00:03:48,340
that represents the
chance that there

74
00:03:48,340 --> 00:03:51,970
might be a cat on the
picture 1 or there

75
00:03:51,970 --> 00:03:54,910
is no cat on the picture 0.

76
00:03:54,910 --> 00:03:57,460
What the model is, and
oftentimes you'll see

77
00:03:57,460 --> 00:03:59,180
me refer to the
model as two things.

78
00:03:59,180 --> 00:04:01,450
There's an architecture
which is essentially

79
00:04:01,450 --> 00:04:05,960
the blueprint of the model,
the skeleton and parameters.

80
00:04:05,960 --> 00:04:07,840
It might be a few
parameters, it might

81
00:04:07,840 --> 00:04:10,570
be billions of parameters
like the models

82
00:04:10,570 --> 00:04:13,455
that OpenAI, DeepMind
and others work on.

83
00:04:16,300 --> 00:04:18,519
Outside of that--

84
00:04:18,519 --> 00:04:21,640
And so when you think about
AI models being deployed

85
00:04:21,640 --> 00:04:23,380
in the wild, when you
think about what's

86
00:04:23,380 --> 00:04:27,250
happening with
ChatGPT, you can really

87
00:04:27,250 --> 00:04:31,210
come down to there's two files
somewhere on the cloud, one that

88
00:04:31,210 --> 00:04:33,400
describes the architecture
of the model, one

89
00:04:33,400 --> 00:04:34,900
that describes the
parameters that

90
00:04:34,900 --> 00:04:36,440
are part of this architecture.

91
00:04:36,440 --> 00:04:38,980
And you keep calling
to those two files,

92
00:04:38,980 --> 00:04:41,568
and you get your
inference or your output.

93
00:04:41,568 --> 00:04:43,610
That's really what's
happening behind the scenes.

94
00:04:43,610 --> 00:04:45,500
Much more complicated
than that, obviously.

95
00:04:45,500 --> 00:04:48,670
But those are the two critical
components of a neural network

96
00:04:48,670 --> 00:04:51,225
architecture, and its
parameters that are trained.

97
00:04:54,080 --> 00:04:55,940
How does the model learn?

98
00:04:55,940 --> 00:04:59,880
Is through a gradient
descent optimization.

99
00:04:59,880 --> 00:05:03,540
Meaning I send the picture
of the cat through the model,

100
00:05:03,540 --> 00:05:06,030
and the model, at the
beginning, is not trained,

101
00:05:06,030 --> 00:05:08,100
so it's probably wrong.

102
00:05:08,100 --> 00:05:11,310
It tells me I think there
is no cat, I think it's 0.

103
00:05:11,310 --> 00:05:13,820
And then I use something
called the loss function

104
00:05:13,820 --> 00:05:15,780
to compare the ground truth.

105
00:05:15,780 --> 00:05:18,740
There is a cat on the
picture with the prediction

106
00:05:18,740 --> 00:05:21,390
from the model at
this point in time.

107
00:05:21,390 --> 00:05:24,120
Those two numbers are
far from each other.

108
00:05:24,120 --> 00:05:28,320
That should be a penalty, which
the loss function describes.

109
00:05:28,320 --> 00:05:31,560
And then, in order to give
feedback to the parameters,

110
00:05:31,560 --> 00:05:34,170
we use this gradient
descent update.

111
00:05:34,170 --> 00:05:36,420
We do that many, many times.

112
00:05:36,420 --> 00:05:39,660
What it means is that we take
our parameters and we tell them,

113
00:05:39,660 --> 00:05:41,210
hey, you should go
a little bit more

114
00:05:41,210 --> 00:05:42,680
to the right or
a little bit more

115
00:05:42,680 --> 00:05:46,070
to the left until
that number, that

116
00:05:46,070 --> 00:05:49,200
is the prediction for the cat,
is closer to the ground truth.

117
00:05:49,200 --> 00:05:52,210
We do that with batches
of data, millions

118
00:05:52,210 --> 00:05:55,390
of images of cats and
images of anything

119
00:05:55,390 --> 00:06:00,250
else, and we give that feedback
repetitively to the model

120
00:06:00,250 --> 00:06:03,160
until the parameters
are calibrated

121
00:06:03,160 --> 00:06:06,910
and the model is, in fact,
finding the cat on this picture.

122
00:06:06,910 --> 00:06:08,082
Nothing new here.

123
00:06:08,082 --> 00:06:09,290
You've seen it in the videos.

124
00:06:09,290 --> 00:06:12,550
Any question on
that learning setup?

125
00:06:12,550 --> 00:06:13,900
No.

126
00:06:13,900 --> 00:06:15,470
OK, easy so far.

127
00:06:15,470 --> 00:06:17,630
There's many things that
can change in this setup,

128
00:06:17,630 --> 00:06:18,797
and you'll see in the class.

129
00:06:18,797 --> 00:06:20,240
First thing is the input.

130
00:06:20,240 --> 00:06:22,250
The input does not
have to be an image.

131
00:06:22,250 --> 00:06:26,050
It can be text like when
you chat with ChatGPT.

132
00:06:26,050 --> 00:06:28,790
It can be audio,
it can be video,

133
00:06:28,790 --> 00:06:32,600
it can be structured data, it
can be spreadsheets and numbers.

134
00:06:32,600 --> 00:06:34,990
Those we'll see a variety
of examples in the class

135
00:06:34,990 --> 00:06:38,920
and how it influences
the architecture.

136
00:06:38,920 --> 00:06:41,660
The output again doesn't
have to be 0 and 1.

137
00:06:41,660 --> 00:06:43,990
This is an example
of a classification.

138
00:06:43,990 --> 00:06:46,840
You could turn this
problem into a regression.

139
00:06:46,840 --> 00:06:50,010
For example, if I was asking
you, what's the age of the cat,

140
00:06:50,010 --> 00:06:51,580
estimate the age of the cat.

141
00:06:51,580 --> 00:06:54,780
That would be a regression
task, not a classification task

142
00:06:54,780 --> 00:06:55,480
anymore.

143
00:06:55,480 --> 00:06:58,450
Later in the class, we'll
also see generative tasks.

144
00:06:58,450 --> 00:07:01,500
In fact, lecture 4 is going
to focus on diffusion models

145
00:07:01,500 --> 00:07:05,220
generative adversarial networks
where the output, actually,

146
00:07:05,220 --> 00:07:08,010
is much bigger than
the input, typically.

147
00:07:08,010 --> 00:07:12,060
So you can have a low resolution
of a cat as input and the output

148
00:07:12,060 --> 00:07:14,230
is a high-resolution
of the same cat.

149
00:07:14,230 --> 00:07:16,590
The output is bigger
than the input, which can

150
00:07:16,590 --> 00:07:19,590
be counterintuitive to people.

151
00:07:19,590 --> 00:07:22,800
Other things that can change
include the architecture.

152
00:07:22,800 --> 00:07:27,060
You've learned about the
vanilla multilayer perceptron

153
00:07:27,060 --> 00:07:28,930
or the fully connected
neural network.

154
00:07:28,930 --> 00:07:31,890
That's what we're learning
right now online together.

155
00:07:31,890 --> 00:07:35,130
By the end of the class,
you'll have many architectures

156
00:07:35,130 --> 00:07:37,860
that you'll be
familiar with from RNNs

157
00:07:37,860 --> 00:07:42,700
and convolutional neural
networks, transformer models.

158
00:07:42,700 --> 00:07:44,650
All of these, at
the, end of the day,

159
00:07:44,650 --> 00:07:48,420
use the basis neural network
that you're learning right now.

160
00:07:48,420 --> 00:07:52,470
They're just stacked on top
of each other differently.

161
00:07:52,470 --> 00:07:55,860
The loss function is actually
a big focus of today's class

162
00:07:55,860 --> 00:07:58,210
and of the class in general.

163
00:07:58,210 --> 00:08:02,820
The loss function, which is what
gives the feedback to the model,

164
00:08:02,820 --> 00:08:06,420
you were right or you were
wrong, and what to do about it

165
00:08:06,420 --> 00:08:07,390
is an art.

166
00:08:07,390 --> 00:08:09,030
Designing good loss functions.

167
00:08:09,030 --> 00:08:12,150
Great deep learning
researchers are

168
00:08:12,150 --> 00:08:15,250
very creative when it comes
to designing loss functions.

169
00:08:15,250 --> 00:08:19,090
And in fact, when we built
the algorithm called YOLO,

170
00:08:19,090 --> 00:08:23,260
it is called YOLO for
you only look once,

171
00:08:23,260 --> 00:08:24,460
not you only live once.

172
00:08:24,460 --> 00:08:29,370
But YOLO has a very, difficult
to understand at first loss

173
00:08:29,370 --> 00:08:30,010
function.

174
00:08:30,010 --> 00:08:32,039
And there's a reason
why the loss function

175
00:08:32,039 --> 00:08:33,100
was designed like that.

176
00:08:33,100 --> 00:08:34,627
So by the end of
this class, you'll

177
00:08:34,627 --> 00:08:36,210
also have a better
intuition on how do

178
00:08:36,210 --> 00:08:40,230
we design great loss functions.

179
00:08:40,230 --> 00:08:42,360
Other things I'm not
going to cover right now,

180
00:08:42,360 --> 00:08:45,900
the activation functions in your
neural network, the optimizer

181
00:08:45,900 --> 00:08:48,360
that you use for your gradient
descent loop, and then the

182
00:08:48,360 --> 00:08:50,580
hyperparameters that
might come in when

183
00:08:50,580 --> 00:08:52,090
you train your algorithms.

184
00:08:52,090 --> 00:08:54,630
OK nothing new here.

185
00:08:54,630 --> 00:08:56,200
This is the basic setup.

186
00:08:56,200 --> 00:08:59,640
You've also learned
this week about neurons.

187
00:08:59,640 --> 00:09:01,440
The easiest way to
think about a neuron

188
00:09:01,440 --> 00:09:05,170
is the classic logistic
regression algorithm.

189
00:09:05,170 --> 00:09:07,240
Where I'm taking the
image of the cat.

190
00:09:07,240 --> 00:09:11,700
So an image in computer science
with the way the machine reads

191
00:09:11,700 --> 00:09:13,360
it is three channels.

192
00:09:13,360 --> 00:09:16,620
RGB for the three
colors red, green, blue.

193
00:09:16,620 --> 00:09:18,430
And we take all these numbers.

194
00:09:18,430 --> 00:09:20,020
We put them in a vector.

195
00:09:20,020 --> 00:09:22,720
The vector is then
fed into a neuron.

196
00:09:22,720 --> 00:09:25,380
And the neuron has two
components the linear part

197
00:09:25,380 --> 00:09:28,620
W transpose X plus B.
W being the weights

198
00:09:28,620 --> 00:09:29,950
and B being the bias.

199
00:09:29,950 --> 00:09:32,710
And then an activation
function, in this case,

200
00:09:32,710 --> 00:09:36,000
the sigmoid function, which
is very handy because it takes

201
00:09:36,000 --> 00:09:38,670
any number, and it
puts it between 0 and 1

202
00:09:38,670 --> 00:09:42,240
so that the output can
look like a probability.

203
00:09:42,240 --> 00:09:43,920
Classic setup.

204
00:09:43,920 --> 00:09:49,360
And here the probability
is 0.73 which is above 0.5.

205
00:09:49,360 --> 00:09:51,900
Which tells me the model thinks
there's a cat on the picture

206
00:09:51,900 --> 00:09:55,410
because 1 is a cat 0 is no cat.

207
00:09:55,410 --> 00:09:58,060
So question for
you to get started.

208
00:09:58,060 --> 00:10:02,880
How would you modify this
binary classification

209
00:10:02,880 --> 00:10:05,730
that detects cats
in an algorithm that

210
00:10:05,730 --> 00:10:07,810
would be able to detect
multiple animals,

211
00:10:07,810 --> 00:10:11,370
such as a cat, a
dog, and a giraffe?

212
00:10:11,370 --> 00:10:14,980
What do you need to change
about this neural network?

213
00:10:14,980 --> 00:10:15,480
Yeah.

214
00:10:22,410 --> 00:10:24,690
OK, so you would
change the output layer

215
00:10:24,690 --> 00:10:27,970
to match to the number of
animals you want to detect.

216
00:10:27,970 --> 00:10:29,370
Yeah, correct?

217
00:10:29,370 --> 00:10:30,940
Anyone wants to
add anything else?

218
00:10:30,940 --> 00:10:31,440
Yeah.

219
00:10:31,440 --> 00:10:32,607
The data that goes in there.

220
00:10:32,607 --> 00:10:33,820
The data that goes in there.

221
00:10:33,820 --> 00:10:35,200
How would you change the data?

222
00:10:35,200 --> 00:10:37,560
It has to be for
the animal data.

223
00:10:37,560 --> 00:10:38,800
OK very good.

224
00:10:38,800 --> 00:10:41,300
Yeah, you need data
from dogs and giraffes

225
00:10:41,300 --> 00:10:44,870
and also maybe
nature in general.

226
00:10:44,870 --> 00:10:47,130
What else do we
need not to forget?

227
00:10:47,130 --> 00:10:48,710
Yeah.

228
00:10:48,710 --> 00:10:51,500
Maybe you could add a
neuron for each node,

229
00:10:51,500 --> 00:10:53,430
and then your prediction
would be whichever

230
00:10:53,430 --> 00:10:54,450
output is the highest.

231
00:10:54,450 --> 00:10:55,260
Yeah, OK.

232
00:10:55,260 --> 00:10:56,850
Add 1 neuron per animal.

233
00:10:56,850 --> 00:10:58,980
Those neurons will be
independent from each other.

234
00:10:58,980 --> 00:11:01,260
And each neuron would
focus on one animal.

235
00:11:01,260 --> 00:11:02,838
Yeah good point.

236
00:11:02,838 --> 00:11:04,380
It's actually what
we're going to do.

237
00:11:04,380 --> 00:11:07,680
So yes, I think your
suggestions were the right one.

238
00:11:07,680 --> 00:11:09,950
We could multiply
this output layer

239
00:11:09,950 --> 00:11:12,330
to have three neurons
instead of one.

240
00:11:12,330 --> 00:11:14,970
All of them, because it's a
fully connected neural net,

241
00:11:14,970 --> 00:11:18,810
see the entire pixels
flattened in the vector.

242
00:11:18,810 --> 00:11:21,480
And then each of them will
be focused on an animal.

243
00:11:21,480 --> 00:11:24,110
The number one mistake
that we see in projects

244
00:11:24,110 --> 00:11:29,070
is that people add more data,
but forget to adjust the labels.

245
00:11:29,070 --> 00:11:31,770
So how do the labels
need to be adjusted here?

246
00:11:31,770 --> 00:11:34,610
It's not any more 0 and 1.

247
00:11:34,610 --> 00:11:41,935
What type of labels do
we need to train this?

248
00:11:47,900 --> 00:11:49,175
Yeah.

249
00:11:49,175 --> 00:11:52,030
The number of the
number of range.

250
00:11:52,030 --> 00:11:53,870
You know how we call that?

251
00:11:53,870 --> 00:11:55,990
Or no.

252
00:11:55,990 --> 00:11:56,490
OK.

253
00:11:56,490 --> 00:11:57,330
So yeah.

254
00:11:57,330 --> 00:11:58,352
Yeah.

255
00:11:58,352 --> 00:11:59,960
Use vectors.

256
00:11:59,960 --> 00:12:00,790
Vectors, yeah.

257
00:12:00,790 --> 00:12:02,540
I think you're seeing
the same thing, but.

258
00:12:02,540 --> 00:12:07,140
Yeah So here you would use a
one hot vector or a multi-hot.

259
00:12:07,140 --> 00:12:13,110
One hot means you'll
have a vector of size 3.

260
00:12:13,110 --> 00:12:15,350
And if there is a
cat on the picture

261
00:12:15,350 --> 00:12:18,860
the label is going to be 0, 1
0 because the second neuron is

262
00:12:18,860 --> 00:12:21,470
going to be responsible
to detect cats.

263
00:12:21,470 --> 00:12:24,890
In fact, that would be
called a one hot vector.

264
00:12:24,890 --> 00:12:27,140
Oftentimes you'll have
multiple animals on the picture

265
00:12:27,140 --> 00:12:29,190
because cats and dogs
can appear together.

266
00:12:29,190 --> 00:12:30,805
Cats and giraffes less so.

267
00:12:30,805 --> 00:12:32,180
And dogs and
giraffes, I've never

268
00:12:32,180 --> 00:12:34,370
seen one in the same
picture, but anyway.

269
00:12:34,370 --> 00:12:36,140
You'll have a
multi-hot vector if you

270
00:12:36,140 --> 00:12:38,160
have a cat and a
dog on the picture,

271
00:12:38,160 --> 00:12:40,928
you'll probably label it as 110.

272
00:12:40,928 --> 00:12:42,470
And the reason I'm
mentioning that it

273
00:12:42,470 --> 00:12:44,480
may sound silly, but in
a lot of projects people

274
00:12:44,480 --> 00:12:45,300
change their data.

275
00:12:45,300 --> 00:12:46,758
They forget to
change their labels,

276
00:12:46,758 --> 00:12:50,591
and then they wonder
why it doesn't work.

277
00:12:50,591 --> 00:12:51,570
OK, cool.

278
00:12:51,570 --> 00:12:54,350
Now in the class, we
use a specific notation

279
00:12:54,350 --> 00:12:56,370
with a superscript
and subscript.

280
00:12:56,370 --> 00:13:02,600
So when you see me refer
to something like a 1 1,

281
00:13:02,600 --> 00:13:06,020
the superscript
in square brackets

282
00:13:06,020 --> 00:13:08,100
indicates the layer
that you're in.

283
00:13:08,100 --> 00:13:09,950
So you're in the first layer.

284
00:13:09,950 --> 00:13:15,200
The subscript refers to the
index of the neuron, OK.

285
00:13:15,200 --> 00:13:18,930
And a is for activation.

286
00:13:18,930 --> 00:13:23,600
So a subscript three
square bracket 1

287
00:13:23,600 --> 00:13:29,510
is the output of the third
neuron of the first layer, OK.

288
00:13:29,510 --> 00:13:32,850
Again if I continue second layer
would be written like that.

289
00:13:32,850 --> 00:13:34,710
And then you'll get
your probability.

290
00:13:34,710 --> 00:13:38,700
The deeper the network is,
the more capacity it has.

291
00:13:38,700 --> 00:13:41,090
This is the word
we use, capacity.

292
00:13:41,090 --> 00:13:45,290
What it means is that if you
send a million pictures of cats

293
00:13:45,290 --> 00:13:49,350
and a million pictures of non
cats to a shallow network,

294
00:13:49,350 --> 00:13:52,140
it might not have the capacity
to learn what's in the data set.

295
00:13:52,140 --> 00:13:53,940
It's just not flexible enough.

296
00:13:53,940 --> 00:13:56,640
The deeper the network,
the more capacity it has.

297
00:13:56,640 --> 00:13:58,770
So in fact, the network
that is super deep.

298
00:13:58,770 --> 00:14:01,520
Imagine a billion
parameters transformer model

299
00:14:01,520 --> 00:14:04,290
with one million pictures
of cat and non-cat,

300
00:14:04,290 --> 00:14:06,513
it will just overfit
to those pictures.

301
00:14:06,513 --> 00:14:08,430
Meaning it's not going
to learn what a cat is,

302
00:14:08,430 --> 00:14:10,430
it's just going to learn
by heart those million

303
00:14:10,430 --> 00:14:14,060
pictures because its capacity
is way bigger than the data set

304
00:14:14,060 --> 00:14:14,730
it's fed.

305
00:14:14,730 --> 00:14:17,150
So it's very important
to understand the amount

306
00:14:17,150 --> 00:14:18,570
of data you're going to feed.

307
00:14:18,570 --> 00:14:20,930
And the complexity,
diversity of that data

308
00:14:20,930 --> 00:14:23,180
will probably
dictate the capacity

309
00:14:23,180 --> 00:14:24,595
of the models you need to use.

310
00:14:27,945 --> 00:14:30,320
Now, just to give you a little
bit more intuition on what

311
00:14:30,320 --> 00:14:32,740
happens inside those
neural networks.

312
00:14:32,740 --> 00:14:34,960
We take these relatively
shallow network,

313
00:14:34,960 --> 00:14:36,990
but call it three layers.

314
00:14:36,990 --> 00:14:42,840
And we train it on a data
set of facial images.

315
00:14:42,840 --> 00:14:43,890
Ignore the task.

316
00:14:43,890 --> 00:14:47,410
In face data sets, there's
a lot of tasks you could do.

317
00:14:47,410 --> 00:14:48,940
You could do a
face verification,

318
00:14:48,940 --> 00:14:54,040
you could do a face recognition,
you could do face clustering,

319
00:14:54,040 --> 00:14:54,880
things like that.

320
00:14:54,880 --> 00:14:56,437
We'll talk about that later.

321
00:14:56,437 --> 00:14:58,020
But let's say it's
been trained really

322
00:14:58,020 --> 00:15:00,810
well on understanding faces.

323
00:15:00,810 --> 00:15:04,800
If you now unpack this network
and you query each neuron

324
00:15:04,800 --> 00:15:08,070
and look what's going on
inside, what you'll notice

325
00:15:08,070 --> 00:15:11,550
is that the first
layers are going

326
00:15:11,550 --> 00:15:15,610
to be better at encoding
low-complexity features,

327
00:15:15,610 --> 00:15:19,740
while the deeper networks are
going to be better at encoding

328
00:15:19,740 --> 00:15:21,490
higher complexity features.

329
00:15:21,490 --> 00:15:22,720
So here's how it goes.

330
00:15:22,720 --> 00:15:25,110
Nothing too complicated for now.

331
00:15:25,110 --> 00:15:27,930
The neuron in the
first layers, they're

332
00:15:27,930 --> 00:15:30,370
looking at pixels because
you're giving them him

333
00:15:30,370 --> 00:15:31,820
directly the pixels.

334
00:15:31,820 --> 00:15:34,893
So they're going to be good at
stitching those pixels together.

335
00:15:34,893 --> 00:15:36,310
And maybe the first
neuron will be

336
00:15:36,310 --> 00:15:40,600
good at detecting diagonal
edge, the second neuron

337
00:15:40,600 --> 00:15:43,510
will be good at vertical
edges, and the third one

338
00:15:43,510 --> 00:15:46,360
at horizontal edges, because
they're just looking at pixels

339
00:15:46,360 --> 00:15:48,740
and trying to make
sense out of them.

340
00:15:48,740 --> 00:15:51,710
Now you go one layer deeper
in the middle of the network.

341
00:15:51,710 --> 00:15:53,500
Those are not seeing
pixels, they're

342
00:15:53,500 --> 00:15:55,960
seeing the output of the
first layer, which is already

343
00:15:55,960 --> 00:15:57,530
slightly more complex.

344
00:15:57,530 --> 00:15:59,500
So what you can
expect the layers

345
00:15:59,500 --> 00:16:02,140
in the middle of the
network to give you

346
00:16:02,140 --> 00:16:04,700
or to activate for is
higher level features,

347
00:16:04,700 --> 00:16:07,550
like an eye or a nose or an ear.

348
00:16:07,550 --> 00:16:09,830
Because it turns out if
you have a few edges,

349
00:16:09,830 --> 00:16:11,780
you can start detecting circles.

350
00:16:11,780 --> 00:16:13,540
And so you would
see a neuron that

351
00:16:13,540 --> 00:16:17,090
is really good at
detecting circles, eyes.

352
00:16:17,090 --> 00:16:18,680
The deeper you go
in the network,

353
00:16:18,680 --> 00:16:22,370
the more you get closer to the
task itself, which in this case,

354
00:16:22,370 --> 00:16:24,080
facial analysis.

355
00:16:24,080 --> 00:16:27,160
Let's say you would see
the last few neurons,

356
00:16:27,160 --> 00:16:29,530
detect larger
features of the face.

357
00:16:29,530 --> 00:16:32,830
Because again, they're seeing
higher complexity information.

358
00:16:32,830 --> 00:16:34,840
Does that make sense?

359
00:16:34,840 --> 00:16:37,220
This concept we
call it encoding,

360
00:16:37,220 --> 00:16:39,110
we'll also talk
about embeddings.

361
00:16:39,110 --> 00:16:43,540
It's very important because
when you train neural networks,

362
00:16:43,540 --> 00:16:45,250
you want to make sure
first that they're

363
00:16:45,250 --> 00:16:49,020
understanding what they're doing
and that's one way to see it.

364
00:16:49,020 --> 00:16:51,520
We'll have an entire lecture
on interpreting and visualizing

365
00:16:51,520 --> 00:16:54,190
neural networks
later this quarter.

366
00:16:54,190 --> 00:16:58,150
And on top of that
you probably can

367
00:16:58,150 --> 00:17:02,210
make use of some of those
encodings and embeddings.

368
00:17:02,210 --> 00:17:03,440
We'll see that later.

369
00:17:03,440 --> 00:17:08,050
But why in the vector space the
distances between those concepts

370
00:17:08,050 --> 00:17:09,079
are important.

371
00:17:09,079 --> 00:17:12,910
You can already imagine that
for tasks like search searching

372
00:17:12,910 --> 00:17:16,150
in a database, having a
neural network able to encode

373
00:17:16,150 --> 00:17:18,280
information in a
very meaningful way,

374
00:17:18,280 --> 00:17:20,290
can allow you to
find concepts that

375
00:17:20,290 --> 00:17:23,922
are close to each other and
associate them with each other,

376
00:17:23,922 --> 00:17:25,630
and concepts that are
far from each other

377
00:17:25,630 --> 00:17:29,170
and dissociate them
from each other.

378
00:17:29,170 --> 00:17:29,840
OK.

379
00:17:29,840 --> 00:17:32,530
So this was the
warm up for today.

380
00:17:32,530 --> 00:17:33,530
How much time we spent.

381
00:17:33,530 --> 00:17:34,910
OK, 15 minutes in the warm up.

382
00:17:34,910 --> 00:17:35,420
That's good.

383
00:17:35,420 --> 00:17:37,250
So we learned a few new words.

384
00:17:37,250 --> 00:17:40,730
Model, architecture, parameters.

385
00:17:40,730 --> 00:17:42,760
I didn't talk about it,
but feature engineering

386
00:17:42,760 --> 00:17:46,030
versus feature learning,
that's the core concept

387
00:17:46,030 --> 00:17:48,148
in deep learning is
feature engineering

388
00:17:48,148 --> 00:17:49,940
is what we used to do
before deep learning.

389
00:17:49,940 --> 00:17:52,810
Which is you might actually
build an algorithm that

390
00:17:52,810 --> 00:17:54,230
is good at detecting eyes.

391
00:17:54,230 --> 00:17:55,910
It's just a good
at scanning eyes.

392
00:17:55,910 --> 00:17:57,850
You manually build it.

393
00:17:57,850 --> 00:17:59,440
And then you build
another one that

394
00:17:59,440 --> 00:18:02,240
is good at detecting a mouth.

395
00:18:02,240 --> 00:18:05,510
And then you put them
together to detect faces.

396
00:18:05,510 --> 00:18:06,680
We don't do that anymore.

397
00:18:06,680 --> 00:18:08,060
We do end-to-end learning.

398
00:18:08,060 --> 00:18:10,850
Meaning we let the data speak
for itself and train the model.

399
00:18:10,850 --> 00:18:12,440
This is called feature learning.

400
00:18:12,440 --> 00:18:15,520
It's automatic, and that's how
the neural network actually

401
00:18:15,520 --> 00:18:19,390
learns those features without
you needing to tell it eyes

402
00:18:19,390 --> 00:18:20,990
are important to detect faces.

403
00:18:20,990 --> 00:18:23,800
You don't need to do that.

404
00:18:23,800 --> 00:18:25,000
Encoding and embedding.

405
00:18:25,000 --> 00:18:29,050
The real difference is encoding
is any vector representation

406
00:18:29,050 --> 00:18:31,610
and an embedding,
though-- sorry,

407
00:18:31,610 --> 00:18:33,610
it should have been--
an embedding is

408
00:18:33,610 --> 00:18:35,920
when an encoding has meaning.

409
00:18:35,920 --> 00:18:40,638
meaning The distance between
two encodings has meaning.

410
00:18:40,638 --> 00:18:42,430
They might be close or
far from each other,

411
00:18:42,430 --> 00:18:43,610
and it tells you something.

412
00:18:43,610 --> 00:18:46,090
There is a logic to it.

413
00:18:46,090 --> 00:18:50,710
And then we talked about one
hot and multi-hot vectors.

414
00:18:50,710 --> 00:18:52,070
So end of the recap.

415
00:18:52,070 --> 00:18:54,830
Now let's go into
supervised learning projects

416
00:18:54,830 --> 00:18:57,400
and we're going to make
decisions together and walk

417
00:18:57,400 --> 00:18:57,920
through it.

418
00:18:57,920 --> 00:19:01,880
The first case study is day
and night classification.

419
00:19:01,880 --> 00:19:04,210
So here's my problem for you.

420
00:19:04,210 --> 00:19:08,620
Given an image that I
give you, classify it

421
00:19:08,620 --> 00:19:12,440
as whether it's the day
or whether it's the night.

422
00:19:12,440 --> 00:19:14,410
OK, open ended problem.

423
00:19:14,410 --> 00:19:18,410
Ignore that foundation
models exist.

424
00:19:18,410 --> 00:19:21,410
You don't have access to
ChatGPT or Claude or whatever.

425
00:19:21,410 --> 00:19:22,840
The point is to
get under the hood

426
00:19:22,840 --> 00:19:24,882
and have those discussions
because obviously it's

427
00:19:24,882 --> 00:19:25,720
a toy example.

428
00:19:25,720 --> 00:19:27,850
So what do you do?

429
00:19:27,850 --> 00:19:31,985
What data do you want to
collect to solve this problem?

430
00:19:35,040 --> 00:19:37,020
Start yes.

431
00:19:37,020 --> 00:19:42,570
Check on the grid pixels
in the same rows, column.

432
00:19:42,570 --> 00:19:43,690
So tell me more.

433
00:19:43,690 --> 00:19:46,740
Check how pixels in
the same row are?

434
00:19:46,740 --> 00:19:47,860
Different rows.

435
00:19:47,860 --> 00:19:52,450
Just see how they are different.

436
00:19:52,450 --> 00:19:54,960
And if they are
very different, it's

437
00:19:54,960 --> 00:19:57,390
a possibility that [INAUDIBLE].

438
00:19:57,390 --> 00:19:59,280
What does it tell
you that if you

439
00:19:59,280 --> 00:20:01,447
look at a row of pixels
and the row next to it,

440
00:20:01,447 --> 00:20:02,530
if they're very different.

441
00:20:02,530 --> 00:20:04,410
What does it tell you?

442
00:20:04,410 --> 00:20:07,470
It has more colors
than just [INAUDIBLE].

443
00:20:07,470 --> 00:20:08,080
OK.

444
00:20:08,080 --> 00:20:10,140
So you say if the
delta between pixels

445
00:20:10,140 --> 00:20:15,190
that are close geographically
to each other is high,

446
00:20:15,190 --> 00:20:17,680
then there's probably colors--

447
00:20:17,680 --> 00:20:19,700
color changes so it's
day, most likely.

448
00:20:19,700 --> 00:20:20,200
Is that it?

449
00:20:20,200 --> 00:20:24,143
So that's an example
of feature engineering.

450
00:20:24,143 --> 00:20:25,810
It's like you're going
for it and great.

451
00:20:25,810 --> 00:20:26,970
You're going for it
and you're trying

452
00:20:26,970 --> 00:20:29,160
to understand what's a
pattern that tells me

453
00:20:29,160 --> 00:20:31,030
that a picture is day or night.

454
00:20:31,030 --> 00:20:36,336
What else can you do in the
world of neural networks.?

455
00:20:41,881 --> 00:20:43,980
Any other ideas?

456
00:20:43,980 --> 00:20:45,120
Yeah.

457
00:20:45,120 --> 00:20:46,660
Just feed in a
bunch of pictures.

458
00:20:46,660 --> 00:20:48,750
Half that are during
the day half du--

459
00:20:48,750 --> 00:20:49,270
OK, good.

460
00:20:49,270 --> 00:20:49,770
Yeah.

461
00:20:49,770 --> 00:20:51,960
So yeah, I agree.

462
00:20:51,960 --> 00:20:54,990
I said 10,000 images, but how
do you even determine how many

463
00:20:54,990 --> 00:20:59,075
pictures you need to get
started with this project?

464
00:21:02,422 --> 00:21:03,870
Feed some data.

465
00:21:03,870 --> 00:21:04,780
Feed some data.

466
00:21:04,780 --> 00:21:07,210
So you start with 10 pictures
and then you continue to go.

467
00:21:07,210 --> 00:21:07,710
Yeah.

468
00:21:07,710 --> 00:21:08,680
You could do that.

469
00:21:08,680 --> 00:21:10,090
Might take some time.

470
00:21:10,090 --> 00:21:13,540
I think the question is how
easy is it to collect that data.

471
00:21:13,540 --> 00:21:16,260
How would you collect that data?

472
00:21:16,260 --> 00:21:18,570
Probably from the
same location you

473
00:21:18,570 --> 00:21:20,880
could get pictures
from day and pictures.

474
00:21:20,880 --> 00:21:21,550
OK yeah.

475
00:21:21,550 --> 00:21:25,620
We could put our phone out there
and record the day and the night

476
00:21:25,620 --> 00:21:28,150
and have a stream of pictures
and add it to the data set.

477
00:21:28,150 --> 00:21:31,180
Same location, but
different lightings.

478
00:21:31,180 --> 00:21:32,430
Yea.

479
00:21:32,430 --> 00:21:36,520
And you mentioned that this
was a very hard problem,

480
00:21:36,520 --> 00:21:42,540
because in my opinion, also it
depends on what kind of model

481
00:21:42,540 --> 00:21:43,750
you want to operate.

482
00:21:43,750 --> 00:21:46,230
For example, if you
were building something

483
00:21:46,230 --> 00:21:49,210
for one location to set,
whether it's day or night,

484
00:21:49,210 --> 00:21:52,030
then that one location
would be really nice.

485
00:21:52,030 --> 00:21:58,870
But then if you were trying to
look at anywhere in the world,

486
00:21:58,870 --> 00:22:01,660
like in any kind of climate,
whether it's day or night,

487
00:22:01,660 --> 00:22:05,830
then you would have to have an
extremely diverse set of images.

488
00:22:05,830 --> 00:22:07,950
And because the
problem is so broad

489
00:22:07,950 --> 00:22:13,140
we would also need, so
many-- a huge amount of data.

490
00:22:13,140 --> 00:22:14,290
That's a great point.

491
00:22:14,290 --> 00:22:17,250
So just to repeat for
the people online,

492
00:22:17,250 --> 00:22:19,140
you have to define
the task first.

493
00:22:19,140 --> 00:22:21,070
Because the task can be easy.

494
00:22:21,070 --> 00:22:23,910
You can be in a park in
a very specific location

495
00:22:23,910 --> 00:22:26,020
and say, just detect
if it's day or night,

496
00:22:26,020 --> 00:22:29,710
or you can have a problem, which
is your camera can be anywhere.

497
00:22:29,710 --> 00:22:31,810
And that makes it
more complicated.

498
00:22:31,810 --> 00:22:33,330
And also the amount
of data you need

499
00:22:33,330 --> 00:22:36,160
is probably much more
for that second problem.

500
00:22:36,160 --> 00:22:37,440
That's what you said right.

501
00:22:37,440 --> 00:22:39,750
Now-- actually,
that's a great thread.

502
00:22:39,750 --> 00:22:42,000
Tell me about cases
where this problem would

503
00:22:42,000 --> 00:22:45,210
be really hard to solve?

504
00:22:45,210 --> 00:22:46,770
Yeah.

505
00:22:46,770 --> 00:22:49,380
Pictures of places like inside.

506
00:22:49,380 --> 00:22:50,370
Yeah, indoor.

507
00:22:50,370 --> 00:22:51,180
Indoor pictures.

508
00:22:51,180 --> 00:22:55,260
Actually, how could you tell
if you took a picture of me

509
00:22:55,260 --> 00:22:58,410
with the screen here
what time it was?

510
00:22:58,410 --> 00:22:59,590
You couldn't.

511
00:22:59,590 --> 00:23:00,640
You actually can.

512
00:23:00,640 --> 00:23:01,710
Yeah.

513
00:23:01,710 --> 00:23:03,870
There's a clock here.

514
00:23:03,870 --> 00:23:07,470
So that's very interesting
because if that

515
00:23:07,470 --> 00:23:10,900
was part of the task then
our problem would be hard.

516
00:23:10,900 --> 00:23:14,760
And 10,000 images is not going
to cut it to understand time.

517
00:23:14,760 --> 00:23:17,220
And so it's very important
to define the task very well.

518
00:23:17,220 --> 00:23:20,810
What else can be hard
other than indoor pictures?

519
00:23:20,810 --> 00:23:22,550
The clock could be AM/ PM.

520
00:23:22,550 --> 00:23:23,720
Also.

521
00:23:23,720 --> 00:23:25,650
Also the clock can be, AM or PM.

522
00:23:25,650 --> 00:23:29,540
But you can probably take
additional information, which

523
00:23:29,540 --> 00:23:33,440
is how people are dressed
and think that it might

524
00:23:33,440 --> 00:23:35,330
be warmer outside than colder.

525
00:23:35,330 --> 00:23:38,120
You could-- again, it
can be very complicated

526
00:23:38,120 --> 00:23:39,860
at the end of the
day, but a human

527
00:23:39,860 --> 00:23:43,080
would say someone is teaching,
students are in class.

528
00:23:43,080 --> 00:23:46,270
It's probably not 12:00 AM.

529
00:23:46,270 --> 00:23:48,890
Or so I mean, it
gets complicated.

530
00:23:48,890 --> 00:23:50,340
What else can be hard?

531
00:23:50,340 --> 00:23:52,190
Sunny versus cloudy.

532
00:23:52,190 --> 00:23:52,790
OK.

533
00:23:52,790 --> 00:23:53,730
Sunny, cloudy.

534
00:23:53,730 --> 00:23:54,230
Yeah.

535
00:23:54,230 --> 00:23:55,880
If you're in the
part of the world

536
00:23:55,880 --> 00:23:58,400
where the sun doesn't
set during the day.

537
00:23:58,400 --> 00:23:59,130
Great point.

538
00:23:59,130 --> 00:24:02,540
If you're in the North of
Norway right now or Sweden,

539
00:24:02,540 --> 00:24:05,760
even the clock can't tell you
probably if it's day or night.

540
00:24:05,760 --> 00:24:06,350
Yeah.

541
00:24:06,350 --> 00:24:07,200
Dawn and dusk.

542
00:24:07,200 --> 00:24:08,040
Dawn and dusk.

543
00:24:08,040 --> 00:24:09,060
Yeah, exactly.

544
00:24:09,060 --> 00:24:10,178
Those are great examples.

545
00:24:10,178 --> 00:24:11,720
Actually, this is
a good semantic one

546
00:24:11,720 --> 00:24:14,802
because you need also to define
exactly what's the definition

547
00:24:14,802 --> 00:24:15,510
of day and night.

548
00:24:15,510 --> 00:24:20,160
So long story short, the
problem can seem easy at first,

549
00:24:20,160 --> 00:24:22,175
it can be very complicated.

550
00:24:22,175 --> 00:24:24,300
And trust me, if you wanted
to do this really well,

551
00:24:24,300 --> 00:24:29,660
even the foundation models today
couldn't do it in certain cases.

552
00:24:29,660 --> 00:24:31,430
OK, let's say we
have 10,000 images,

553
00:24:31,430 --> 00:24:33,720
and you talked about the
split of images earlier.

554
00:24:33,720 --> 00:24:36,530
And I agree with you, you want
a mix of different situations

555
00:24:36,530 --> 00:24:39,020
in order to be able
to cover all of them.

556
00:24:39,020 --> 00:24:41,660
And going back to our
discussion on model capacity,

557
00:24:41,660 --> 00:24:44,540
if it's just a simple
problem, you probably

558
00:24:44,540 --> 00:24:46,560
need just a small
capacity model.

559
00:24:46,560 --> 00:24:48,450
If you want to add
all these edge cases,

560
00:24:48,450 --> 00:24:51,470
you probably are looking for
bigger-capacity models and more

561
00:24:51,470 --> 00:24:53,300
data.

562
00:24:53,300 --> 00:24:56,420
What's the input to our model?

563
00:24:56,420 --> 00:24:57,810
I think someone said it already.

564
00:24:57,810 --> 00:25:01,490
So let's say a picture of--

565
00:25:01,490 --> 00:25:05,280
a picture of day or
night or whatever.

566
00:25:05,280 --> 00:25:07,410
What's the resolution
we're going to work with?

567
00:25:07,410 --> 00:25:10,467
How do you determine resolution
when you build a data set,

568
00:25:10,467 --> 00:25:11,425
and why does it matter?

569
00:25:17,120 --> 00:25:17,900
Yes.

570
00:25:17,900 --> 00:25:23,090
Give a number of pixels that
would [INAUDIBLE] values

571
00:25:23,090 --> 00:25:25,880
and round them of to
be the same vector.

572
00:25:25,880 --> 00:25:27,877
So you want to choose
a resolution that's

573
00:25:27,877 --> 00:25:29,460
going to be the same
across the board.

574
00:25:29,460 --> 00:25:31,502
You're going to vectorize
everything that's going

575
00:25:31,502 --> 00:25:32,850
to fit in the same size matrix.

576
00:25:32,850 --> 00:25:33,500
Yeah.

577
00:25:33,500 --> 00:25:40,430
Why is it important to have
the right resolution let's say?

578
00:25:40,430 --> 00:25:43,460
It's going to have the
super resolution so

579
00:25:43,460 --> 00:25:45,960
that it can recognize patterns.

580
00:25:45,960 --> 00:25:48,020
If the picture is
the same to us.

581
00:25:48,020 --> 00:25:50,180
But it's in a
limited resolution.

582
00:25:50,180 --> 00:25:52,760
So you're saying
we want homogeneity

583
00:25:52,760 --> 00:25:54,605
in the data set in
terms of resolution.

584
00:25:54,605 --> 00:25:55,980
I would say that
could be a thing

585
00:25:55,980 --> 00:25:57,770
but, today, you
can actually write

586
00:25:57,770 --> 00:26:00,680
scripts that downsize high
resolution images before it

587
00:26:00,680 --> 00:26:01,950
gets in the model.

588
00:26:01,950 --> 00:26:05,130
And so it would probably solve--
upsizing is slightly harder.

589
00:26:05,130 --> 00:26:06,540
You will need another algorithm.

590
00:26:06,540 --> 00:26:09,120
But, it's OK to have
different resolutions,

591
00:26:09,120 --> 00:26:12,020
but you still want to what's the
target resolution that you're

592
00:26:12,020 --> 00:26:12,840
looking for.

593
00:26:12,840 --> 00:26:15,660
If you have too low resolution,
you lose some of your features.

594
00:26:15,660 --> 00:26:17,750
If you have too high resolution,
you have too much data

595
00:26:17,750 --> 00:26:19,500
and you have a lot of
weights to optimize.

596
00:26:19,500 --> 00:26:20,150
Exactly.

597
00:26:20,150 --> 00:26:20,910
Exactly.

598
00:26:20,910 --> 00:26:23,160
Low resolution, we
lack information,

599
00:26:23,160 --> 00:26:24,570
so we might get things wrong.

600
00:26:24,570 --> 00:26:27,890
For example, the clock might
not appear in the picture

601
00:26:27,890 --> 00:26:29,340
if it's too low resolution.

602
00:26:29,340 --> 00:26:31,680
High resolution means
more compute needed.

603
00:26:31,680 --> 00:26:33,920
If you have big
pictures, it's going

604
00:26:33,920 --> 00:26:36,050
to be heavier to
train your model

605
00:26:36,050 --> 00:26:37,620
and you're going to pay more.

606
00:26:37,620 --> 00:26:39,380
And also your
cycles of iterations

607
00:26:39,380 --> 00:26:40,460
are going to be longer.

608
00:26:40,460 --> 00:26:44,090
In an AI project, that's why
your resolution matters a lot.

609
00:26:44,090 --> 00:26:47,635
So my question is, what
resolution do we go for?

610
00:26:50,840 --> 00:26:53,660
Is there a way to
just get to a number

611
00:26:53,660 --> 00:26:56,060
really quickly in the next
10 minutes, let's say,

612
00:26:56,060 --> 00:26:57,355
if we were doing this project?

613
00:27:08,480 --> 00:27:08,980
Yeah.

614
00:27:08,980 --> 00:27:09,130
Who's--

615
00:27:09,130 --> 00:27:10,780
[INAUDIBLE] you
stick the image size

616
00:27:10,780 --> 00:27:13,360
and see whether you can detect.

617
00:27:13,360 --> 00:27:14,600
Yeah, exactly.

618
00:27:14,600 --> 00:27:16,070
I think that's a great idea.

619
00:27:16,070 --> 00:27:18,740
You're saying use
the human as a proxy.

620
00:27:18,740 --> 00:27:21,970
Yeah so actually,
this is how we did it.

621
00:27:21,970 --> 00:27:26,320
Back in the days we
would print pictures

622
00:27:26,320 --> 00:27:29,750
in different resolutions and run
it through our friends and say,

623
00:27:29,750 --> 00:27:32,030
can you tell if it's
the day or the night.

624
00:27:32,030 --> 00:27:34,280
And it turns out that
below a certain resolution,

625
00:27:34,280 --> 00:27:35,330
they can't anymore.

626
00:27:35,330 --> 00:27:37,790
It's just like, I don't
have the information I need.

627
00:27:37,790 --> 00:27:44,360
And where we ended is
somewhere around 64 by 64 by 3.

628
00:27:44,360 --> 00:27:48,460
And I stress the 3 because
later this quarter we'll

629
00:27:48,460 --> 00:27:50,930
see that OpenAI and DeepMind--

630
00:27:50,930 --> 00:27:53,530
DeepMind is [? well ?] in
reinforcement learning.

631
00:27:53,530 --> 00:27:56,650
It turns out for one of the
famous algorithms we learned

632
00:27:56,650 --> 00:28:01,000
together for reinforcement
learning discovered that you can

633
00:28:01,000 --> 00:28:04,310
actually remove colors, and
the model is not impacted,

634
00:28:04,310 --> 00:28:06,470
but your training
is way simpler.

635
00:28:06,470 --> 00:28:11,090
In this case, I think colors
matter because of the blue sky,

636
00:28:11,090 --> 00:28:14,920
because it does have an inherent
information about whether it's

637
00:28:14,920 --> 00:28:15,770
day or night.

638
00:28:15,770 --> 00:28:18,170
And it turns out the task,
if you give it to humans,

639
00:28:18,170 --> 00:28:21,760
is much harder without colors
than it is with colors.

640
00:28:21,760 --> 00:28:23,380
So those type of
insights we could

641
00:28:23,380 --> 00:28:25,540
get over the next 10
minutes, literally

642
00:28:25,540 --> 00:28:27,680
by using humans as a proxy.

643
00:28:27,680 --> 00:28:29,800
This is a toy
example, but I want

644
00:28:29,800 --> 00:28:31,700
you to think about that
in your AI project.

645
00:28:31,700 --> 00:28:33,117
You're going to
be at points where

646
00:28:33,117 --> 00:28:34,550
you want to validate
a hypothesis,

647
00:28:34,550 --> 00:28:37,060
and the best proxy
you'll have is the human.

648
00:28:37,060 --> 00:28:39,040
Yeah.

649
00:28:39,040 --> 00:28:41,670
OK, what's the output
for this model?

650
00:28:44,405 --> 00:28:45,340
Day or night.

651
00:28:45,340 --> 00:28:46,300
Yeah day or night.

652
00:28:46,300 --> 00:28:47,240
0 or 1.

653
00:28:47,240 --> 00:28:48,340
Yeah.

654
00:28:48,340 --> 00:28:49,750
The other question.

655
00:28:49,750 --> 00:28:53,020
Is there a relationship
on how many input you have

656
00:28:53,020 --> 00:28:56,770
and how many neurons you
have in the proxy in there.

657
00:28:56,770 --> 00:28:59,710
Do you just start
from and then you

658
00:28:59,710 --> 00:29:01,990
look at the number of their
relationship in seperate.

659
00:29:01,990 --> 00:29:02,960
Yeah good question.

660
00:29:02,960 --> 00:29:06,830
So does the size of the
image impact the input layer?

661
00:29:06,830 --> 00:29:08,780
The size of the input
layer, is that it?

662
00:29:08,780 --> 00:29:10,310
Yeah it doesn't.

663
00:29:10,310 --> 00:29:11,570
You can make your decisions.

664
00:29:11,570 --> 00:29:14,680
You can have three
neurons and you send

665
00:29:14,680 --> 00:29:16,810
a massive picture inside it.

666
00:29:16,810 --> 00:29:19,240
Oftentime, and
that's why I often

667
00:29:19,240 --> 00:29:21,380
say deep learning is
an engineering field.

668
00:29:21,380 --> 00:29:25,330
You have to try it or you
have to know the hacks.

669
00:29:25,330 --> 00:29:31,400
Typically the network-- in
a binary classification,

670
00:29:31,400 --> 00:29:34,210
you're going from a
high-dimensional input

671
00:29:34,210 --> 00:29:36,440
to a very low
dimensional output.

672
00:29:36,440 --> 00:29:38,770
So by nature, you'd imagine
your network is going

673
00:29:38,770 --> 00:29:41,080
to be like this, probably.

674
00:29:41,080 --> 00:29:44,020
Meaning you need
more edge detection

675
00:29:44,020 --> 00:29:46,520
at the beginning and then higher
feature and higher feature,

676
00:29:46,520 --> 00:29:47,960
until at the end it
detects the face,

677
00:29:47,960 --> 00:29:49,210
so it's going to be like that.

678
00:29:49,210 --> 00:29:50,980
We're going to see
examples in week four

679
00:29:50,980 --> 00:29:54,580
where the output is bigger
than the input and your network

680
00:29:54,580 --> 00:29:56,530
is probably like this.

681
00:29:56,530 --> 00:29:59,170
So you'll build intuition
during the class

682
00:29:59,170 --> 00:30:01,900
to when you want to
downsize your input layer

683
00:30:01,900 --> 00:30:03,950
or upsize your input layer.

684
00:30:03,950 --> 00:30:04,700
Yeah.

685
00:30:04,700 --> 00:30:07,050
And by the way, these
hacks are valuable.

686
00:30:07,050 --> 00:30:10,430
Why does meta go out
and give crazy offers

687
00:30:10,430 --> 00:30:11,700
to a few researchers?

688
00:30:11,700 --> 00:30:12,835
Because they know stuff.

689
00:30:12,835 --> 00:30:15,015
They know those
hacks, literally.

690
00:30:15,015 --> 00:30:15,515
OK.

691
00:30:18,620 --> 00:30:19,440
OK.

692
00:30:19,440 --> 00:30:22,380
So again, someone said it.

693
00:30:22,380 --> 00:30:24,270
The output is 0 or 1.

694
00:30:24,270 --> 00:30:26,490
The last activation is
going to be a sigmoid.

695
00:30:26,490 --> 00:30:27,600
Yes question.

696
00:30:27,600 --> 00:30:28,100
Yeah.

697
00:30:28,100 --> 00:30:31,410
So forcing yourself to
pick a specific resolution,

698
00:30:31,410 --> 00:30:34,160
you come up with a more general
model by taking any image

699
00:30:34,160 --> 00:30:35,010
and then sampling.

700
00:30:35,010 --> 00:30:35,510
Yeah.

701
00:30:35,510 --> 00:30:37,385
[INAUDIBLE]

702
00:30:38,660 --> 00:30:39,630
Yeah you can do that.

703
00:30:39,630 --> 00:30:42,560
You can probably say in the data
set, I don't care of the sizes

704
00:30:42,560 --> 00:30:44,850
because I'm scraping
data from everywhere.

705
00:30:44,850 --> 00:30:46,380
I'm collecting data on my phone.

706
00:30:46,380 --> 00:30:48,150
I'm putting it
all in a database.

707
00:30:48,150 --> 00:30:50,960
I write a script that
downsamples or upsamples

708
00:30:50,960 --> 00:30:52,680
everything to the
same resolution,

709
00:30:52,680 --> 00:30:55,920
and then I send those lower
res images in the network.

710
00:30:55,920 --> 00:30:58,650
Because what I care about is
the training of my network.

711
00:30:58,650 --> 00:31:00,620
I want it to be fast
and inefficient.

712
00:31:00,620 --> 00:31:01,400
You could do that.

713
00:31:01,400 --> 00:31:02,590
Yeah, for sure.

714
00:31:02,590 --> 00:31:03,267
Yeah.

715
00:31:03,267 --> 00:31:05,350
And there are networks
we're going to see in the--

716
00:31:05,350 --> 00:31:07,090
YOLO is an example
of a network where

717
00:31:07,090 --> 00:31:11,060
in the later versions of YOLO
it does that automatically.

718
00:31:11,060 --> 00:31:16,060
It can take any resolution and
it just samples it accordingly.

719
00:31:16,060 --> 00:31:18,740
OK last activation is
going to be sigmoid.

720
00:31:18,740 --> 00:31:20,810
We wanted the output
to be between 0 and 1.

721
00:31:20,810 --> 00:31:24,680
And the architecture most
likely a shallow network.

722
00:31:24,680 --> 00:31:26,830
We don't need too much
capacity unless the task

723
00:31:26,830 --> 00:31:27,880
is highly complex.

724
00:31:27,880 --> 00:31:30,470
And a convolution should
do the work really well.

725
00:31:30,470 --> 00:31:32,030
You don't what a
convolution is yet,

726
00:31:32,030 --> 00:31:33,530
you're going to
know in a few weeks,

727
00:31:33,530 --> 00:31:38,110
but those are known to be good
for sequences and for images.

728
00:31:38,110 --> 00:31:39,815
And finally, the loss function.

729
00:31:39,815 --> 00:31:41,190
What loss function
would you use?

730
00:31:44,920 --> 00:31:46,540
Uh.

731
00:31:46,540 --> 00:31:47,330
Which one?

732
00:31:47,330 --> 00:31:47,830
Sigmoid.

733
00:31:47,830 --> 00:31:48,620
Sigmoid.

734
00:31:48,620 --> 00:31:50,390
No, sigmoid is the activation.

735
00:31:50,390 --> 00:31:51,615
What's the loss function?

736
00:31:54,728 --> 00:31:55,630
Logistic loss.

737
00:31:55,630 --> 00:31:56,450
Logistic loss.

738
00:31:56,450 --> 00:32:00,150
Yeah, logistic loss also called
binary cross-entropy loss,

739
00:32:00,150 --> 00:32:03,660
which is the one you've seen
in the videos this week.

740
00:32:03,660 --> 00:32:05,040
Yeah.

741
00:32:05,040 --> 00:32:08,500
When you're like selecting your
images and your resolution,

742
00:32:08,500 --> 00:32:12,180
how much is designing that
around the hardware limitations

743
00:32:12,180 --> 00:32:12,700
matter?

744
00:32:12,700 --> 00:32:16,470
If you have certain
memory bandwidth,

745
00:32:16,470 --> 00:32:20,490
adder sizes, caching, is that
an issue or do you just let that

746
00:32:20,490 --> 00:32:21,060
come out raw.

747
00:32:21,060 --> 00:32:25,200
So repeating the question how
much the amount of hardware

748
00:32:25,200 --> 00:32:27,900
and the quality of the hardware
you have at your disposal

749
00:32:27,900 --> 00:32:29,650
influences those decisions?

750
00:32:29,650 --> 00:32:32,310
The answer is a lot.

751
00:32:32,310 --> 00:32:34,890
But, in fact, I'm
assuming you're

752
00:32:34,890 --> 00:32:38,800
training it on your laptop
right now, which will work.

753
00:32:38,800 --> 00:32:40,600
But you need to make
those trade offs.

754
00:32:40,600 --> 00:32:42,430
And I think that's why
those skills matter.

755
00:32:42,430 --> 00:32:45,370
Now, if the task is
more complicated,

756
00:32:45,370 --> 00:32:47,320
you look at the hardware
you have available,

757
00:32:47,320 --> 00:32:50,080
you'll make a quick back of
the envelope calculation.

758
00:32:50,080 --> 00:32:52,380
The calculation is
really about how fast

759
00:32:52,380 --> 00:32:54,100
our iteration cycles can be.

760
00:32:54,100 --> 00:32:56,770
It's not about necessarily
the performance of the model.

761
00:32:56,770 --> 00:32:59,770
In the end, you want to be
able to iterate super quickly.

762
00:32:59,770 --> 00:33:02,075
And if your model takes
one year to train,

763
00:33:02,075 --> 00:33:03,700
you're not going to
be able to iterate.

764
00:33:03,700 --> 00:33:05,440
You need to reduce it somehow.

765
00:33:05,440 --> 00:33:10,260
So there are situations that
we're going from 64 by 64 to 65

766
00:33:10,260 --> 00:33:13,760
by 65, will double your
compute time depending on--

767
00:33:13,760 --> 00:33:14,260
Yeah.

768
00:33:14,260 --> 00:33:16,230
Yeah, for sure.

769
00:33:16,230 --> 00:33:17,190
Yeah.

770
00:33:17,190 --> 00:33:21,180
Are the features of the
model, every scaler in the 64

771
00:33:21,180 --> 00:33:24,450
by 64 by 3 array?

772
00:33:24,450 --> 00:33:25,620
What do you mean?

773
00:33:25,620 --> 00:33:26,460
The--

774
00:33:26,460 --> 00:33:32,530
Like the features is every
single scalar value in this--

775
00:33:32,530 --> 00:33:36,540
So the 64 by 64 by 3 is like
literally, you look at a picture

776
00:33:36,540 --> 00:33:38,560
and you look at one pixel.

777
00:33:38,560 --> 00:33:41,050
These have three values
that describe the color.

778
00:33:41,050 --> 00:33:43,210
And then you just flatten it.

779
00:33:43,210 --> 00:33:45,330
And then those numbers
are given to the input

780
00:33:45,330 --> 00:33:46,930
layer of your neural network.

781
00:33:46,930 --> 00:33:48,060
[INAUDIBLE]

782
00:33:48,060 --> 00:33:50,280
Yeah right.

783
00:33:50,280 --> 00:33:53,740
OK let's move on just
for the sake of time.

784
00:33:53,740 --> 00:33:55,330
This was just the easy warm up.

785
00:33:55,330 --> 00:33:57,840
The two things I just want you
to remember from that project

786
00:33:57,840 --> 00:34:00,460
is we're going to build
a lot of proxy projects,

787
00:34:00,460 --> 00:34:03,082
and it's important to remember
them for your own projects.

788
00:34:03,082 --> 00:34:05,040
So you remember, oh, I
remember this experiment

789
00:34:05,040 --> 00:34:06,160
we did on humans.

790
00:34:06,160 --> 00:34:09,040
Or I remember how many images
we needed for that project.

791
00:34:09,040 --> 00:34:12,159
And then that can help
you make decisions faster.

792
00:34:12,159 --> 00:34:14,370
And then the other thing
that is worth highlighting

793
00:34:14,370 --> 00:34:16,556
is the human experiments.

794
00:34:16,556 --> 00:34:18,389
We're going to do a lot
of human experiments

795
00:34:18,389 --> 00:34:19,989
starting with the next example.

796
00:34:19,989 --> 00:34:22,710
And those are usually helpful
to make quick decisions

797
00:34:22,710 --> 00:34:25,000
in your project when
you're in the industry.

798
00:34:25,000 --> 00:34:27,580
So second project,
trigger word detection.

799
00:34:27,580 --> 00:34:29,820
Let me give some
context on this.

800
00:34:29,820 --> 00:34:32,040
The general problem--

801
00:34:32,040 --> 00:34:37,770
OK, you're familiar with
Alexa and all these--

802
00:34:37,770 --> 00:34:40,320
let's say Siri and
things like that that you

803
00:34:40,320 --> 00:34:42,843
might have in your
kitchen listening to you.

804
00:34:42,843 --> 00:34:43,510
Everybody knows.

805
00:34:43,510 --> 00:34:46,860
So the way these
networks typically

806
00:34:46,860 --> 00:34:49,840
work these models is
it's not a single model,

807
00:34:49,840 --> 00:34:54,730
it's a cascade of models for
energy and efficiency purposes.

808
00:34:54,730 --> 00:34:58,260
So for example, if you
have a virtual assistant

809
00:34:58,260 --> 00:35:02,230
in your kitchen, the first
model is activity detection.

810
00:35:02,230 --> 00:35:04,380
It just detects if
there is any volume.

811
00:35:04,380 --> 00:35:07,620
Because you don't want to
listen with the heavy model

812
00:35:07,620 --> 00:35:09,550
at all times, it just
uses a lot of energy.

813
00:35:09,550 --> 00:35:11,220
You want a very
lightweight model

814
00:35:11,220 --> 00:35:13,660
that understands when
volume is playing.

815
00:35:13,660 --> 00:35:16,810
And so let's say this
network detects volume,

816
00:35:16,810 --> 00:35:20,400
it calls another network that
is focused on the activation

817
00:35:20,400 --> 00:35:22,320
word the trigger word.

818
00:35:22,320 --> 00:35:26,050
Alexa, hey Siri, OK Google.

819
00:35:26,050 --> 00:35:28,440
That's usually the second layer.

820
00:35:28,440 --> 00:35:31,450
And that one is only listening
for a specific keyword.

821
00:35:31,450 --> 00:35:33,870
If the keyword comes
in, it would typically

822
00:35:33,870 --> 00:35:37,600
call a better model
that's slightly slower,

823
00:35:37,600 --> 00:35:41,050
that might be heavier and
more energy consumption,

824
00:35:41,050 --> 00:35:45,480
and that might understand
what you're trying to do.

825
00:35:45,480 --> 00:35:48,280
And then I'm not going
to go into the details,

826
00:35:48,280 --> 00:35:51,180
but you have architectures
that get very complicated.

827
00:35:51,180 --> 00:35:53,000
Back in the day, some
of these companies

828
00:35:53,000 --> 00:35:57,110
were doing one model to
set up a timer, one model

829
00:35:57,110 --> 00:35:59,430
to buy something online.

830
00:35:59,430 --> 00:36:01,590
One model was very complicated.

831
00:36:01,590 --> 00:36:03,708
Today it's slightly
simpler and more end to end

832
00:36:03,708 --> 00:36:05,750
but I just want you to
know the cascade of models

833
00:36:05,750 --> 00:36:07,730
that are being called
because this case

834
00:36:07,730 --> 00:36:10,980
study is about the second model
is about the trigger word.

835
00:36:10,980 --> 00:36:12,600
So here's my problem for you.

836
00:36:12,600 --> 00:36:17,900
Given a 10-second audio speech,
detect the word activate.

837
00:36:17,900 --> 00:36:23,180
How would you build that off the
shelf like that starting from 0.

838
00:36:23,180 --> 00:36:24,535
What data would you collect?

839
00:36:30,410 --> 00:36:31,160
Yep.

840
00:36:31,160 --> 00:36:35,910
So we are looking at frequencies
that we might receive.

841
00:36:35,910 --> 00:36:40,530
And from those frequency we want
to figure out what was the word.

842
00:36:40,530 --> 00:36:44,600
And we also want to
look at [INAUDIBLE].

843
00:36:44,600 --> 00:36:45,390
OK.

844
00:36:45,390 --> 00:36:49,580
How long does this usually
take to say the word, and then

845
00:36:49,580 --> 00:36:51,350
we will run some
kind of algorithm

846
00:36:51,350 --> 00:36:53,840
to translate our
frequency to [INAUDIBLE].

847
00:36:53,840 --> 00:36:57,350
Like a Fourier transform, or--

848
00:36:57,350 --> 00:36:59,300
what you described is
a Fourier transform

849
00:36:59,300 --> 00:37:01,048
or plus the preprocessing.

850
00:37:01,048 --> 00:37:01,590
You're right.

851
00:37:01,590 --> 00:37:05,000
So you're saying audio
is a bunch of frequencies

852
00:37:05,000 --> 00:37:06,870
with values.

853
00:37:06,870 --> 00:37:09,560
And we want to first
pre-process that, then

854
00:37:09,560 --> 00:37:11,040
to give it to an algorithm.

855
00:37:11,040 --> 00:37:13,647
And the length of the
sequence matters as well.

856
00:37:13,647 --> 00:37:15,230
Because if you want
to detect the word

857
00:37:15,230 --> 00:37:18,128
activate you know that
the length needs to be--

858
00:37:18,128 --> 00:37:19,170
there's a minimum length.

859
00:37:19,170 --> 00:37:21,830
You can't say activate in
less than 10 milliseconds.

860
00:37:21,830 --> 00:37:23,550
So the length matters as well.

861
00:37:23,550 --> 00:37:25,180
That's good insight.

862
00:37:25,180 --> 00:37:25,680
What else?

863
00:37:25,680 --> 00:37:26,180
What data?

864
00:37:26,180 --> 00:37:29,840
How would you collect that data?

865
00:37:29,840 --> 00:37:31,640
Yes.

866
00:37:31,640 --> 00:37:33,460
Microphone like with your phone.

867
00:37:33,460 --> 00:37:33,960
OK.

868
00:37:33,960 --> 00:37:36,480
You would go around
campus and record people.

869
00:37:36,480 --> 00:37:40,070
How would you-- what
would you ask them to say?

870
00:37:40,070 --> 00:37:41,340
The word activate.

871
00:37:41,340 --> 00:37:43,460
You would ask-- you would
record a bunch of people

872
00:37:43,460 --> 00:37:44,940
saying the word activate.

873
00:37:44,940 --> 00:37:48,230
You would ask them
anything else?

874
00:37:48,230 --> 00:37:48,800
Other words.

875
00:37:48,800 --> 00:37:49,610
State--

876
00:37:49,610 --> 00:37:50,430
Other words.

877
00:37:50,430 --> 00:37:51,020
Yeah.

878
00:37:51,020 --> 00:37:51,890
Say a sentence.

879
00:37:51,890 --> 00:37:52,770
Say a sentence.

880
00:37:52,770 --> 00:37:53,540
Yeah.

881
00:37:53,540 --> 00:37:56,460
It turns out you have websites
that are random generators,

882
00:37:56,460 --> 00:37:58,647
and you just say, say
this and say this,

883
00:37:58,647 --> 00:37:59,730
and you record everything.

884
00:37:59,730 --> 00:38:00,230
Yeah.

885
00:38:00,230 --> 00:38:01,640
Say the word deactivate.

886
00:38:01,640 --> 00:38:02,490
Oh good one.

887
00:38:02,490 --> 00:38:05,570
So you're saying you want
to find negative words that

888
00:38:05,570 --> 00:38:08,115
are close to the positive
word, just to make sure

889
00:38:08,115 --> 00:38:09,240
that the model learns that.

890
00:38:09,240 --> 00:38:09,840
That's a great one.

891
00:38:09,840 --> 00:38:10,340
Yeah.

892
00:38:10,340 --> 00:38:14,610
Actually, it turns out activate
is a really bad word to choose.

893
00:38:14,610 --> 00:38:16,670
The reason Alexa,
and actually, there

894
00:38:16,670 --> 00:38:19,280
was a lot of
discussions at Amazon

895
00:38:19,280 --> 00:38:21,630
back in the days around
what would the word be.

896
00:38:21,630 --> 00:38:23,880
And it turns out it's very
important, what you choose,

897
00:38:23,880 --> 00:38:25,755
because you want to
choose a word that is not

898
00:38:25,755 --> 00:38:28,130
used in common language,
otherwise your assistant

899
00:38:28,130 --> 00:38:30,711
is always turning on.

900
00:38:30,711 --> 00:38:32,670
And Alexa is not ideal either.

901
00:38:32,670 --> 00:38:34,730
It's not bad, but
it's not ideal either.

902
00:38:34,730 --> 00:38:37,170
OK, so let me just
narrow down the problem.

903
00:38:37,170 --> 00:38:39,470
Let's say we've
gone around campus,

904
00:38:39,470 --> 00:38:43,300
and we've collected a bunch
of 10-second audio clips.

905
00:38:48,131 --> 00:38:51,150
Do we need to think about
the distribution of the data?

906
00:38:51,150 --> 00:38:52,410
Why does it matter?

907
00:38:52,410 --> 00:38:56,840
Why campus only might
be limited, let's say?

908
00:38:56,840 --> 00:38:57,560
Yeah.

909
00:38:57,560 --> 00:38:59,180
Maybe [? I'd ?] get
a lot of accents.

910
00:38:59,180 --> 00:39:00,810
OK, accents.

911
00:39:00,810 --> 00:39:04,010
Turns out the first
version of this model

912
00:39:04,010 --> 00:39:08,190
that I built with Andrew,
my German friends,

913
00:39:08,190 --> 00:39:09,420
could not make it work.

914
00:39:09,420 --> 00:39:11,490
None of my German friends
would make it work.

915
00:39:11,490 --> 00:39:13,380
And we had to collect
more data from--

916
00:39:13,380 --> 00:39:15,300
and I had two German
roommates at the time,

917
00:39:15,300 --> 00:39:17,720
so we had to collect
more data from them,

918
00:39:17,720 --> 00:39:22,070
because there's just a certain
way that people would say words.

919
00:39:22,070 --> 00:39:23,010
OK good insight.

920
00:39:23,010 --> 00:39:26,030
What else other than accents?

921
00:39:26,030 --> 00:39:27,480
Yes.

922
00:39:27,480 --> 00:39:29,240
Average age.

923
00:39:29,240 --> 00:39:30,180
Good point.

924
00:39:30,180 --> 00:39:32,840
Average age on
campus is probably

925
00:39:32,840 --> 00:39:35,630
younger than if you
actually cross campus and go

926
00:39:35,630 --> 00:39:37,010
to Palo Alto.

927
00:39:37,010 --> 00:39:39,260
And in fact, the frequencies
are going to be different

928
00:39:39,260 --> 00:39:40,830
that younger people use.

929
00:39:40,830 --> 00:39:43,330
That's correct.

930
00:39:43,330 --> 00:39:43,957
Yeah.

931
00:39:43,957 --> 00:39:45,040
There is also the cadence.

932
00:39:45,040 --> 00:39:46,270
How fast you say words.

933
00:39:46,270 --> 00:39:48,020
How fast-- some
people speak fast,

934
00:39:48,020 --> 00:39:50,570
some people-- and it has to do
with the language of origin.

935
00:39:50,570 --> 00:39:53,280
Some people just speak faster.

936
00:39:53,280 --> 00:39:53,780
Correct.

937
00:39:53,780 --> 00:39:56,210
And look at this.

938
00:39:56,210 --> 00:39:58,960
When you hear someone
who speaks fast

939
00:39:58,960 --> 00:40:02,320
versus someone who speaks slow,
it doesn't make a big difference

940
00:40:02,320 --> 00:40:04,220
to you as a human.

941
00:40:04,220 --> 00:40:06,400
But if you actually
just had access

942
00:40:06,400 --> 00:40:08,330
to the numbers and
the frequencies,

943
00:40:08,330 --> 00:40:09,920
it would look
completely different.

944
00:40:09,920 --> 00:40:13,580
So the model actually struggles
a lot with that problem.

945
00:40:13,580 --> 00:40:14,470
Yeah.

946
00:40:14,470 --> 00:40:18,070
I don't think this is very
important for the ratio of male

947
00:40:18,070 --> 00:40:18,890
to female voice.

948
00:40:18,890 --> 00:40:19,760
Yeah for sure.

949
00:40:19,760 --> 00:40:21,230
Ratio of male to female.

950
00:40:21,230 --> 00:40:23,300
Anything that would
modify the frequencies.

951
00:40:23,300 --> 00:40:25,720
And on average, yes, there's
different frequencies

952
00:40:25,720 --> 00:40:27,080
or distribution male to female.

953
00:40:27,080 --> 00:40:27,688
Yeah.

954
00:40:27,688 --> 00:40:29,980
[INAUDIBLE] noise or other
people like talking to them.

955
00:40:29,980 --> 00:40:30,890
Background noise.

956
00:40:30,890 --> 00:40:31,880
Very important.

957
00:40:31,880 --> 00:40:36,850
Turns out on Stanford campus,
you don't hear the metro.

958
00:40:36,850 --> 00:40:39,670
So it's very likely that
your algorithm will not

959
00:40:39,670 --> 00:40:42,520
work for people in New York
that are taking the subway all

960
00:40:42,520 --> 00:40:44,690
the time because of the
background noise behind it.

961
00:40:44,690 --> 00:40:45,830
Yeah OK.

962
00:40:45,830 --> 00:40:48,490
I think we get a sense of
the again, the complexity

963
00:40:48,490 --> 00:40:50,800
of the task ahead of us.

964
00:40:50,800 --> 00:40:54,050
Let's say the input is
a 10-second audio CLIP.

965
00:40:54,050 --> 00:40:56,920
I'm going to call X.
And this audio CLIP

966
00:40:56,920 --> 00:41:00,190
has a few things that
are special to it.

967
00:41:00,190 --> 00:41:03,610
So one of the things
is a negative words

968
00:41:03,610 --> 00:41:08,990
which are in purple, positive
words which are in green,

969
00:41:08,990 --> 00:41:11,090
and then the background
is in orange.

970
00:41:11,090 --> 00:41:13,820
So this is for example,
someone saying hi,

971
00:41:13,820 --> 00:41:16,150
activate yourself, whatever.

972
00:41:16,150 --> 00:41:17,300
You see what I mean?

973
00:41:17,300 --> 00:41:19,720
Activate is the positive word.

974
00:41:19,720 --> 00:41:22,392
What's the resolution we want?

975
00:41:22,392 --> 00:41:24,100
OK I'm not going to
ask you this question

976
00:41:24,100 --> 00:41:26,558
because we don't have speech
expert-- a speech expert would

977
00:41:26,558 --> 00:41:27,400
know it.

978
00:41:27,400 --> 00:41:30,500
What you can do, though,
without being a speech expert,

979
00:41:30,500 --> 00:41:34,120
is to go on GitHub and find
another speech project,

980
00:41:34,120 --> 00:41:37,190
and you actually search for the
hyperparameters they're using.

981
00:41:37,190 --> 00:41:39,590
And if you're using
human audio, you'll

982
00:41:39,590 --> 00:41:42,650
find that the same numbers
will work for your project.

983
00:41:42,650 --> 00:41:44,060
OK.

984
00:41:44,060 --> 00:41:46,100
So you do that little
search, and you'll

985
00:41:46,100 --> 00:41:50,210
find that there is just a
certain sample rate that

986
00:41:50,210 --> 00:41:53,480
works well with human voice.

987
00:41:53,480 --> 00:41:54,425
What's the output?

988
00:41:59,360 --> 00:42:00,330
0 or 1.

989
00:42:00,330 --> 00:42:01,500
OK, let's try something.

990
00:42:01,500 --> 00:42:03,500
So let's say the
output is 0 or 1.

991
00:42:03,500 --> 00:42:05,608
0 meaning there is
no positive word.

992
00:42:05,608 --> 00:42:06,900
The word activate is not there.

993
00:42:06,900 --> 00:42:10,030
One meaning there is a positive
word in that 10-second audio

994
00:42:10,030 --> 00:42:10,530
clip.

995
00:42:10,530 --> 00:42:13,140
So we're going to do a
little human experiment.

996
00:42:13,140 --> 00:42:18,840
I've selected three-- let
me turn the volume on.

997
00:42:18,840 --> 00:42:24,210
I've selected three audio
samples of around 10 seconds.

998
00:42:24,210 --> 00:42:26,510
OK.

999
00:42:26,510 --> 00:42:29,550
I'm not going to tell
you what the language is,

1000
00:42:29,550 --> 00:42:31,430
because the model
doesn't language

1001
00:42:31,430 --> 00:42:32,640
when we start training it.

1002
00:42:32,640 --> 00:42:34,470
So you're acting like the model.

1003
00:42:34,470 --> 00:42:35,850
That's the experiment.

1004
00:42:35,850 --> 00:42:38,660
I'm just telling
you that the first.

1005
00:42:38,660 --> 00:42:42,132
And the third sample have the
word that we're looking for.

1006
00:42:42,132 --> 00:42:44,090
And I'm not telling you
what the word is again,

1007
00:42:44,090 --> 00:42:45,840
because the model
doesn't what the word is

1008
00:42:45,840 --> 00:42:47,482
at the beginning of training.

1009
00:42:47,482 --> 00:42:50,650
So now up to you to
guess what the word is.

1010
00:42:50,650 --> 00:42:53,390
And I hope one of you will
save us and find the word.

1011
00:42:53,390 --> 00:42:56,590
OK let's try it.

1012
00:42:56,590 --> 00:42:59,590
[NON-ENGLISH SPEECH]

1013
00:43:02,440 --> 00:43:03,790
To loud.

1014
00:43:03,790 --> 00:43:06,160
Second sample.

1015
00:43:06,160 --> 00:43:08,310
Wait let me see if I can
put the microphone here.

1016
00:43:11,946 --> 00:43:15,220
[NON-ENGLISH SPEECH]

1017
00:43:18,760 --> 00:43:19,700
Anybody has it or.

1018
00:43:19,700 --> 00:43:20,900
No, no.

1019
00:43:20,900 --> 00:43:21,670
No way.

1020
00:43:21,670 --> 00:43:23,110
Impossible.

1021
00:43:23,110 --> 00:43:25,246
Third one.

1022
00:43:25,246 --> 00:43:27,895
[NON-ENGLISH SPEECH]

1023
00:43:31,770 --> 00:43:32,270
OK.

1024
00:43:32,270 --> 00:43:33,090
Who has the word?

1025
00:43:35,860 --> 00:43:37,300
[NON-ENGLISH SPEECH]

1026
00:43:37,300 --> 00:43:38,260
OK, maybe.

1027
00:43:38,260 --> 00:43:41,140
That's not it, but maybe.

1028
00:43:41,140 --> 00:43:42,477
Yeah in the back?

1029
00:43:42,477 --> 00:43:43,810
Sounds like [NON-ENGLISH SPEECH]

1030
00:43:43,810 --> 00:43:46,150
You're Italian?

1031
00:43:46,150 --> 00:43:47,080
You speak-- OK.

1032
00:43:47,080 --> 00:43:50,450
[LAUGHTER] Yeah it's funny.

1033
00:43:50,450 --> 00:43:54,560
Nobody finds it usually in the
first try, but you did find it.

1034
00:43:54,560 --> 00:43:56,020
Yeah that's correct.

1035
00:43:56,020 --> 00:44:01,660
OK, let's try again and do it
with a different labeling scheme

1036
00:44:01,660 --> 00:44:02,360
this time.

1037
00:44:02,360 --> 00:44:04,400
OK, let's try again, I'm
going to play it again,

1038
00:44:04,400 --> 00:44:05,900
but the labeling
scheme has changed.

1039
00:44:09,740 --> 00:44:13,623
[NON-ENGLISH SPEECH]

1040
00:44:14,545 --> 00:44:17,734
It's the first one.

1041
00:44:17,734 --> 00:44:21,020
[NON-ENGLISH SPEECH]

1042
00:44:23,860 --> 00:44:24,860
Third one.

1043
00:44:24,860 --> 00:44:27,760
[NON-ENGLISH SPEECH]

1044
00:44:31,980 --> 00:44:33,720
What's the word.

1045
00:44:33,720 --> 00:44:35,960
Someone who's not
Italian speaker.

1046
00:44:42,570 --> 00:44:45,100
I'm not sure people heard so
I want to try someone else.

1047
00:44:45,100 --> 00:44:46,530
Yeah, you've heard it.

1048
00:44:46,530 --> 00:44:47,790
Something like promojo.

1049
00:44:47,790 --> 00:44:52,470
OK not far promojo, it's
pomeriggio But you're close.

1050
00:44:52,470 --> 00:44:54,860
Was it easier the second
time or the first time?

1051
00:44:54,860 --> 00:44:55,860
The second.

1052
00:44:55,860 --> 00:44:56,680
Way more.

1053
00:44:56,680 --> 00:44:57,490
Way easier.

1054
00:44:57,490 --> 00:45:01,890
So if it's easier for you, it's
easier for the model, basically.

1055
00:45:01,890 --> 00:45:04,680
And so what is the
question I'm posing here

1056
00:45:04,680 --> 00:45:08,400
is we could go with the first
labeling scheme which is

1057
00:45:08,400 --> 00:45:10,390
easier for us to label frankly.

1058
00:45:10,390 --> 00:45:14,490
You don't need to indicate
the location of the word.

1059
00:45:14,490 --> 00:45:16,980
But how much more
data do you think

1060
00:45:16,980 --> 00:45:20,400
we'll need in order for
the model to figure it out?

1061
00:45:20,400 --> 00:45:24,270
It's probably 1,000x data.

1062
00:45:24,270 --> 00:45:27,930
And so the question is is the
second labeling scheme 1,000

1063
00:45:27,930 --> 00:45:31,270
times harder for us to
label than the first one

1064
00:45:31,270 --> 00:45:32,820
and the answer is no.

1065
00:45:32,820 --> 00:45:34,120
So the answer is very clear.

1066
00:45:34,120 --> 00:45:36,220
You would rather have the
second labeling scheme,

1067
00:45:36,220 --> 00:45:37,720
and your model is
going to learn way

1068
00:45:37,720 --> 00:45:41,140
faster than in the first case.

1069
00:45:41,140 --> 00:45:43,600
So that's the type of human
experiment you can do.

1070
00:45:43,600 --> 00:45:46,510
Now we're going to use
that labeling scheme.

1071
00:45:46,510 --> 00:45:47,310
Yeah question.

1072
00:45:47,310 --> 00:45:48,450
For the label.

1073
00:45:48,450 --> 00:45:49,870
This is Marvin, right?

1074
00:45:49,870 --> 00:45:51,340
Yeah we'll talk about it.

1075
00:45:51,340 --> 00:45:53,260
Question is it manual
labeling or not?

1076
00:45:53,260 --> 00:45:54,720
Yes right now it's
manual, but I'll

1077
00:45:54,720 --> 00:45:56,860
explain some tricks we can use.

1078
00:45:56,860 --> 00:45:57,547
Yeah.

1079
00:45:57,547 --> 00:46:02,610
Is there a tradeoff where
you spend a lot more time

1080
00:46:02,610 --> 00:46:04,060
to label something.

1081
00:46:04,060 --> 00:46:06,267
But then it does do
better on the model.

1082
00:46:06,267 --> 00:46:08,100
How do you think that
would [? trade off? ?]

1083
00:46:08,100 --> 00:46:10,290
Yeah how do you
pick the trade-off

1084
00:46:10,290 --> 00:46:12,510
between the different
labeling strategies and not.

1085
00:46:12,510 --> 00:46:17,430
In fact, today you could
have a pre-training

1086
00:46:17,430 --> 00:46:20,970
and a post-training that have
different labeling schemes.

1087
00:46:20,970 --> 00:46:24,070
The question is
for pre-training.

1088
00:46:24,070 --> 00:46:26,250
You want the model
to get really good

1089
00:46:26,250 --> 00:46:29,710
and you don't want to-- you want
to avoid a cold start problem.

1090
00:46:29,710 --> 00:46:33,280
The problem of the cold start is
with the first labeling scheme,

1091
00:46:33,280 --> 00:46:36,690
maybe even with 1,000 data
points that you collect

1092
00:46:36,690 --> 00:46:38,910
and label manually, it's
not even going to understand

1093
00:46:38,910 --> 00:46:39,610
anything.

1094
00:46:39,610 --> 00:46:42,240
So you need-- the
second labeling

1095
00:46:42,240 --> 00:46:46,390
scheme can be a great way to
work around the cold starts.

1096
00:46:46,390 --> 00:46:48,780
You might need less data, but
it will start understanding

1097
00:46:48,780 --> 00:46:50,880
what you mean and then
the rest of the data

1098
00:46:50,880 --> 00:46:53,398
might be labeled
differently, essentially.

1099
00:46:53,398 --> 00:46:55,690
OK, let me talk a little bit
about the labeling scheme.

1100
00:46:55,690 --> 00:46:57,690
We're actually going to
use a slightly different

1101
00:46:57,690 --> 00:46:58,630
labeling scheme.

1102
00:46:58,630 --> 00:47:04,380
And the reason is the
one with 1, 1 and only 0

1103
00:47:04,380 --> 00:47:07,500
the risk is you can,
actually, have a model that

1104
00:47:07,500 --> 00:47:12,600
performs 99.9% accurate that
is, all zeros just predict 0

1105
00:47:12,600 --> 00:47:13,360
all the time.

1106
00:47:13,360 --> 00:47:14,950
It's very accurate.

1107
00:47:14,950 --> 00:47:17,610
But the thing is there's
just one one to find,

1108
00:47:17,610 --> 00:47:18,970
and it's very hard to find it.

1109
00:47:18,970 --> 00:47:23,070
And so the network is
going to lean toward zeros.

1110
00:47:23,070 --> 00:47:26,970
Meaning the data is-- the
labels are so skewed towards 0

1111
00:47:26,970 --> 00:47:29,690
that it's going to be hard
to find any signal in it.

1112
00:47:29,690 --> 00:47:31,660
And so the trick that
deep learning researchers

1113
00:47:31,660 --> 00:47:34,210
use generally is
can we actually do

1114
00:47:34,210 --> 00:47:37,420
a little more balanced
between the positive

1115
00:47:37,420 --> 00:47:39,190
and the negative labels.

1116
00:47:39,190 --> 00:47:41,210
Just a pure engineering hack.

1117
00:47:41,210 --> 00:47:43,240
Not much science behind it.

1118
00:47:43,240 --> 00:47:45,347
The last activation we're
going to use a sigmoid.

1119
00:47:45,347 --> 00:47:46,930
But because it's a
sequential problem,

1120
00:47:46,930 --> 00:47:48,710
we're going to use
sigmoid in sequence.

1121
00:47:48,710 --> 00:47:50,170
At every time
step, there's going

1122
00:47:50,170 --> 00:47:52,100
to be a sigmoid activation.

1123
00:47:52,100 --> 00:47:53,907
And then the
architecture, we're going

1124
00:47:53,907 --> 00:47:56,240
to learn it later in the
class, you don't need to worry.

1125
00:47:56,240 --> 00:47:58,430
This would be likely an RNN.

1126
00:47:58,430 --> 00:48:00,880
You'll learn later in the
class what that means.

1127
00:48:00,880 --> 00:48:03,370
And then the loss
function-- can someone

1128
00:48:03,370 --> 00:48:06,610
guess what loss function,
we would be using?

1129
00:48:06,610 --> 00:48:08,830
Yes yeah.

1130
00:48:08,830 --> 00:48:12,260
Binary cross-entropy, but we're
going to use it sequentially.

1131
00:48:12,260 --> 00:48:14,350
Meaning at every
step, we're going

1132
00:48:14,350 --> 00:48:17,360
to compute that with
the sigmoid output.

1133
00:48:17,360 --> 00:48:17,860
Yes.

1134
00:48:17,860 --> 00:48:18,440
Sorry.

1135
00:48:18,440 --> 00:48:19,160
On the output.

1136
00:48:19,160 --> 00:48:23,750
So you just label the activate
word as multiple ones.

1137
00:48:23,750 --> 00:48:24,250
Correct.

1138
00:48:24,250 --> 00:48:27,600
You take your 10 second inputs.

1139
00:48:27,600 --> 00:48:30,290
You look at where the
positive word activate is

1140
00:48:30,290 --> 00:48:34,430
and you put ones in
there when it plays.

1141
00:48:34,430 --> 00:48:37,190
And you force the model
to predict, hey, activate

1142
00:48:37,190 --> 00:48:38,130
has been played.

1143
00:48:38,130 --> 00:48:40,003
There is a lot of
nuances you see,

1144
00:48:40,003 --> 00:48:42,170
because you actually are
going to build this project

1145
00:48:42,170 --> 00:48:44,420
with do we want
the ones to start

1146
00:48:44,420 --> 00:48:45,810
exactly when the words start.

1147
00:48:45,810 --> 00:48:47,188
Do we want to have a delay?

1148
00:48:47,188 --> 00:48:49,230
There's a lot of technical
questions around that.

1149
00:48:49,230 --> 00:48:49,530
Yeah.

1150
00:48:49,530 --> 00:48:50,210
The other question.

1151
00:48:50,210 --> 00:48:51,960
I was wondering what
do you mean by steps.

1152
00:48:51,960 --> 00:48:54,290
Are they like [INAUDIBLE]?

1153
00:48:54,290 --> 00:48:59,300
What I mean by steps is
audio is sequence data.

1154
00:48:59,300 --> 00:49:02,490
And so it's time
step by time step.

1155
00:49:02,490 --> 00:49:05,930
And so what I mean is that
at every time step, whatever

1156
00:49:05,930 --> 00:49:07,910
your sample rate
is, you're going

1157
00:49:07,910 --> 00:49:12,035
to have a prediction
at every step.

1158
00:49:12,035 --> 00:49:15,620
So what is critical to the
success of this project

1159
00:49:15,620 --> 00:49:18,120
is really the labeling strategy.

1160
00:49:18,120 --> 00:49:21,410
Here is how we did it.

1161
00:49:21,410 --> 00:49:25,530
And you just have to
it or not know it.

1162
00:49:25,530 --> 00:49:28,740
It actually takes a long time to
figure something like that out.

1163
00:49:28,740 --> 00:49:31,260
And so thankfully, when
I was a grad student,

1164
00:49:31,260 --> 00:49:35,700
one of my senior PhDs helped
me with these methods,

1165
00:49:35,700 --> 00:49:38,600
and he was able to guide me
and saved me probably a month

1166
00:49:38,600 --> 00:49:41,670
because I would not have figured
out myself this probably.

1167
00:49:41,670 --> 00:49:43,050
So here's what we did.

1168
00:49:43,050 --> 00:49:45,060
We took three databases.

1169
00:49:45,060 --> 00:49:49,290
We created three databases, one
that would have positive words.

1170
00:49:49,290 --> 00:49:51,260
So as you were
saying earlier, we

1171
00:49:51,260 --> 00:49:53,490
record people for
the word activate.

1172
00:49:53,490 --> 00:49:57,770
One that would have negative
words, other words including

1173
00:49:57,770 --> 00:50:02,280
deactivate, but also kitchen
and lion and dog and whatever.

1174
00:50:02,280 --> 00:50:05,510
And then we have
background noise.

1175
00:50:05,510 --> 00:50:08,940
Turns out background noise
in audio data is almost free.

1176
00:50:08,940 --> 00:50:11,250
There's just a ton of
background noise online.

1177
00:50:11,250 --> 00:50:14,130
You can go on many platforms,
online video platforms.

1178
00:50:14,130 --> 00:50:15,510
You can just take the audio.

1179
00:50:15,510 --> 00:50:17,070
It will be background noise.

1180
00:50:17,070 --> 00:50:18,330
So background noise is free.

1181
00:50:18,330 --> 00:50:21,560
You don't need to go and
collect it, most likely.

1182
00:50:21,560 --> 00:50:23,280
The other two,
though, are harder.

1183
00:50:23,280 --> 00:50:24,180
Yeah question.

1184
00:50:24,180 --> 00:50:26,580
Did you record those
first two yourself.

1185
00:50:26,580 --> 00:50:27,260
Yeah.

1186
00:50:27,260 --> 00:50:28,040
How many?

1187
00:50:28,040 --> 00:50:29,120
So I'll tell you--

1188
00:50:29,120 --> 00:50:31,350
I'll tell you how we
do it in a second.

1189
00:50:31,350 --> 00:50:34,010
But yes, we recorded
everything manually.

1190
00:50:34,010 --> 00:50:37,110
What we did is we went online.

1191
00:50:37,110 --> 00:50:40,530
We scraped free license data.

1192
00:50:40,530 --> 00:50:42,943
Actually, it's a skill to
know the licensing models.

1193
00:50:42,943 --> 00:50:45,110
You're going to learn that
in the project mentorship

1194
00:50:45,110 --> 00:50:46,110
with the TAs.

1195
00:50:46,110 --> 00:50:49,170
What licensing allows
you to do what with data

1196
00:50:49,170 --> 00:50:50,510
it's good to know forever.

1197
00:50:50,510 --> 00:50:52,010
You learn it once
and then you know.

1198
00:50:52,010 --> 00:50:56,120
What is CC BY, what is CC BY NA,
what is the MIT license, what is

1199
00:50:56,120 --> 00:50:58,490
the Apache license, et cetera.

1200
00:50:58,490 --> 00:51:00,660
And so we take
10-second audio clips.

1201
00:51:00,660 --> 00:51:02,060
We CLIP the background noise.

1202
00:51:02,060 --> 00:51:04,490
And we went around
campus and we literally

1203
00:51:04,490 --> 00:51:07,380
recorded people saying
activate and other words.

1204
00:51:07,380 --> 00:51:10,260
And we cut it when they said it.

1205
00:51:10,260 --> 00:51:13,400
So the word was contained
exactly to the amount

1206
00:51:13,400 --> 00:51:14,430
that it was needed.

1207
00:51:14,430 --> 00:51:17,960
Then we created a Python script
that randomly inserts words,

1208
00:51:17,960 --> 00:51:18,680
randomly.

1209
00:51:18,680 --> 00:51:20,250
Non-overlapping words.

1210
00:51:20,250 --> 00:51:23,040
So for example, this would
be created synthetically.

1211
00:51:23,040 --> 00:51:25,200
I would have tens
of background noise.

1212
00:51:25,200 --> 00:51:27,530
I would insert two
negative words,

1213
00:51:27,530 --> 00:51:30,680
and I would insert
one positive word, OK.

1214
00:51:30,680 --> 00:51:33,410
The trick is that
because the Python script

1215
00:51:33,410 --> 00:51:38,180
did that, the Python script
knows where activate was put

1216
00:51:38,180 --> 00:51:40,730
so it can label automatically.

1217
00:51:40,730 --> 00:51:43,170
And so it turns out that
we went around campus,

1218
00:51:43,170 --> 00:51:45,300
we used an app even to get help.

1219
00:51:45,300 --> 00:51:48,000
So we hired a couple of
people to come with us.

1220
00:51:48,000 --> 00:51:49,850
Each brought their
phone, and we went all

1221
00:51:49,850 --> 00:51:51,560
around campus
recording people to say

1222
00:51:51,560 --> 00:51:52,830
activate, and other words.

1223
00:51:52,830 --> 00:51:54,360
And we created those data sets.

1224
00:51:54,360 --> 00:51:59,810
Within three hours, we had
millions of data points.

1225
00:51:59,810 --> 00:52:00,960
Because think about it.

1226
00:52:00,960 --> 00:52:06,620
Let's say you have 1,000
activates across campus,

1227
00:52:06,620 --> 00:52:09,840
10,000 other words,
infinite background noise.

1228
00:52:09,840 --> 00:52:11,990
Imagine how much data
you can create with that.

1229
00:52:11,990 --> 00:52:13,890
When you actually write
the Python script,

1230
00:52:13,890 --> 00:52:15,617
you can also add some
data augmentation.

1231
00:52:15,617 --> 00:52:16,950
You can reduce some frequencies.

1232
00:52:16,950 --> 00:52:18,030
You can augment
some frequencies,

1233
00:52:18,030 --> 00:52:20,030
you can accelerate it,
you can decelerate it.

1234
00:52:20,030 --> 00:52:23,140
So you can actually create a
pretty meaningful data sets

1235
00:52:23,140 --> 00:52:25,265
for this problem in three hours.

1236
00:52:28,630 --> 00:52:31,660
Now I'm talking
about training sets

1237
00:52:31,660 --> 00:52:34,070
because you don't want to
use that data for test sets.

1238
00:52:34,070 --> 00:52:37,018
For test sets, you want data
to be as real as possible.

1239
00:52:37,018 --> 00:52:39,310
You're familiar with the
concept of train and test set.

1240
00:52:39,310 --> 00:52:42,380
So for the test set, we
had to manually label data,

1241
00:52:42,380 --> 00:52:45,940
but it was a much smaller
set than the training set.

1242
00:52:45,940 --> 00:52:47,390
So it's much more convenient.

1243
00:52:47,390 --> 00:52:51,365
The second important part
was architecture search.

1244
00:52:51,365 --> 00:52:53,240
I'm not going to talk
about it too much here,

1245
00:52:53,240 --> 00:52:56,830
but there are architectures
that just work better

1246
00:52:56,830 --> 00:52:58,400
for these types of problems.

1247
00:52:58,400 --> 00:53:01,360
And this is an example
of an architecture,

1248
00:53:01,360 --> 00:53:04,180
on the right, that
works way better.

1249
00:53:04,180 --> 00:53:07,360
And my learning
from that project

1250
00:53:07,360 --> 00:53:10,120
was just go to the expert and
ask them what they've tried

1251
00:53:10,120 --> 00:53:12,050
and try to learn
from their mistakes.

1252
00:53:12,050 --> 00:53:14,000
And in fact, I
remember Ani Hainan,

1253
00:53:14,000 --> 00:53:17,380
which was in the first level
at the gates computer science

1254
00:53:17,380 --> 00:53:21,620
building, and he knew that this
architecture was going to fail.

1255
00:53:21,620 --> 00:53:24,130
And he knew why, because he's
done so many speech projects,

1256
00:53:24,130 --> 00:53:27,130
and he just knows what
works and what doesn't work.

1257
00:53:27,130 --> 00:53:31,930
So that's why your TAs are here
for actually in your projects.

1258
00:53:31,930 --> 00:53:33,740
So you should give
them a call, and say,

1259
00:53:33,740 --> 00:53:36,550
hey, is what I'm
doing good or give me

1260
00:53:36,550 --> 00:53:41,050
a pointer for what I might
spend my next week doing.

1261
00:53:41,050 --> 00:53:41,690
OK.

1262
00:53:41,690 --> 00:53:44,980
So learnings from this
section, this case study.

1263
00:53:44,980 --> 00:53:47,810
Data collection strategy
is extremely important,

1264
00:53:47,810 --> 00:53:49,960
including the data
labeling strategy

1265
00:53:49,960 --> 00:53:53,000
using human experiments
matters as well,

1266
00:53:53,000 --> 00:53:55,452
and then referring
to expert advice.

1267
00:53:55,452 --> 00:53:57,660
That's the type of thing
you want to do in a project.

1268
00:54:01,020 --> 00:54:03,370
Do you understand
conceptually how such project

1269
00:54:03,370 --> 00:54:04,810
is built now at a high level?

1270
00:54:04,810 --> 00:54:06,080
OK, good.

1271
00:54:06,080 --> 00:54:08,050
Yeah question.

1272
00:54:08,050 --> 00:54:10,840
At the application
layer, how often

1273
00:54:10,840 --> 00:54:13,300
do you have to create
your own architecture?

1274
00:54:13,300 --> 00:54:17,410
How often it's just like
top the shelf [INAUDIBLE]?

1275
00:54:17,410 --> 00:54:23,050
So how often do you need to do
an architecture search nowadays?

1276
00:54:23,050 --> 00:54:24,590
Less often than before.

1277
00:54:24,590 --> 00:54:26,350
But this class is all
about understanding

1278
00:54:26,350 --> 00:54:27,650
what's going on under the hood.

1279
00:54:27,650 --> 00:54:29,380
So we're going to
walk you through that.

1280
00:54:29,380 --> 00:54:33,945
In practice, it depends
on your problem.

1281
00:54:33,945 --> 00:54:35,320
I'll give you in
the industry you

1282
00:54:35,320 --> 00:54:38,290
might be building a company
that requires the model

1283
00:54:38,290 --> 00:54:39,920
to be running on the browser.

1284
00:54:39,920 --> 00:54:41,740
And so you have additional
constraints that

1285
00:54:41,740 --> 00:54:43,520
push you to create
your own architecture,

1286
00:54:43,520 --> 00:54:46,940
collect your own data fine tune
the model the way you want.

1287
00:54:46,940 --> 00:54:50,390
For many startups out there
and companies out there,

1288
00:54:50,390 --> 00:54:52,460
you're going to start
from a foundation model.

1289
00:54:52,460 --> 00:54:54,377
You're going to start
from a foundation model,

1290
00:54:54,377 --> 00:54:56,230
and then you might,
actually, quantize it

1291
00:54:56,230 --> 00:55:00,830
or prune it or modify it to meet
your needs in terms of latency,

1292
00:55:00,830 --> 00:55:03,200
in terms of memory, in
terms of hardware capacity.

1293
00:55:03,200 --> 00:55:06,560
And knowing what's going on we
did is important in those cases.

1294
00:55:06,560 --> 00:55:08,590
Yeah.

1295
00:55:08,590 --> 00:55:09,880
OK.

1296
00:55:09,880 --> 00:55:10,570
Super.

1297
00:55:10,570 --> 00:55:12,320
So we're one hour in.

1298
00:55:12,320 --> 00:55:15,110
We are about halfway through
the class, a little bit ahead,

1299
00:55:15,110 --> 00:55:19,960
and I have a few more case
studies to cover with you.

1300
00:55:19,960 --> 00:55:24,670
By the end, we will have a
full set of proxy projects

1301
00:55:24,670 --> 00:55:25,310
to work with.

1302
00:55:25,310 --> 00:55:27,500
So this one is cool.

1303
00:55:27,500 --> 00:55:30,280
It's face verification.

1304
00:55:30,280 --> 00:55:32,470
A school wants to
use face verification

1305
00:55:32,470 --> 00:55:35,290
to validate student IDs
in facilities like dining

1306
00:55:35,290 --> 00:55:37,810
halls, gym, and pool.

1307
00:55:37,810 --> 00:55:39,320
So let me explain what it is.

1308
00:55:39,320 --> 00:55:42,250
It's like you arrive
at the Arriaga gym

1309
00:55:42,250 --> 00:55:47,230
and instead of just normally
you would swipe your student ID.

1310
00:55:47,230 --> 00:55:49,840
And your picture will
show up on the screen,

1311
00:55:49,840 --> 00:55:51,522
and there's someone
sitting there.

1312
00:55:51,522 --> 00:55:53,230
It's going to compare
the picture they're

1313
00:55:53,230 --> 00:55:55,690
seeing on the screen
to the picture they're

1314
00:55:55,690 --> 00:55:57,020
seeing with their eyes.

1315
00:55:57,020 --> 00:56:00,190
And if it's the same, they're
going to say can go ahead.

1316
00:56:00,190 --> 00:56:01,330
This is slightly different.

1317
00:56:01,330 --> 00:56:07,010
Here you're going to be
verified by a camera.

1318
00:56:07,010 --> 00:56:09,020
So there's actually
a camera ahead.

1319
00:56:09,020 --> 00:56:13,510
And you're walking in and
you're swiping your card.

1320
00:56:13,510 --> 00:56:15,810
And we're going to compare
the picture in the database

1321
00:56:15,810 --> 00:56:18,390
to the picture that is
being taken by the camera

1322
00:56:18,390 --> 00:56:20,030
to make sure that
it's the same person.

1323
00:56:23,001 --> 00:56:24,290
How do you get started?

1324
00:56:33,690 --> 00:56:36,540
[INAUDIBLE] students
[INAUDIBLE].

1325
00:56:36,540 --> 00:56:41,200
So everybody who got admitted
uploaded pictures at least one.

1326
00:56:41,200 --> 00:56:44,110
So we have that already
in the database, correct?

1327
00:56:44,110 --> 00:56:44,610
Yeah.

1328
00:56:44,610 --> 00:56:45,400
What else?

1329
00:56:45,400 --> 00:56:47,070
You would probably
use cameras that

1330
00:56:47,070 --> 00:56:49,560
are going to be used for
detecting fakes later.

1331
00:56:49,560 --> 00:56:51,050
You're saying the
camera matters?

1332
00:56:51,050 --> 00:56:51,550
Yeah.

1333
00:56:51,550 --> 00:56:53,590
Yeah for sure the
camera matters.

1334
00:56:53,590 --> 00:56:55,570
In fact, we talked
about resolution.

1335
00:56:55,570 --> 00:56:58,920
You think the resolution is
lower or higher than the day

1336
00:56:58,920 --> 00:57:00,820
and night project?

1337
00:57:00,820 --> 00:57:01,320
[INAUDIBLE]

1338
00:57:01,320 --> 00:57:02,260
Probably higher.

1339
00:57:02,260 --> 00:57:03,670
In fact, it's
going to be higher.

1340
00:57:03,670 --> 00:57:06,420
And again, I would go
back to literally doing

1341
00:57:06,420 --> 00:57:09,390
a human experiment and
showing pictures of twins

1342
00:57:09,390 --> 00:57:13,110
and asking people if they
can differentiate the twins.

1343
00:57:13,110 --> 00:57:16,390
And you'll see that the
resolution matters, actually.

1344
00:57:16,390 --> 00:57:16,993
OK yeah.

1345
00:57:16,993 --> 00:57:18,160
You wanted to add something.

1346
00:57:18,160 --> 00:57:19,170
Oh, yeah.

1347
00:57:19,170 --> 00:57:22,300
You also need noise.

1348
00:57:22,300 --> 00:57:24,190
Other people that
are not used to.

1349
00:57:24,190 --> 00:57:26,970
Also, data from
outside the University.

1350
00:57:26,970 --> 00:57:29,190
We could look at the features.

1351
00:57:29,190 --> 00:57:30,630
Which features.

1352
00:57:30,630 --> 00:57:33,840
The person's human features
that we could compare.

1353
00:57:33,840 --> 00:57:35,512
Understand the
person's human feature,

1354
00:57:35,512 --> 00:57:36,970
but they're already
in the picture.

1355
00:57:36,970 --> 00:57:38,550
So the picture would
have the feature

1356
00:57:38,550 --> 00:57:40,258
or you would add
anything on top of that?

1357
00:57:42,720 --> 00:57:44,933
So you typically
would not actually

1358
00:57:44,933 --> 00:57:46,350
want to get the
feature level, you

1359
00:57:46,350 --> 00:57:49,000
would just want to say we need
to make sure it's in the data.

1360
00:57:49,000 --> 00:57:51,510
If it's in the data, the
neural network will learn it.

1361
00:57:51,510 --> 00:57:53,550
We likely use like the
same person multiple

1362
00:57:53,550 --> 00:57:55,350
times [INAUDIBLE].

1363
00:57:55,350 --> 00:57:56,310
Absolutely.

1364
00:57:56,310 --> 00:57:58,800
Same person multiple
times because the angle

1365
00:57:58,800 --> 00:58:01,810
matters, the time
matters, the-- et cetera.

1366
00:58:01,810 --> 00:58:02,310
Yeah.

1367
00:58:02,310 --> 00:58:02,900
[INAUDIBLE]

1368
00:58:02,900 --> 00:58:03,400
OK.

1369
00:58:03,400 --> 00:58:05,058
Same thing.

1370
00:58:05,058 --> 00:58:08,310
Try to crop the image
so that the [INAUDIBLE].

1371
00:58:08,310 --> 00:58:10,980
OK, so you're saying we
might do some pre-processing

1372
00:58:10,980 --> 00:58:13,830
to crop all the images so that
it's centered, or at least

1373
00:58:13,830 --> 00:58:15,990
that the image that we're
training the model with

1374
00:58:15,990 --> 00:58:17,615
looks like the image
that the camera is

1375
00:58:17,615 --> 00:58:20,380
going to take because the
model will run on the camera.

1376
00:58:20,380 --> 00:58:22,030
All of these are good.

1377
00:58:22,030 --> 00:58:25,620
So let's say our data set is
picture of every student labeled

1378
00:58:25,620 --> 00:58:26,320
with their name.

1379
00:58:26,320 --> 00:58:28,600
So this is one of
my friends Bertrand.

1380
00:58:28,600 --> 00:58:32,070
And he has his picture, which is
his picture from the student ID.

1381
00:58:32,070 --> 00:58:33,470
And then the input is--

1382
00:58:33,470 --> 00:58:33,970
OK.

1383
00:58:33,970 --> 00:58:36,550
He shows up in front
of the building.

1384
00:58:36,550 --> 00:58:37,930
He's a little bit confused.

1385
00:58:37,930 --> 00:58:41,070
But he showed up and
a picture was taken.

1386
00:58:41,070 --> 00:58:43,150
The resolution, we
talked about it.

1387
00:58:43,150 --> 00:58:45,940
What we use here
is 412, 412, by 3.

1388
00:58:45,940 --> 00:58:48,070
It's much higher
than before, OK,

1389
00:58:48,070 --> 00:58:51,490
as we were expecting because
we need small details.

1390
00:58:51,490 --> 00:58:53,710
Even eye color is identifiable.

1391
00:58:53,710 --> 00:58:57,880
So these things we cannot find
without a higher resolution.

1392
00:58:57,880 --> 00:59:02,730
In fact, if you actually go
through airport security and you

1393
00:59:02,730 --> 00:59:06,635
use some of these FasTracks
which take a picture of you.

1394
00:59:06,635 --> 00:59:08,010
Trust me, the
resolution is going

1395
00:59:08,010 --> 00:59:09,390
to be even higher than that.

1396
00:59:09,390 --> 00:59:11,640
Much higher than that because
they're actually getting

1397
00:59:11,640 --> 00:59:14,100
into the iris at that level.

1398
00:59:14,100 --> 00:59:15,910
So what's the output?

1399
00:59:15,910 --> 00:59:17,320
The output is 0 or 1.

1400
00:59:17,320 --> 00:59:20,340
Yeah it's Bertrand
or it's not Bertrand.

1401
00:59:20,340 --> 00:59:23,550
We're good so far?

1402
00:59:23,550 --> 00:59:24,766
The architecture.

1403
00:59:27,510 --> 00:59:33,960
Let me actually ask how
would you do this comparison

1404
00:59:33,960 --> 00:59:35,410
without neural networks?

1405
00:59:35,410 --> 00:59:37,510
Let's say a very basic way.

1406
00:59:37,510 --> 00:59:40,440
If you have to start
with the first method.

1407
00:59:40,440 --> 00:59:42,450
Yeah.

1408
00:59:42,450 --> 00:59:45,600
We have like average
list of characteristics

1409
00:59:45,600 --> 00:59:48,510
that [INAUDIBLE] applies.

1410
00:59:48,510 --> 00:59:49,870
So you would feature engineer.

1411
00:59:49,870 --> 00:59:51,250
You would say, for example--

1412
00:59:51,250 --> 00:59:53,220
you would define 10
features that are

1413
00:59:53,220 --> 00:59:54,967
good for identifying people.

1414
00:59:54,967 --> 00:59:56,800
And you would have a
filter for each of them

1415
00:59:56,800 --> 00:59:58,633
and you would run it
on the picture and say,

1416
00:59:58,633 --> 01:00:01,500
yes do we have this feature
or not, essentially.

1417
01:00:01,500 --> 01:00:02,140
OK.

1418
01:00:02,140 --> 01:00:03,190
Yeah, it's a good one.

1419
01:00:03,190 --> 01:00:06,860
Even more basic than that
would be a pixel comparison.

1420
01:00:06,860 --> 01:00:08,840
Just compare the two pixels.

1421
01:00:08,840 --> 01:00:12,950
What's the problem with
doing a pixel comparison?

1422
01:00:12,950 --> 01:00:16,580
So the idea is I take the
two pictures, I compare them,

1423
01:00:16,580 --> 01:00:19,970
and if they're close enough
in pixel-wise comparison,

1424
01:00:19,970 --> 01:00:22,580
then it's the same
person, if they're far,

1425
01:00:22,580 --> 01:00:24,020
it's not the same person.

1426
01:00:24,020 --> 01:00:26,320
What can go wrong?

1427
01:00:26,320 --> 01:00:27,460
Difference in lighting.

1428
01:00:27,460 --> 01:00:28,640
Difference in what?

1429
01:00:28,640 --> 01:00:29,140
Lighting.

1430
01:00:29,140 --> 01:00:29,900
Lighting.

1431
01:00:29,900 --> 01:00:31,510
Yeah actually,
what's interesting

1432
01:00:31,510 --> 01:00:33,290
with the lighting
is if you look here.

1433
01:00:33,290 --> 01:00:38,960
So in this one you take the
top left pixel right here, OK.

1434
01:00:38,960 --> 01:00:40,010
It's bright.

1435
01:00:40,010 --> 01:00:43,220
You take the top left
pixel on this one,

1436
01:00:43,220 --> 01:00:46,070
it's dark or at
least dark green.

1437
01:00:46,070 --> 01:00:48,830
The difference between
these two pixels is massive.

1438
01:00:48,830 --> 01:00:52,700
It's close to 255 yet the
pixel doesn't even matter.

1439
01:00:52,700 --> 01:00:54,080
So why would you use that?

1440
01:00:54,080 --> 01:00:56,470
It would penalize the
comparison without actually

1441
01:00:56,470 --> 01:00:57,500
mattering at all.

1442
01:00:57,500 --> 01:00:58,460
So that's a good point.

1443
01:00:58,460 --> 01:00:58,960
Yeah.

1444
01:01:02,150 --> 01:01:06,290
Background noise
can be [INAUDIBLE].

1445
01:01:06,290 --> 01:01:08,630
Yeah, absolutely
background difference.

1446
01:01:08,630 --> 01:01:10,440
Translation invariance.

1447
01:01:10,440 --> 01:01:13,670
Imagine the same
picture, but the person

1448
01:01:13,670 --> 01:01:16,460
is like three
pixels to the right.

1449
01:01:16,460 --> 01:01:18,590
The comparison will be
completely different

1450
01:01:18,590 --> 01:01:21,320
because it's a pixel comparison
rather than a semantically

1451
01:01:21,320 --> 01:01:22,590
meaningful comparison.

1452
01:01:22,590 --> 01:01:26,300
What are other things
that can go wrong?

1453
01:01:26,300 --> 01:01:27,720
The distance from the camera.

1454
01:01:27,720 --> 01:01:28,790
Distance from the camera.

1455
01:01:28,790 --> 01:01:32,550
Again rotation invariance,
translation invariance,

1456
01:01:32,550 --> 01:01:34,950
scale invariance,
all of these matter.

1457
01:01:34,950 --> 01:01:39,180
What are other things that are
not geometric modifications?

1458
01:01:39,180 --> 01:01:39,680
Yeah.

1459
01:01:39,680 --> 01:01:40,970
Wearing glasses or a hat.

1460
01:01:40,970 --> 01:01:42,390
Wearing glasses or hats.

1461
01:01:42,390 --> 01:01:45,290
What else?

1462
01:01:45,290 --> 01:01:46,130
Hairstyle.

1463
01:01:46,130 --> 01:01:47,060
Yeah.

1464
01:01:47,060 --> 01:01:49,040
The big beard.

1465
01:01:49,040 --> 01:01:51,740
And in fact, you look
here he has much more

1466
01:01:51,740 --> 01:01:53,340
beard than on the picture.

1467
01:01:53,340 --> 01:01:56,660
And he was much younger, by
the way, on the first picture.

1468
01:01:56,660 --> 01:02:01,640
And often time we look-- were
younger on our student IDs

1469
01:02:01,640 --> 01:02:04,890
than we actually are in person,
and that may make a difference.

1470
01:02:04,890 --> 01:02:07,280
OK, so these issues
we all talked about.

1471
01:02:07,280 --> 01:02:09,350
I think people get it.

1472
01:02:09,350 --> 01:02:10,020
Good.

1473
01:02:10,020 --> 01:02:14,190
So our solution is to use
the concept of encoding.

1474
01:02:14,190 --> 01:02:17,100
Remember what we talked about
earlier with the face example?

1475
01:02:17,100 --> 01:02:19,790
It turns out a network
that has been trained

1476
01:02:19,790 --> 01:02:22,700
to understand faces should
have meaningful information

1477
01:02:22,700 --> 01:02:25,430
in those layers that you
can use as comparison

1478
01:02:25,430 --> 01:02:27,780
that is more meaningful
than a pixel comparison.

1479
01:02:27,780 --> 01:02:29,400
So this is how it goes.

1480
01:02:29,400 --> 01:02:32,660
We have Bertrand picture
from the student ID

1481
01:02:32,660 --> 01:02:38,090
we run it through a deep neural
network and we get a vector.

1482
01:02:38,090 --> 01:02:39,270
What is this vector?

1483
01:02:39,270 --> 01:02:41,660
It's a vector that we grab
in the middle of the network

1484
01:02:41,660 --> 01:02:42,620
somewhere.

1485
01:02:42,620 --> 01:02:45,350
Remember what I said
if the vector is

1486
01:02:45,350 --> 01:02:47,630
taken earlier in the
network we're going

1487
01:02:47,630 --> 01:02:49,200
to get lower level features.

1488
01:02:49,200 --> 01:02:51,750
If you go deeper, it's going
to get more facial features.

1489
01:02:51,750 --> 01:02:53,460
So you probably go
slightly deeper.

1490
01:02:53,460 --> 01:02:55,920
You go much deeper
actually for this example.

1491
01:02:55,920 --> 01:02:56,880
And then same thing.

1492
01:02:56,880 --> 01:03:01,590
You run the exact same network
on the picture from the camera,

1493
01:03:01,590 --> 01:03:04,620
and normally if the
network was trained well,

1494
01:03:04,620 --> 01:03:08,180
those two vectors should
be close to each other.

1495
01:03:08,180 --> 01:03:09,930
Distance is 0.4.

1496
01:03:09,930 --> 01:03:11,340
You set the threshold.

1497
01:03:11,340 --> 01:03:14,397
You might do a little study to
see what's the right threshold.

1498
01:03:14,397 --> 01:03:15,980
And of course, this
threshold is going

1499
01:03:15,980 --> 01:03:17,780
to determine the number
of true positives

1500
01:03:17,780 --> 01:03:19,940
that you're going to get
versus false positives

1501
01:03:19,940 --> 01:03:21,210
versus false negatives.

1502
01:03:21,210 --> 01:03:24,030
The higher the threshold, the
more likely you make a mistake.

1503
01:03:24,030 --> 01:03:25,850
The more relaxed you are.

1504
01:03:25,850 --> 01:03:30,120
So here let's say I set
a threshold of a 0.5.

1505
01:03:30,120 --> 01:03:33,320
Hey, I'm confident enough
that this is Bertrand.

1506
01:03:33,320 --> 01:03:36,740
And again, airport security
might have a much higher lower

1507
01:03:36,740 --> 01:03:42,080
threshold than the dining
Hall at Stanford, obviously.

1508
01:03:42,080 --> 01:03:46,378
OK, so this is the general
idea behind what we want to do,

1509
01:03:46,378 --> 01:03:48,170
but I still haven't
talked to you about how

1510
01:03:48,170 --> 01:03:49,488
the network is trained.

1511
01:03:49,488 --> 01:03:51,030
The network right
now is not trained.

1512
01:03:51,030 --> 01:03:52,160
We haven't learned
how to train it.

1513
01:03:52,160 --> 01:03:52,660
Yeah.

1514
01:03:52,660 --> 01:03:53,393
Question.

1515
01:03:53,393 --> 01:03:55,310
Question about the
[INAUDIBLE] networking use.

1516
01:03:55,310 --> 01:04:01,310
Is it like just a regular
picture classification or as

1517
01:04:01,310 --> 01:04:03,320
a binary output or is it--

1518
01:04:03,320 --> 01:04:04,290
What's the network?

1519
01:04:04,290 --> 01:04:05,120
We'll learn it, actually.

1520
01:04:05,120 --> 01:04:06,412
We're going to see it together.

1521
01:04:06,412 --> 01:04:07,380
Yeah, I'll describe it.

1522
01:04:07,380 --> 01:04:10,760
But I don't want to get into
the architectural nitty gritty,

1523
01:04:10,760 --> 01:04:13,280
I want to focus on the
general training scheme

1524
01:04:13,280 --> 01:04:14,690
that we're going
to use, and then

1525
01:04:14,690 --> 01:04:16,148
you're actually
going to build that

1526
01:04:16,148 --> 01:04:17,840
at some point in the quarter.

1527
01:04:17,840 --> 01:04:18,930
Yeah other question.

1528
01:04:18,930 --> 01:04:19,430
Yes.

1529
01:04:19,430 --> 01:04:23,930
This 128 dimensional vector,
what exactly hidden features

1530
01:04:23,930 --> 01:04:24,440
are there?

1531
01:04:24,440 --> 01:04:29,660
What are the hidden features
in the 128 dimensional vector?

1532
01:04:29,660 --> 01:04:30,650
We don't know.

1533
01:04:30,650 --> 01:04:32,180
That's the point
of deep learning

1534
01:04:32,180 --> 01:04:36,710
is you have to create
a loss function that

1535
01:04:36,710 --> 01:04:39,530
will modify your
parameters in a way that

1536
01:04:39,530 --> 01:04:41,250
forces it to learn features.

1537
01:04:41,250 --> 01:04:45,060
But I can't tell you that a
dimension number 3 is for eyes,

1538
01:04:45,060 --> 01:04:47,580
and dimension number
6 is for ears.

1539
01:04:47,580 --> 01:04:50,720
I can make a study, we'll
study it later this quarter,

1540
01:04:50,720 --> 01:04:54,160
and tell you that this
neuron is actually

1541
01:04:54,160 --> 01:04:56,870
good at detecting certain
types of features,

1542
01:04:56,870 --> 01:05:00,110
but right now, I can't tell
you unless I do that study.

1543
01:05:00,110 --> 01:05:01,380
Yeah.

1544
01:05:01,380 --> 01:05:01,880
OK.

1545
01:05:01,880 --> 01:05:02,990
So question for you.

1546
01:05:02,990 --> 01:05:06,130
How would you build a
training and a loss function

1547
01:05:06,130 --> 01:05:08,540
to make that possible
to train that network?

1548
01:05:08,540 --> 01:05:09,390
Do you have ideas?

1549
01:05:13,540 --> 01:05:15,110
It's not an easy question.

1550
01:05:15,110 --> 01:05:16,060
OK.

1551
01:05:16,060 --> 01:05:16,560
Try.

1552
01:05:21,400 --> 01:05:24,250
Where to start?

1553
01:05:24,250 --> 01:05:25,010
Yes.

1554
01:05:25,010 --> 01:05:29,020
Supporting two vectors I
think mean square error

1555
01:05:29,020 --> 01:05:30,050
is going to be better.

1556
01:05:30,050 --> 01:05:31,632
So mean squared
error between what?

1557
01:05:31,632 --> 01:05:32,590
You're right, actually.

1558
01:05:32,590 --> 01:05:35,125
Two vectors mean squared
error because it's a-- yeah.

1559
01:05:35,125 --> 01:05:35,625
But--

1560
01:05:35,625 --> 01:05:42,040
[INAUDIBLE] the loss
function probably--

1561
01:05:42,040 --> 01:05:46,120
the cost function probably
cross entropy [INAUDIBLE].

1562
01:05:46,120 --> 01:05:50,090
So are you saying we would
take pairs of pictures.

1563
01:05:50,090 --> 01:05:52,010
We would run it
through the network.

1564
01:05:52,010 --> 01:05:54,790
We will then take the
two vectors that we get

1565
01:05:54,790 --> 01:05:59,840
and apply the loss function
some distance L1 distance,

1566
01:05:59,840 --> 01:06:02,530
L2 distance, and
then trace it back

1567
01:06:02,530 --> 01:06:06,170
and say these were the same
people should have been closer.

1568
01:06:06,170 --> 01:06:07,150
That's what you mean?

1569
01:06:07,150 --> 01:06:08,310
Yeah it's a good idea.

1570
01:06:08,310 --> 01:06:10,060
Yeah someone else
wanted to say something.

1571
01:06:10,060 --> 01:06:12,330
I was gonna say it's like
the cosine similarity of two

1572
01:06:12,330 --> 01:06:14,080
vectors or any other
distance [INAUDIBLE].

1573
01:06:14,080 --> 01:06:15,122
Yeah, that's another one.

1574
01:06:15,122 --> 01:06:16,070
Cosine similarity.

1575
01:06:16,070 --> 01:06:18,520
That could also be
our loss function.

1576
01:06:18,520 --> 01:06:20,770
We could also add some
data manipulation.

1577
01:06:20,770 --> 01:06:22,040
OK like what?

1578
01:06:22,040 --> 01:06:25,750
Like create a data set
from the original picture

1579
01:06:25,750 --> 01:06:29,230
and play around with
the [INAUDIBLE].

1580
01:06:29,230 --> 01:06:30,290
Great idea.

1581
01:06:30,290 --> 01:06:31,580
Data augmentation.

1582
01:06:31,580 --> 01:06:34,900
So you say I can take
the picture of Bertrand

1583
01:06:34,900 --> 01:06:40,310
and probably mirror it,
flip it, rotate it, crop it

1584
01:06:40,310 --> 01:06:41,810
and I would use
more data that way.

1585
01:06:41,810 --> 01:06:42,920
Yeah, absolutely.

1586
01:06:42,920 --> 01:06:44,570
That would help a lot actually.

1587
01:06:44,570 --> 01:06:47,560
So all of these are good ones.

1588
01:06:47,560 --> 01:06:49,760
If I summarize
your point though,

1589
01:06:49,760 --> 01:06:52,990
because that's really
the key to the designing

1590
01:06:52,990 --> 01:06:55,000
a good loss function,
what we really

1591
01:06:55,000 --> 01:06:58,550
want is that similar
picture of the same person,

1592
01:06:58,550 --> 01:07:02,740
end up with similar vectors
and picture of different people

1593
01:07:02,740 --> 01:07:07,420
end up with different vectors if
we rephrase it in plain English.

1594
01:07:07,420 --> 01:07:11,770
So what we'll do is that we'll
build a data set of triplets.

1595
01:07:11,770 --> 01:07:14,830
The triplets includes
a picture that

1596
01:07:14,830 --> 01:07:18,122
is the anchor, a picture
that is the positive.

1597
01:07:18,122 --> 01:07:20,080
The reason it's called
positive is because it's

1598
01:07:20,080 --> 01:07:21,860
the same person as the anchor.

1599
01:07:21,860 --> 01:07:24,550
And a picture that is called
the negative because it's

1600
01:07:24,550 --> 01:07:27,010
a different person than the
anchor, and by definition

1601
01:07:27,010 --> 01:07:30,160
also a different person
than the positive.

1602
01:07:30,160 --> 01:07:32,620
Well, what if we only have
one picture of a person.

1603
01:07:32,620 --> 01:07:34,040
One picture of a person.

1604
01:07:34,040 --> 01:07:35,440
Great question.

1605
01:07:35,440 --> 01:07:39,250
If you have one picture of a
person, then you can't do that.

1606
01:07:39,250 --> 01:07:41,050
We'll actually
see another method

1607
01:07:41,050 --> 01:07:42,760
that would allow
us to do it even

1608
01:07:42,760 --> 01:07:44,780
with one picture of a person.

1609
01:07:44,780 --> 01:07:45,910
Yeah.

1610
01:07:45,910 --> 01:07:47,210
You could kind of rotate.

1611
01:07:47,210 --> 01:07:47,960
You can rotate it.

1612
01:07:47,960 --> 01:07:48,460
That's true.

1613
01:07:48,460 --> 01:07:50,390
You could actually do
some data augmentation,

1614
01:07:50,390 --> 01:07:52,570
as he was mentioning,
and build a data

1615
01:07:52,570 --> 01:07:54,620
set starting with one picture.

1616
01:07:54,620 --> 01:07:56,780
But this approach will
not be the best one.

1617
01:07:56,780 --> 01:07:59,680
We'll see another approach right
after that would work better.

1618
01:07:59,680 --> 01:08:00,910
Yeah.

1619
01:08:00,910 --> 01:08:04,670
Why are we comparing like
a vector from the model

1620
01:08:04,670 --> 01:08:06,710
from the vector
from another model

1621
01:08:06,710 --> 01:08:11,200
instead of just comparing
the output to the output?

1622
01:08:11,200 --> 01:08:12,110
A good question.

1623
01:08:12,110 --> 01:08:13,780
Why do we compare
a vector rather

1624
01:08:13,780 --> 01:08:17,302
than the output of the model?

1625
01:08:17,302 --> 01:08:19,267
So what's the
output of the model.

1626
01:08:19,267 --> 01:08:21,350
We actually haven't talked
about the architecture,

1627
01:08:21,350 --> 01:08:23,634
but I'm assuming you're
saying it's a binary number.

1628
01:08:23,634 --> 01:08:24,970
It's between 0 and 1.

1629
01:08:24,970 --> 01:08:28,689
Because it's a single dimension,
it cannot hold meaningful

1630
01:08:28,689 --> 01:08:29,810
information.

1631
01:08:29,810 --> 01:08:33,800
So you probably want to have
a vector that is big enough,

1632
01:08:33,800 --> 01:08:36,430
where you believe it
has enough flexibility

1633
01:08:36,430 --> 01:08:39,399
to hold information
that can allow

1634
01:08:39,399 --> 01:08:42,550
us to verify if the same
person is on the picture.

1635
01:08:42,550 --> 01:08:43,550
Yeah, essentially.

1636
01:08:43,550 --> 01:08:44,600
OK, I'm going to move on.

1637
01:08:44,600 --> 01:08:47,200
And if there's-- so what we
want is to minimize the encoding

1638
01:08:47,200 --> 01:08:50,120
distance between the
anchor and the positive,

1639
01:08:50,120 --> 01:08:52,899
and we want to maximize the
encoding distance between

1640
01:08:52,899 --> 01:08:54,620
the anchor and the negative.

1641
01:08:54,620 --> 01:08:56,180
So question for you.

1642
01:08:56,180 --> 01:08:59,720
What I'm going to ask you
is to take 10, 15 seconds,

1643
01:08:59,720 --> 01:09:01,130
look at the slide.

1644
01:09:01,130 --> 01:09:10,000
And you're going to start voting
for A, B or C. By the way enc

1645
01:09:10,000 --> 01:09:11,529
is encoding.

1646
01:09:11,529 --> 01:09:15,555
Is just how I call the vector
that we get out of the network.

1647
01:09:21,189 --> 01:09:27,500
A is the anchor, n is the
negative and p is the positive.

1648
01:09:43,420 --> 01:09:49,064
So A is the anchor picture N
is the negative picture, which

1649
01:09:49,064 --> 01:09:51,189
is different from the
anchor, the different person,

1650
01:09:51,189 --> 01:09:54,275
and P is the positive picture,
which is the same as the anchor.

1651
01:09:57,120 --> 01:10:01,510
And anchor of A is when you
run A through the network,

1652
01:10:01,510 --> 01:10:08,735
you get the vector anchor of A.
OK, let's look at the results.

1653
01:10:14,160 --> 01:10:25,540
A 47 for A 23 for B 3 for C. So
someone who said good job first.

1654
01:10:25,540 --> 01:10:27,780
That is correct.

1655
01:10:27,780 --> 01:10:30,610
Someone who selected A
wants to tell us why?

1656
01:10:30,610 --> 01:10:31,980
Yeah.

1657
01:10:31,980 --> 01:10:35,790
Because we're to minimize the
distance between the anchor

1658
01:10:35,790 --> 01:10:36,730
and the positive.

1659
01:10:36,730 --> 01:10:38,760
You just minimize the
distance right there.

1660
01:10:38,760 --> 01:10:40,950
But to maximize the
distance between the anchor

1661
01:10:40,950 --> 01:10:44,550
and the negative that's the
same as minimizing the negative.

1662
01:10:44,550 --> 01:10:45,740
Correct Correct.

1663
01:10:45,740 --> 01:10:48,610
Great so actually the
keyword here is minimize.

1664
01:10:48,610 --> 01:10:50,327
If I had said maximize
the answer indeed,

1665
01:10:50,327 --> 01:10:52,410
as you say would have been
different because here,

1666
01:10:52,410 --> 01:10:55,230
we're looking at minimizing
the distance between the anchor

1667
01:10:55,230 --> 01:10:56,070
and the positive.

1668
01:10:56,070 --> 01:11:02,800
And in fact minimizing this or
maximizing the opposite of it

1669
01:11:02,800 --> 01:11:05,520
that's why the answer is a.

1670
01:11:05,520 --> 01:11:08,010
OK good stuff.

1671
01:11:08,010 --> 01:11:10,050
Let's keep going.

1672
01:11:10,050 --> 01:11:15,180
So going back to the
initial setup, we had a cat

1673
01:11:15,180 --> 01:11:17,130
and we were predicting
a binary number.

1674
01:11:17,130 --> 01:11:19,260
Here instead, we
have three pictures

1675
01:11:19,260 --> 01:11:21,670
going through the
network in parallel

1676
01:11:21,670 --> 01:11:23,518
so you can imagine
it's batch processing.

1677
01:11:23,518 --> 01:11:25,560
It's like the three are
going in the same network

1678
01:11:25,560 --> 01:11:26,680
at the same time.

1679
01:11:26,680 --> 01:11:28,810
And then you're
getting three vectors.

1680
01:11:28,810 --> 01:11:32,110
You're computing the
loss function OK.

1681
01:11:32,110 --> 01:11:34,980
You're doing this loss
function we talked about.

1682
01:11:34,980 --> 01:11:37,090
I'm not going to talk here
about the alpha number

1683
01:11:37,090 --> 01:11:38,882
but you're going to
learn when you build it

1684
01:11:38,882 --> 01:11:41,580
why the alpha number matters.

1685
01:11:41,580 --> 01:11:45,180
Hint is maybe 0 would have been
a correct answer if you didn't

1686
01:11:45,180 --> 01:11:48,240
have the alpha number,
so it would have created

1687
01:11:48,240 --> 01:11:49,920
instability in the model.

1688
01:11:49,920 --> 01:11:51,730
But you do that
many, many times.

1689
01:11:51,730 --> 01:11:54,100
You push the parameters
to the right or the left.

1690
01:11:54,100 --> 01:11:56,670
And because of the way you
created your loss function

1691
01:11:56,670 --> 01:11:59,970
and your data labeling,
the way you structured

1692
01:11:59,970 --> 01:12:03,630
your data and the loss
function, essentially the model

1693
01:12:03,630 --> 01:12:07,440
is going to learn by itself
to create similar encoding

1694
01:12:07,440 --> 01:12:09,280
for pictures that are
of the same person,

1695
01:12:09,280 --> 01:12:11,520
and separate encodings
for pictures that

1696
01:12:11,520 --> 01:12:13,090
are not from the same person.

1697
01:12:13,090 --> 01:12:15,070
And you didn't need to
do feature engineering,

1698
01:12:15,070 --> 01:12:16,950
you didn't need to talk
about eyes and ears

1699
01:12:16,950 --> 01:12:18,940
and whatever because
it will figure it out.

1700
01:12:18,940 --> 01:12:21,270
You know that you created
the learning environment

1701
01:12:21,270 --> 01:12:22,385
to allow that to happen.

1702
01:12:25,890 --> 01:12:29,550
So congratulations, you designed
your first loss function,

1703
01:12:29,550 --> 01:12:33,030
and we're going to design
many more in this course.

1704
01:12:33,030 --> 01:12:35,110
This, by the way,
is from facenet.

1705
01:12:35,110 --> 01:12:38,500
It's a paper from 2015
from Schrott et al.

1706
01:12:38,500 --> 01:12:40,750
And you'll see in
the slides I used,

1707
01:12:40,750 --> 01:12:44,190
I always put the reference
to the papers in case

1708
01:12:44,190 --> 01:12:46,210
you want to go back and
study the actual paper.

1709
01:12:46,210 --> 01:12:47,890
Many students do it
for their projects.

1710
01:12:47,890 --> 01:12:48,880
This is a great one.

1711
01:12:48,880 --> 01:12:50,850
Great, great paper to look.

1712
01:12:50,850 --> 01:12:53,220
A lot of citations as well.

1713
01:12:53,220 --> 01:12:56,290
Let me make it slightly
complicated-- more complicated.

1714
01:12:56,290 --> 01:12:58,020
We learned face
verification, now

1715
01:12:58,020 --> 01:13:00,670
we want to do face
identification.

1716
01:13:00,670 --> 01:13:02,550
How is that different?

1717
01:13:02,550 --> 01:13:07,380
Identification is a
school wants to use--

1718
01:13:07,380 --> 01:13:09,220
to recognize students
in facilities.

1719
01:13:09,220 --> 01:13:12,670
So imagine face verification
is you swipe your card

1720
01:13:12,670 --> 01:13:15,272
and then that
picture was compared

1721
01:13:15,272 --> 01:13:16,480
to the picture of the camera.

1722
01:13:16,480 --> 01:13:17,770
That's verification.

1723
01:13:17,770 --> 01:13:19,690
The two are they
the same or not?

1724
01:13:19,690 --> 01:13:23,490
Identification is
you have this picture

1725
01:13:23,490 --> 01:13:25,180
in the database somewhere.

1726
01:13:25,180 --> 01:13:28,980
The person enters immediately
you can identify them.

1727
01:13:28,980 --> 01:13:32,460
So the difference for those
of you who fly in the US

1728
01:13:32,460 --> 01:13:37,670
is when you go
through global entry,

1729
01:13:37,670 --> 01:13:40,410
many people don't even need to
put their passport or anything.

1730
01:13:40,410 --> 01:13:42,750
They just want to look at
the camera and they move on.

1731
01:13:42,750 --> 01:13:44,310
That's identification.

1732
01:13:44,310 --> 01:13:47,160
But actually, when you're
in Europe, for example,

1733
01:13:47,160 --> 01:13:50,670
you put your passport
in, then you walk in,

1734
01:13:50,670 --> 01:13:53,060
then it takes a picture,
that's verification.

1735
01:13:53,060 --> 01:13:54,300
You see the difference or no.

1736
01:13:54,300 --> 01:13:55,107
Yeah.

1737
01:13:55,107 --> 01:13:59,330
You choosing for verification
the negative, for the input, how

1738
01:13:59,330 --> 01:14:01,430
would you [INAUDIBLE]?

1739
01:14:01,430 --> 01:14:04,550
The negative or how do
you create those triplets

1740
01:14:04,550 --> 01:14:05,450
essentially?

1741
01:14:05,450 --> 01:14:06,995
When you have to
do it in real time?

1742
01:14:06,995 --> 01:14:08,762
No in real time--
that's a great question.

1743
01:14:08,762 --> 01:14:09,720
I didn't talk about it.

1744
01:14:09,720 --> 01:14:12,230
So at train time
you have a databases

1745
01:14:12,230 --> 01:14:14,430
and you create the
triplets automatically.

1746
01:14:14,430 --> 01:14:16,590
Like you pick pictures
from the same person,

1747
01:14:16,590 --> 01:14:19,190
or you use data augmentation,
and you add a random picture

1748
01:14:19,190 --> 01:14:20,130
from someone else.

1749
01:14:20,130 --> 01:14:22,140
You create millions
of triplets like that,

1750
01:14:22,140 --> 01:14:23,300
or billions of triplets.

1751
01:14:23,300 --> 01:14:28,700
At test time, you only take
the picture from the camera,

1752
01:14:28,700 --> 01:14:30,720
run it-- you don't
use the negative.

1753
01:14:30,720 --> 01:14:32,700
You just take the
picture from the camera.

1754
01:14:32,700 --> 01:14:34,260
You run it through the network.

1755
01:14:34,260 --> 01:14:35,420
The person swipes.

1756
01:14:35,420 --> 01:14:37,320
You, take the picture
from the swipe,

1757
01:14:37,320 --> 01:14:38,610
run it through the network.

1758
01:14:38,610 --> 01:14:40,650
You do the comparison,
you let them in or not.

1759
01:14:40,650 --> 01:14:44,310
So there's no more negative
at test time in practice.

1760
01:14:44,310 --> 01:14:47,180
It's just a trick
to train the model.

1761
01:14:47,180 --> 01:14:49,730
OK, so how would you
do face identification

1762
01:14:49,730 --> 01:14:52,020
using what we learned
for face verification.

1763
01:14:52,020 --> 01:14:54,920
Is there any small
tweak you can make

1764
01:14:54,920 --> 01:14:57,836
that would make this network
work for identification?

1765
01:15:03,710 --> 01:15:04,490
Yes.

1766
01:15:04,490 --> 01:15:07,310
[INAUDIBLE]

1767
01:15:11,863 --> 01:15:12,363
Correct.

1768
01:15:12,363 --> 01:15:13,050
Correct.

1769
01:15:13,050 --> 01:15:14,633
What is it called
in machine learning?

1770
01:15:18,560 --> 01:15:20,060
There's a machine
learning algorithm

1771
01:15:20,060 --> 01:15:23,330
that we can stack on
top of what we just did.

1772
01:15:23,330 --> 01:15:25,773
He said you can compare--

1773
01:15:25,773 --> 01:15:27,690
so because we don't have
two pictures anymore,

1774
01:15:27,690 --> 01:15:29,610
we just have one
from the camera.

1775
01:15:29,610 --> 01:15:33,150
You just compare the vector of--
you run this by the network,

1776
01:15:33,150 --> 01:15:34,100
you get the vector.

1777
01:15:34,100 --> 01:15:36,586
And then you compare
it to the database--

1778
01:15:36,586 --> 01:15:38,330
[INAUDIBLE]

1779
01:15:38,330 --> 01:15:40,350
No, but you try.

1780
01:15:40,350 --> 01:15:42,450
You have a database of
all the student pictures.

1781
01:15:42,450 --> 01:15:44,550
You run everything
through the network.

1782
01:15:44,550 --> 01:15:45,980
Instead of storing
the image, you

1783
01:15:45,980 --> 01:15:49,040
store the vectors and
then someone shows up

1784
01:15:49,040 --> 01:15:50,910
and you're looking
in the database.

1785
01:15:50,910 --> 01:15:54,020
Is there any vector that
is super close to this one.

1786
01:15:54,020 --> 01:15:55,500
That's identification.

1787
01:15:55,500 --> 01:16:00,350
What is this algorithm
called in machine learning?

1788
01:16:00,350 --> 01:16:03,069
It's pretty simple algorithm.

1789
01:16:03,069 --> 01:16:04,245
[INAUDIBLE]

1790
01:16:04,245 --> 01:16:04,745
No.

1791
01:16:08,150 --> 01:16:09,600
OK I'm going to make it easier.

1792
01:16:09,600 --> 01:16:11,810
What if instead of
having one picture

1793
01:16:11,810 --> 01:16:15,260
of a student in the database,
you had three of each student.

1794
01:16:15,260 --> 01:16:18,170
You have three vectors
for each person.

1795
01:16:18,170 --> 01:16:20,630
And then you're trying
to find the nearest

1796
01:16:20,630 --> 01:16:25,460
vectors in the database from
the one that the camera takes.

1797
01:16:25,460 --> 01:16:26,330
I used the keyword.

1798
01:16:26,330 --> 01:16:27,200
[INAUDIBLE]

1799
01:16:27,200 --> 01:16:28,490
No.

1800
01:16:28,490 --> 01:16:29,750
K-Nearest neighbor.

1801
01:16:29,750 --> 01:16:31,280
Yeah.

1802
01:16:31,280 --> 01:16:32,220
K-Nearest neighbors.

1803
01:16:32,220 --> 01:16:34,170
That's a K-Nearest
neighbor algorithm.

1804
01:16:34,170 --> 01:16:39,870
It's essentially-- you want
to explain what you meant.

1805
01:16:39,870 --> 01:16:42,620
Why is it K-Nearest neighbor?

1806
01:16:42,620 --> 01:16:45,960
Well, I was just thinking about
what's the nearest vector--

1807
01:16:45,960 --> 01:16:46,460
Yeah.

1808
01:16:46,460 --> 01:16:49,320
--closest to the
vector [INAUDIBLE].

1809
01:16:49,320 --> 01:16:49,820
Yeah.

1810
01:16:49,820 --> 01:16:53,040
It's K-Nearest neighbor for
high dimensional vectors.

1811
01:16:53,040 --> 01:16:55,430
So here is a simple example
of K-Nearest neighbor

1812
01:16:55,430 --> 01:16:56,520
per two dimensions.

1813
01:16:56,520 --> 01:16:59,510
In practice, it's 128
dimensions so I can't put it

1814
01:16:59,510 --> 01:17:00,750
on a slide, of course.

1815
01:17:00,750 --> 01:17:03,720
But let's say in green,
you have the query point.

1816
01:17:03,720 --> 01:17:07,130
The query point is the
camera picture, OK.

1817
01:17:07,130 --> 01:17:11,180
And then you run a nearest
neighbor algorithm,

1818
01:17:11,180 --> 01:17:15,410
and you say, are there three
vectors in the database that

1819
01:17:15,410 --> 01:17:17,880
are close to this vector.

1820
01:17:17,880 --> 01:17:20,270
And you can add additional
checks are these three

1821
01:17:20,270 --> 01:17:22,430
vectors from the same person.

1822
01:17:22,430 --> 01:17:24,980
If they are, then it's very
likely the person is correct

1823
01:17:24,980 --> 01:17:28,370
because you just prove
that the three closest

1824
01:17:28,370 --> 01:17:31,570
vectors in the database are
from three the same person

1825
01:17:31,570 --> 01:17:33,790
three times, so it's
higher likelihood.

1826
01:17:33,790 --> 01:17:35,767
You could even do it
for 10 nearest neighbor

1827
01:17:35,767 --> 01:17:37,100
if you want to be really secure.

1828
01:17:37,100 --> 01:17:39,100
Let's say you go to
the airport every time.

1829
01:17:39,100 --> 01:17:41,030
And every time they
take a picture of you,

1830
01:17:41,030 --> 01:17:44,840
and now they can do a 10
nearest neighbor on that search.

1831
01:17:44,840 --> 01:17:46,360
Does that make sense?

1832
01:17:46,360 --> 01:17:48,260
Now it's slightly
more complicated,

1833
01:17:48,260 --> 01:17:49,760
you want to do face clustering.

1834
01:17:49,760 --> 01:17:52,450
So in your phone,
sometimes, it says

1835
01:17:52,450 --> 01:17:55,990
it put automatically all
the pictures from your mom

1836
01:17:55,990 --> 01:17:59,672
in one folder and from
your dad in another folder.

1837
01:17:59,672 --> 01:18:01,010
How does it do it?

1838
01:18:01,010 --> 01:18:03,790
How could you make a tweak
to again, what we created

1839
01:18:03,790 --> 01:18:05,030
or encoding network?

1840
01:18:05,030 --> 01:18:07,946
How can you use
that to create that?

1841
01:18:07,946 --> 01:18:09,820
K-Means.

1842
01:18:09,820 --> 01:18:10,880
K-Means yeah, exactly.

1843
01:18:10,880 --> 01:18:13,570
K-Means algorithm, which
is an unsupervised learning

1844
01:18:13,570 --> 01:18:14,890
algorithm clustering.

1845
01:18:14,890 --> 01:18:17,060
So you have a bunch of pictures.

1846
01:18:17,060 --> 01:18:19,790
You have vectorized all of them
with the network you trained.

1847
01:18:19,790 --> 01:18:22,420
And normally the vectors
that are from the same person

1848
01:18:22,420 --> 01:18:24,320
should be clustered
around the same place.

1849
01:18:24,320 --> 01:18:28,305
And that's very simply how big
companies do it on your phone.

1850
01:18:32,830 --> 01:18:33,758
Yes.

1851
01:18:33,758 --> 01:18:35,800
What happens if the person's
not on the database?

1852
01:18:35,800 --> 01:18:39,380
Yeah so if the person
is not in the database,

1853
01:18:39,380 --> 01:18:42,100
then you shouldn't
find any vector that

1854
01:18:42,100 --> 01:18:44,510
is close to the vector
you're taking a picture of.

1855
01:18:44,510 --> 01:18:47,050
The closest vector might
be above your threshold

1856
01:18:47,050 --> 01:18:50,110
in terms of distance, and you
wouldn't let that person in.

1857
01:18:50,110 --> 01:18:51,760
Yeah.

1858
01:18:51,760 --> 01:18:54,850
I think that's why they make
you, sign up for global entry.

1859
01:18:54,850 --> 01:18:55,580
Yeah for sure.

1860
01:18:55,580 --> 01:18:56,770
Yeah, they make you sign up.

1861
01:18:56,770 --> 01:18:59,800
So it's interesting because
companies know that they

1862
01:18:59,800 --> 01:19:01,790
need to build these algorithms.

1863
01:19:01,790 --> 01:19:04,690
And then some-- the
admission process the sign up

1864
01:19:04,690 --> 01:19:06,530
process might include
certain data points.

1865
01:19:06,530 --> 01:19:08,238
And now you're starting
to understand how

1866
01:19:08,238 --> 01:19:10,190
it's used in the background.

1867
01:19:10,190 --> 01:19:12,320
OK Let's move on.

1868
01:19:12,320 --> 01:19:13,190
Yeah one question.

1869
01:19:13,190 --> 01:19:16,285
Are you comparing every new
example with each vector

1870
01:19:16,285 --> 01:19:19,120
or with a centroid?

1871
01:19:19,120 --> 01:19:20,540
Oh, a good question.

1872
01:19:20,540 --> 01:19:23,090
So are you comparing
each new picture?

1873
01:19:23,090 --> 01:19:28,150
So I take a picture of
my mom with my phone.

1874
01:19:28,150 --> 01:19:29,270
What's going to happen?

1875
01:19:29,270 --> 01:19:30,987
This picture is
going to likely--

1876
01:19:30,987 --> 01:19:33,070
if you're doing clustering
is going to be compared

1877
01:19:33,070 --> 01:19:34,670
to the centroid of my mom.

1878
01:19:34,670 --> 01:19:37,700
So the phone keeps probably
a centroid of my mom.

1879
01:19:37,700 --> 01:19:40,810
And if it's close enough to the
centroid over another centroid

1880
01:19:40,810 --> 01:19:44,622
it's going to probably put it
in that folder, essentially.

1881
01:19:44,622 --> 01:19:46,580
Yeah one more question,
and then we'll move on.

1882
01:19:46,580 --> 01:19:48,880
Yeah what algorithm do
you use to determine

1883
01:19:48,880 --> 01:19:51,190
how many centroids you want?

1884
01:19:51,190 --> 01:19:53,470
Yeah there is-- how
do you figure out

1885
01:19:53,470 --> 01:19:54,830
how many centroids you want?

1886
01:19:54,830 --> 01:19:56,440
There is an algorithm.

1887
01:19:56,440 --> 01:20:04,810
You'll study it in
CS230 not in CS230.

1888
01:20:04,810 --> 01:20:08,300
What we learned here is
what an encoder network is.

1889
01:20:08,300 --> 01:20:12,230
We learned about positive anchor
negative for the triplet loss.

1890
01:20:12,230 --> 01:20:15,220
The loss I showed you is
called the triplet loss because

1891
01:20:15,220 --> 01:20:16,130
of the triplets.

1892
01:20:16,130 --> 01:20:18,010
And then we learned the
different variations

1893
01:20:18,010 --> 01:20:20,690
of face verification
identification and clustering.

1894
01:20:20,690 --> 01:20:24,520
Now we're going to get to an
interesting section, which

1895
01:20:24,520 --> 01:20:27,980
is brand new around
self-supervised learning.

1896
01:20:27,980 --> 01:20:31,360
So note that everything we
did so far, the day and night

1897
01:20:31,360 --> 01:20:36,520
classification, the trigger word
detection, and the triplet loss

1898
01:20:36,520 --> 01:20:39,160
were supervised learning.

1899
01:20:39,160 --> 01:20:41,270
We had labels, essentially.

1900
01:20:41,270 --> 01:20:43,430
Day and night is very
classic supervised learning.

1901
01:20:43,430 --> 01:20:45,170
You label data with 0 and 1.

1902
01:20:45,170 --> 01:20:47,630
Same for trigger word
detection, face verification.

1903
01:20:47,630 --> 01:20:50,600
You can debate-- it
can be different.

1904
01:20:50,600 --> 01:20:53,330
But anyway, we focused
on supervised learning.

1905
01:20:53,330 --> 01:20:57,170
Now we're going to talk about
self-supervised learning.

1906
01:20:57,170 --> 01:21:01,270
And my question for
you is the following.

1907
01:21:01,270 --> 01:21:02,750
Labeling is expensive.

1908
01:21:02,750 --> 01:21:03,770
We know that.

1909
01:21:03,770 --> 01:21:09,430
So how would you redo what we
did with a different approach

1910
01:21:09,430 --> 01:21:12,820
that does not require labels?

1911
01:21:12,820 --> 01:21:15,590
Meaning you remember even
in face verification,

1912
01:21:15,590 --> 01:21:20,060
we had the name of the students
in the database with their face,

1913
01:21:20,060 --> 01:21:22,822
and we might have
multiple pictures of them.

1914
01:21:22,822 --> 01:21:24,280
Let's say you don't
even have that.

1915
01:21:24,280 --> 01:21:29,850
You just have faces
in the wild unlabeled.

1916
01:21:29,850 --> 01:21:34,050
How would you do
things differently?

1917
01:21:34,050 --> 01:21:34,930
Any idea?

1918
01:21:34,930 --> 01:21:35,670
Yeah.

1919
01:21:35,670 --> 01:21:37,590
So that's neural
network to decide

1920
01:21:37,590 --> 01:21:40,290
like to point out which
images are close to each other

1921
01:21:40,290 --> 01:21:41,310
and [INAUDIBLE].

1922
01:21:41,310 --> 01:21:42,360
OK.

1923
01:21:42,360 --> 01:21:44,790
Let the neural network
find the pictures that

1924
01:21:44,790 --> 01:21:47,500
are close to each other, but how
would you train that network?

1925
01:21:47,500 --> 01:21:49,950
You're starting with a network
that doesn't do anything

1926
01:21:49,950 --> 01:21:51,533
and you give it an
image, it gives you

1927
01:21:51,533 --> 01:21:52,930
a random vector at first.

1928
01:21:52,930 --> 01:21:54,730
So how would you train it?

1929
01:21:54,730 --> 01:21:55,800
Yeah.

1930
01:21:55,800 --> 01:21:57,870
Do some kind of
clustering offline,

1931
01:21:57,870 --> 01:22:02,160
and then use that as-- use
those centroids as your labels.

1932
01:22:02,160 --> 01:22:03,555
Do some clustering offline.

1933
01:22:03,555 --> 01:22:05,680
But again, my question is
the clustering algorithm.

1934
01:22:05,680 --> 01:22:06,580
How is it trained?

1935
01:22:06,580 --> 01:22:10,440
How do you cluster if you
don't have any encoder network?

1936
01:22:10,440 --> 01:22:12,720
Because the clustering
came after we trained

1937
01:22:12,720 --> 01:22:14,110
the encoder network.

1938
01:22:14,110 --> 01:22:16,860
The clustering only worked
because we had a good encoder

1939
01:22:16,860 --> 01:22:17,760
network.

1940
01:22:17,760 --> 01:22:20,100
But if you have for
example, a bunch

1941
01:22:20,100 --> 01:22:23,160
of pictures of the same
person, and then you

1942
01:22:23,160 --> 01:22:24,498
run that through the network.

1943
01:22:24,498 --> 01:22:26,290
But you don't know if
it's the same person.

1944
01:22:26,290 --> 01:22:27,070
That's what I'm saying is like--

1945
01:22:27,070 --> 01:22:28,140
Vectors would be similar.

1946
01:22:28,140 --> 01:22:30,520
No, because that's the
network you're training.

1947
01:22:30,520 --> 01:22:32,422
The vectors are not
similar because that's

1948
01:22:32,422 --> 01:22:33,630
the network we want to train.

1949
01:22:33,630 --> 01:22:36,070
Right now I gave you a network.

1950
01:22:36,070 --> 01:22:37,510
It's completely random.

1951
01:22:37,510 --> 01:22:39,720
You give it my
picture on Saturday

1952
01:22:39,720 --> 01:22:43,140
and on Sunday the vectors
are completely off.

1953
01:22:43,140 --> 01:22:44,350
So how do you start?

1954
01:22:44,350 --> 01:22:45,250
Yeah, yeah.

1955
01:22:45,250 --> 01:22:46,532
You want it.

1956
01:22:46,532 --> 01:22:48,690
You could do a carbon copy.

1957
01:22:48,690 --> 01:22:50,330
OK, tell me more.

1958
01:22:50,330 --> 01:22:55,260
You could just train the
model to [INAUDIBLE] image.

1959
01:22:55,260 --> 01:22:57,340
First create a layer of
representation of that.

1960
01:22:57,340 --> 01:23:01,650
And then given the
label representation,

1961
01:23:01,650 --> 01:23:04,530
and then another model that
given the label representation.

1962
01:23:04,530 --> 01:23:05,280
OK, OK.

1963
01:23:05,280 --> 01:23:06,520
Yeah you're ahead.

1964
01:23:06,520 --> 01:23:09,130
But we'll study that
in two weeks actually.

1965
01:23:09,130 --> 01:23:14,190
So we'll do a two encoders
two weeks from now in class.

1966
01:23:14,190 --> 01:23:15,820
Anyone else has an idea?

1967
01:23:15,820 --> 01:23:16,410
Yeah.

1968
01:23:16,410 --> 01:23:19,386
[INAUDIBLE]

1969
01:23:28,860 --> 01:23:31,560
It's actually similar
to what he was saying.

1970
01:23:31,560 --> 01:23:33,422
To reconstruct the
original image.

1971
01:23:33,422 --> 01:23:34,630
Yeah, that's what we learned.

1972
01:23:34,630 --> 01:23:36,640
There's a lot of methods.

1973
01:23:36,640 --> 01:23:38,980
Diffusion models work like that.

1974
01:23:38,980 --> 01:23:42,000
Autoencoders we learn
about that in two weeks

1975
01:23:42,000 --> 01:23:43,660
when we focus on generative AI.

1976
01:23:43,660 --> 01:23:45,450
Generative modeling.

1977
01:23:45,450 --> 01:23:51,400
Here I want to present
also a generative method,

1978
01:23:51,400 --> 01:23:54,330
but it's really interesting
because it will be your Foray

1979
01:23:54,330 --> 01:23:56,110
into self-supervised learning.

1980
01:23:56,110 --> 01:23:57,340
Here is the idea.

1981
01:23:57,340 --> 01:23:59,400
If we have pictures in the wild.

1982
01:23:59,400 --> 01:24:01,950
Going with the methods you
had mentioned around data

1983
01:24:01,950 --> 01:24:05,610
augmentation, you can
actually force the network

1984
01:24:05,610 --> 01:24:07,810
to learn from the data itself.

1985
01:24:07,810 --> 01:24:09,490
So let me give you an example.

1986
01:24:09,490 --> 01:24:12,210
I take-- you look at
the picture of this dog

1987
01:24:12,210 --> 01:24:15,370
and you rotate it by 90 degrees.

1988
01:24:15,370 --> 01:24:17,820
It's still the same dog.

1989
01:24:17,820 --> 01:24:20,290
A human would say
it's the same dog.

1990
01:24:20,290 --> 01:24:21,910
What are we using in our brain?

1991
01:24:21,910 --> 01:24:24,660
We're using the ability to
understand rotation invariance

1992
01:24:24,660 --> 01:24:26,800
and to understand the
semantics of the dog.

1993
01:24:26,800 --> 01:24:32,220
And so technically, if you gave
those two images to the network,

1994
01:24:32,220 --> 01:24:35,220
you could create a loss function
that compares those two pairs

1995
01:24:35,220 --> 01:24:39,030
and has to have vectors that
are close to each other.

1996
01:24:39,030 --> 01:24:40,770
Does that make sense?

1997
01:24:40,770 --> 01:24:43,060
The other thing you can
do you can do a patch.

1998
01:24:43,060 --> 01:24:45,000
You can literally take
an image of a face

1999
01:24:45,000 --> 01:24:49,950
and put a patch on
half of the image.

2000
01:24:49,950 --> 01:24:52,257
And then you say--

2001
01:24:52,257 --> 01:24:54,340
and then you do the same
thing on the other image.

2002
01:24:54,340 --> 01:24:56,140
You put the other
patch, the other half.

2003
01:24:56,140 --> 01:24:58,050
And now you tell the
network, these two

2004
01:24:58,050 --> 01:25:00,930
should have the same
vector, pretty much.

2005
01:25:00,930 --> 01:25:04,050
So you use your
data augmentation

2006
01:25:04,050 --> 01:25:08,640
scheme on massive data sets
online to force the network

2007
01:25:08,640 --> 01:25:12,870
to learn from the data itself.

2008
01:25:12,870 --> 01:25:16,200
OK no need to forge
triplets, per se.

2009
01:25:16,200 --> 01:25:17,790
You just take a picture.

2010
01:25:17,790 --> 01:25:21,870
You make a variance of it
with noise, with rotation,

2011
01:25:21,870 --> 01:25:24,960
with cropping, with translation,
with whatever you want.

2012
01:25:24,960 --> 01:25:27,360
And then you put these two
in the data set and you say,

2013
01:25:27,360 --> 01:25:28,610
these are two the same person.

2014
01:25:28,610 --> 01:25:30,350
It should have the same vector.

2015
01:25:30,350 --> 01:25:31,610
Does that make sense?

2016
01:25:31,610 --> 01:25:34,430
That's why it's called
self-supervised learning.

2017
01:25:34,430 --> 01:25:37,670
Because you don't
have labels, you just

2018
01:25:37,670 --> 01:25:40,520
create a learning
environment that

2019
01:25:40,520 --> 01:25:45,710
makes the network learn from the
patterns of the data directly

2020
01:25:45,710 --> 01:25:48,500
itself.

2021
01:25:48,500 --> 01:25:51,210
So this is an example
called SimCLR.

2022
01:25:51,210 --> 01:25:53,670
Again the paper is right there.

2023
01:25:53,670 --> 01:25:58,310
And this shift from
supervised triplets,

2024
01:25:58,310 --> 01:26:04,020
facenet which was a paper from
2015 to self-supervised pairs.

2025
01:26:04,020 --> 01:26:08,120
That is why modern models
are trained on billions

2026
01:26:08,120 --> 01:26:11,330
of unlabeled images.

2027
01:26:11,330 --> 01:26:14,740
That's how we create-- it's much
simpler when you think about it.

2028
01:26:14,740 --> 01:26:18,090
You can literally write
a script and scrape

2029
01:26:18,090 --> 01:26:20,280
and it will label--
auto label the images

2030
01:26:20,280 --> 01:26:22,840
and put them in
pairs, do variations,

2031
01:26:22,840 --> 01:26:26,910
and then you'll end up with a
very powerful pre-trained model,

2032
01:26:26,910 --> 01:26:29,610
much simpler than people think.

2033
01:26:29,610 --> 01:26:31,510
It's not that hard at
the end of the day.

2034
01:26:31,510 --> 01:26:33,843
Most of the complexities are
going to come from compute.

2035
01:26:36,570 --> 01:26:39,960
So this method is called
contrastive learning.

2036
01:26:39,960 --> 01:26:44,370
We're going to talk about it
a little more in two weeks.

2037
01:26:44,370 --> 01:26:47,650
Self-supervision is not
only an image thing,

2038
01:26:47,650 --> 01:26:49,870
it's also used in
other modalities.

2039
01:26:49,870 --> 01:26:53,440
For example in texts the
principle is the same.

2040
01:26:53,440 --> 01:26:56,820
You predict what belongs
together and you push away what

2041
01:26:56,820 --> 01:26:57,640
doesn't.

2042
01:26:57,640 --> 01:26:58,960
That's for images.

2043
01:26:58,960 --> 01:27:00,630
And here.

2044
01:27:00,630 --> 01:27:03,480
What the core of
GPT, some of you

2045
01:27:03,480 --> 01:27:06,840
probably have heard of
that is a method called

2046
01:27:06,840 --> 01:27:08,320
next token prediction.

2047
01:27:08,320 --> 01:27:10,950
We're going to learn later
about tokens in the class.

2048
01:27:10,950 --> 01:27:13,650
But today, forget about
tokens, just think the words.

2049
01:27:13,650 --> 01:27:15,290
We're trying to
look at a sentence

2050
01:27:15,290 --> 01:27:17,280
and predict the next word.

2051
01:27:17,280 --> 01:27:20,640
Why is this
self-supervised learning?

2052
01:27:20,640 --> 01:27:21,990
Because you don't label data.

2053
01:27:21,990 --> 01:27:24,120
You just literally
grab data from online,

2054
01:27:24,120 --> 01:27:26,750
and you create a scheme
that forces the model

2055
01:27:26,750 --> 01:27:28,550
to learn from the
patterns of the data

2056
01:27:28,550 --> 01:27:30,870
using self-supervised learning.

2057
01:27:30,870 --> 01:27:33,230
But the self comes from
the fact that you didn't

2058
01:27:33,230 --> 01:27:36,050
label it manually yourself.

2059
01:27:36,050 --> 01:27:38,070
So let's do a few examples.

2060
01:27:38,070 --> 01:27:41,150
And the reason I want to do
the examples is because we want

2061
01:27:41,150 --> 01:27:45,200
to talk about emerging
emergent behaviors that

2062
01:27:45,200 --> 01:27:47,460
stem from the tasks we defined.

2063
01:27:47,460 --> 01:27:50,520
So give me the word
you're thinking of.

2064
01:27:50,520 --> 01:27:54,140
I poured myself a cup of--

2065
01:27:54,140 --> 01:27:56,390
some people said tea, coffee.

2066
01:27:56,390 --> 01:27:58,560
Anybody said anything else?

2067
01:27:58,560 --> 01:27:59,060
Water.

2068
01:27:59,060 --> 01:28:00,570
Water, a cup of water.

2069
01:28:00,570 --> 01:28:02,270
Healthy people said water.

2070
01:28:02,270 --> 01:28:03,900
OK yeah.

2071
01:28:03,900 --> 01:28:07,220
So what's the emergent
behavior that you

2072
01:28:07,220 --> 01:28:09,900
can expect the model
is going to learn

2073
01:28:09,900 --> 01:28:11,533
based on just that example?

2074
01:28:16,080 --> 01:28:18,210
[INAUDIBLE]

2075
01:28:18,210 --> 01:28:22,020
A good point, because you
that whatever is here first

2076
01:28:22,020 --> 01:28:25,170
fits in a cup, so
it understands that.

2077
01:28:25,170 --> 01:28:26,860
The second reason is poured.

2078
01:28:26,860 --> 01:28:28,030
So it's a liquid.

2079
01:28:28,030 --> 01:28:30,540
So just this sentence,
without even labeling,

2080
01:28:30,540 --> 01:28:33,570
is going to generate emergent
behaviors that we've never

2081
01:28:33,570 --> 01:28:35,020
trained the model for.

2082
01:28:35,020 --> 01:28:40,462
That's what's interesting
about modern AI, if you will.

2083
01:28:40,462 --> 01:28:42,125
You don't need to
define the tasks.

2084
01:28:45,720 --> 01:28:47,470
Frankly, the same way.

2085
01:28:47,470 --> 01:28:49,287
Think about face verification.

2086
01:28:49,287 --> 01:28:51,370
Back in the days, we used
to do what I showed you,

2087
01:28:51,370 --> 01:28:53,190
where we would create
triplets, and we

2088
01:28:53,190 --> 01:28:56,560
would be very specific about
this is for face verification.

2089
01:28:56,560 --> 01:28:59,370
You could actually scrape
all the images online

2090
01:28:59,370 --> 01:29:02,320
and do the contrastive
learning that I showed you next

2091
01:29:02,320 --> 01:29:04,380
and it will still be
good at detecting faces

2092
01:29:04,380 --> 01:29:07,110
without you having even defined
that task in the first place,

2093
01:29:07,110 --> 01:29:10,720
just by doing the
contrastive prediction.

2094
01:29:10,720 --> 01:29:11,220
OK.

2095
01:29:11,220 --> 01:29:11,940
First example.

2096
01:29:11,940 --> 01:29:13,950
Also, again, I was
trying to predict,

2097
01:29:13,950 --> 01:29:15,690
but people usually
say different things.

2098
01:29:15,690 --> 01:29:17,640
I think the majority
of people think coffee.

2099
01:29:17,640 --> 01:29:18,810
It's very cultural.

2100
01:29:18,810 --> 01:29:21,920
You go in another country,
it's going to be tea for sure.

2101
01:29:21,920 --> 01:29:26,000
And that forces the
model to really think

2102
01:29:26,000 --> 01:29:32,310
about everyday co-occurrence
pattern like them, being liquid,

2103
01:29:32,310 --> 01:29:34,740
being of a certain size
occurring together.

2104
01:29:34,740 --> 01:29:37,700
So for example, there's probably
a lot of sentences online that

2105
01:29:37,700 --> 01:29:40,610
says pouring a cup of
tea and there's a lot

2106
01:29:40,610 --> 01:29:42,385
saying pouring a cup of coffee.

2107
01:29:42,385 --> 01:29:43,760
Because of that,
the model should

2108
01:29:43,760 --> 01:29:46,468
understand that these two things
are probably close to each other

2109
01:29:46,468 --> 01:29:48,290
because their
context is similar.

2110
01:29:48,290 --> 01:29:52,225
Second example, the
capital of France is--

2111
01:29:55,144 --> 01:29:55,644
Paris.

2112
01:29:58,610 --> 01:29:59,750
Paris, OK.

2113
01:29:59,750 --> 01:30:05,110
What's the emergent behavior you
can expect the model to learn?

2114
01:30:05,110 --> 01:30:06,360
[INAUDIBLE]

2115
01:30:06,360 --> 01:30:06,860
Yeah.

2116
01:30:06,860 --> 01:30:08,230
Learn about facts, exactly.

2117
01:30:08,230 --> 01:30:11,110
So this is really predicting
the next token forces

2118
01:30:11,110 --> 01:30:13,570
the model to encode real-world
facts, such as Paris

2119
01:30:13,570 --> 01:30:15,020
being the capital of France.

2120
01:30:15,020 --> 01:30:16,540
Oops, sorry.

2121
01:30:16,540 --> 01:30:18,530
What about the third example?

2122
01:30:18,530 --> 01:30:25,000
She unlocked her phone
using her body parts.

2123
01:30:25,000 --> 01:30:27,310
I don't what your phone--

2124
01:30:27,310 --> 01:30:31,000
type of phone you have, but
wait, what would you say.

2125
01:30:31,000 --> 01:30:31,660
Face.

2126
01:30:31,660 --> 01:30:32,780
Face.

2127
01:30:32,780 --> 01:30:33,400
Password.

2128
01:30:33,400 --> 01:30:34,930
Password.

2129
01:30:34,930 --> 01:30:36,460
Fingerprint.

2130
01:30:36,460 --> 01:30:38,540
Yeah, all of them are possible.

2131
01:30:38,540 --> 01:30:40,660
So again, the network
will learn probably

2132
01:30:40,660 --> 01:30:42,340
that password,
fingerprint, and face

2133
01:30:42,340 --> 01:30:45,730
can be used to unlock stuff.

2134
01:30:45,730 --> 01:30:49,060
And, in fact, here probably
fingerprint or face

2135
01:30:49,060 --> 01:30:51,880
might nowadays be the
more common because

2136
01:30:51,880 --> 01:30:53,770
of how the world has evolved.

2137
01:30:53,770 --> 01:30:56,950
But back in the days, it
would be password for sure.

2138
01:30:56,950 --> 01:30:59,860
And so these forces
semantic understanding

2139
01:30:59,860 --> 01:31:04,960
that these things are probably
all meant to unlock information.

2140
01:31:04,960 --> 01:31:11,600
The next one the cat chased
the dog or mouse or ball.

2141
01:31:11,600 --> 01:31:16,240
And again, the model we learn
probabilistic reasoning.

2142
01:31:16,240 --> 01:31:18,700
Meaning because
in the data set it

2143
01:31:18,700 --> 01:31:20,650
will find variations
of the sentence

2144
01:31:20,650 --> 01:31:23,690
with different
actually conclusion

2145
01:31:23,690 --> 01:31:25,630
it would say that
there's a lot of things

2146
01:31:25,630 --> 01:31:27,460
that the cat can chase.

2147
01:31:27,460 --> 01:31:29,690
And so that's
probabilistic reasoning.

2148
01:31:29,690 --> 01:31:30,890
What about the last one?

2149
01:31:30,890 --> 01:31:35,890
If it's raining, I
should bring an umbrella.

2150
01:31:35,890 --> 01:31:37,430
What's the emergent behavior.

2151
01:31:37,430 --> 01:31:40,630
It's reasoning and
inference is the model

2152
01:31:40,630 --> 01:31:42,740
we learn to connect conditions.

2153
01:31:42,740 --> 01:31:46,840
So for example raining requires
you to be protecting yourself

2154
01:31:46,840 --> 01:31:48,880
from the rain with an umbrella.

2155
01:31:48,880 --> 01:31:51,910
That's reasoning.

2156
01:31:51,910 --> 01:31:54,790
So long story short
emergent behaviors

2157
01:31:54,790 --> 01:31:57,370
are an expected
capabilities that

2158
01:31:57,370 --> 01:31:59,920
arise from simple training
objectives at scale

2159
01:31:59,920 --> 01:32:04,310
without being explicitly
taught or labeled.

2160
01:32:04,310 --> 01:32:06,680
Later in this class, we're
going to have a full lecture

2161
01:32:06,680 --> 01:32:08,870
on deep reinforcement
learning, where we're

2162
01:32:08,870 --> 01:32:12,590
going to talk about emergent
behaviors in robotics

2163
01:32:12,590 --> 01:32:17,750
or in gaming, where turns
out, the agent you're training

2164
01:32:17,750 --> 01:32:20,240
learns to do certain
strategies that you

2165
01:32:20,240 --> 01:32:21,870
didn't expect they would do.

2166
01:32:21,870 --> 01:32:26,460
AlphaGo is a good example, if
you've watched the documentary.

2167
01:32:26,460 --> 01:32:26,960
OK.

2168
01:32:30,140 --> 01:32:33,690
Self-supervision is not
just about text and images.

2169
01:32:33,690 --> 01:32:35,940
We've seen the next
token prediction for GPT,

2170
01:32:35,940 --> 01:32:38,940
and we've also seen contrastive
learning for images.

2171
01:32:38,940 --> 01:32:41,220
My question here
is the following.

2172
01:32:41,220 --> 01:32:44,530
What other examples of
modalities can you think of?

2173
01:32:48,350 --> 01:32:50,230
And tell me the task
that you would define.

2174
01:32:50,230 --> 01:32:50,730
Audio.

2175
01:32:50,730 --> 01:32:52,022
So for audio what would you do?

2176
01:32:57,750 --> 01:32:58,250
Audio.

2177
01:32:58,250 --> 01:33:01,400
How would you do a
self-supervision in audio?

2178
01:33:01,400 --> 01:33:03,640
Mask out portions of the audio.

2179
01:33:03,640 --> 01:33:04,390
Exactly.

2180
01:33:04,390 --> 01:33:08,680
Mask out portion-- so
mask out 20 times steps.

2181
01:33:08,680 --> 01:33:11,600
And because you know what the
data was, you have a label.

2182
01:33:11,600 --> 01:33:12,920
You knew what the truth was.

2183
01:33:12,920 --> 01:33:14,960
You can do a
self-supervision task.

2184
01:33:14,960 --> 01:33:15,920
It would work great.

2185
01:33:15,920 --> 01:33:19,840
Again, the only limitation
is compute and scale.

2186
01:33:19,840 --> 01:33:22,750
What other modalities?

2187
01:33:22,750 --> 01:33:27,340
Maybe self-driving trains
with conditions of the--

2188
01:33:27,340 --> 01:33:29,600
Yeah so self-driving
is a good example.

2189
01:33:29,600 --> 01:33:31,180
It's very multimodal.

2190
01:33:31,180 --> 01:33:32,560
There's a lot of
different things

2191
01:33:32,560 --> 01:33:33,650
happening in self-driving.

2192
01:33:33,650 --> 01:33:35,780
We'll talk about it
in a future lecture.

2193
01:33:35,780 --> 01:33:36,740
What else.

2194
01:33:36,740 --> 01:33:38,720
What other modalities
can you think of?

2195
01:33:38,720 --> 01:33:39,220
Videos.

2196
01:33:39,220 --> 01:33:39,890
Videos.

2197
01:33:39,890 --> 01:33:41,410
What would you do video?

2198
01:33:41,410 --> 01:33:42,820
Take some portions.

2199
01:33:42,820 --> 01:33:44,120
Take frames out.

2200
01:33:44,120 --> 01:33:45,380
You can take some frames out.

2201
01:33:45,380 --> 01:33:47,890
Same principle as audio.

2202
01:33:47,890 --> 01:33:49,070
Biology.

2203
01:33:49,070 --> 01:33:52,870
Some people work in
health biology here.

2204
01:33:52,870 --> 01:33:54,730
Yeah a couple.

2205
01:33:54,730 --> 01:33:58,660
Well you know about amino
acids and protein structure.

2206
01:33:58,660 --> 01:34:02,950
You can actually mask
portion of the inputs,

2207
01:34:02,950 --> 01:34:08,460
such as the protein structure
or DNA, and then complete it,

2208
01:34:08,460 --> 01:34:12,180
and it will force the model
to understand those patterns.

2209
01:34:12,180 --> 01:34:14,430
So great stuff in there.

2210
01:34:14,430 --> 01:34:16,570
But the world is
very multimodal.

2211
01:34:16,570 --> 01:34:19,660
We experience words, images,
sounds, and actions together.

2212
01:34:19,660 --> 01:34:21,280
How can we connect them?

2213
01:34:21,280 --> 01:34:25,830
When you think about
multi-modality,

2214
01:34:25,830 --> 01:34:29,880
you want to connect texts
and images, let's say.

2215
01:34:29,880 --> 01:34:31,380
What do you need to do those?

2216
01:34:31,380 --> 01:34:33,310
You actually need labeled data.

2217
01:34:33,310 --> 01:34:35,367
You need image captions.

2218
01:34:35,367 --> 01:34:37,200
So for example, you
have a bunch of pictures

2219
01:34:37,200 --> 01:34:40,060
online of the cat is
looking at the camera.

2220
01:34:40,060 --> 01:34:43,080
So there's a picture and there's
a label underneath just like

2221
01:34:43,080 --> 01:34:43,960
on Instagram.

2222
01:34:43,960 --> 01:34:46,650
Let's say people
put captions, right.

2223
01:34:46,650 --> 01:34:50,550
And the reason you can
connect those modalities

2224
01:34:50,550 --> 01:34:53,770
is because of that data set,
because you have a lot of that.

2225
01:34:53,770 --> 01:34:56,350
Now this is not typically
called supervised learning,

2226
01:34:56,350 --> 01:34:58,170
it will be called
weakly supervised

2227
01:34:58,170 --> 01:35:01,800
learning because you're not
actually labeling images

2228
01:35:01,800 --> 01:35:04,080
with captions,
you are benefiting

2229
01:35:04,080 --> 01:35:08,560
from naturally occurring
pairings in the world.

2230
01:35:08,560 --> 01:35:12,070
There is naturally occurring
pairings of images and texts.

2231
01:35:12,070 --> 01:35:14,220
OK, so now what I
want you to do is

2232
01:35:14,220 --> 01:35:18,150
to find other examples
that are not just images

2233
01:35:18,150 --> 01:35:21,960
captioned, but
naturally-occurring examples

2234
01:35:21,960 --> 01:35:25,230
of different modalities
that appear in the wild

2235
01:35:25,230 --> 01:35:28,570
together that we could
use to connect modalities.

2236
01:35:28,570 --> 01:35:30,330
The whole point of
connecting modalities

2237
01:35:30,330 --> 01:35:35,640
is that our vectors now can
represent different modalities

2238
01:35:35,640 --> 01:35:39,165
close to each other in space.

2239
01:35:39,165 --> 01:35:40,370
So think about that.

2240
01:35:50,520 --> 01:35:51,187
Please continue.

2241
01:35:51,187 --> 01:35:52,770
I'm going to read
some of the answers.

2242
01:35:52,770 --> 01:35:53,720
But we keep going.

2243
01:35:58,350 --> 01:36:01,810
So stock price sequence
is the single modality.

2244
01:36:01,810 --> 01:36:06,350
You would look at stock price
and you can mask and then

2245
01:36:06,350 --> 01:36:06,850
predict.

2246
01:36:06,850 --> 01:36:09,517
But I think maybe what you mean
is you would put additional data

2247
01:36:09,517 --> 01:36:11,220
points in there as well.

2248
01:36:11,220 --> 01:36:12,520
Let me see if I have something.

2249
01:36:12,520 --> 01:36:13,570
Audio paired with video.

2250
01:36:13,570 --> 01:36:14,862
Audio and video is a great one.

2251
01:36:14,862 --> 01:36:16,660
Audio and video is
naturally paired.

2252
01:36:16,660 --> 01:36:20,650
You take a YouTube video, it
has the audio and the video.

2253
01:36:20,650 --> 01:36:24,005
And so when a dog
is barking, you

2254
01:36:24,005 --> 01:36:25,380
have the audio of
the dog barking

2255
01:36:25,380 --> 01:36:26,890
and the video of
the dog barking.

2256
01:36:26,890 --> 01:36:31,500
And so you can create a pairing
between those two modalities.

2257
01:36:31,500 --> 01:36:34,810
Transcription so a lot
of movies have subtitles.

2258
01:36:34,810 --> 01:36:37,740
And so, by definition, a video
stream or a stream of images

2259
01:36:37,740 --> 01:36:41,340
will be naturally connected
to text, which would also be

2260
01:36:41,340 --> 01:36:42,695
naturally connected to audio.

2261
01:36:54,900 --> 01:36:56,287
Music and song title.

2262
01:36:56,287 --> 01:36:57,370
Again, that's a great one.

2263
01:36:57,370 --> 01:37:01,500
Audio and text are connected.

2264
01:37:01,500 --> 01:37:03,580
Genotype and phenotype.

2265
01:37:03,580 --> 01:37:04,610
Good one as well.

2266
01:37:08,790 --> 01:37:11,080
Medical imaging with ultrasound.

2267
01:37:11,080 --> 01:37:12,250
That's a great one.

2268
01:37:12,250 --> 01:37:13,480
Naturally occurring.

2269
01:37:13,480 --> 01:37:15,090
You usually if you
go to an ultrasound

2270
01:37:15,090 --> 01:37:17,040
you'll have the
different types of images

2271
01:37:17,040 --> 01:37:19,120
that occur together naturally.

2272
01:37:21,840 --> 01:37:23,740
Game footage and
keyboard action.

2273
01:37:23,740 --> 01:37:25,030
Again another great one.

2274
01:37:25,030 --> 01:37:27,400
So price and area code.

2275
01:37:27,400 --> 01:37:30,270
Good one.

2276
01:37:30,270 --> 01:37:31,860
Great examples.

2277
01:37:31,860 --> 01:37:32,860
Facial expression.

2278
01:37:32,860 --> 01:37:39,810
So TLDR is we have ways
to connect modalities.

2279
01:37:39,810 --> 01:37:43,600
Oftentimes some modalities are
going to connect very naturally.

2280
01:37:43,600 --> 01:37:46,200
Most things connect to text.

2281
01:37:46,200 --> 01:37:50,700
So that's what you want to use
as your shared space typically.

2282
01:37:50,700 --> 01:37:54,210
But here it is an example of
a paper called image bind.

2283
01:37:54,210 --> 01:37:56,480
And the interesting
thing about image bind

2284
01:37:56,480 --> 01:38:02,480
is it says that
most things connect

2285
01:38:02,480 --> 01:38:03,900
through a single modality.

2286
01:38:03,900 --> 01:38:08,190
So, for example, thermal
data connects to imaging.

2287
01:38:08,190 --> 01:38:09,960
Imaging connects to text.

2288
01:38:09,960 --> 01:38:12,110
So text is going to
connect through images

2289
01:38:12,110 --> 01:38:14,180
with thermal data.

2290
01:38:14,180 --> 01:38:16,280
And what's the
consequence of that

2291
01:38:16,280 --> 01:38:18,380
if I may show you
a little example.

2292
01:38:18,380 --> 01:38:20,090
Is that you can--

2293
01:38:20,090 --> 01:38:23,670
this is a demo from
Meta called image bind.

2294
01:38:23,670 --> 01:38:24,600
It's a cool one.

2295
01:38:24,600 --> 01:38:27,240
You can actually see
things occurring together.

2296
01:38:27,240 --> 01:38:29,420
So, for example, you
put a text drums.

2297
01:38:29,420 --> 01:38:33,080
And of course, you can
get an audio of a drum.

2298
01:38:33,080 --> 01:38:36,560
But you can also see what the
closest image in the vector

2299
01:38:36,560 --> 01:38:38,550
space is to that concept.

2300
01:38:38,550 --> 01:38:41,960
So all the spaces are
now bound together.

2301
01:38:41,960 --> 01:38:44,730
You can also do audio and image.

2302
01:38:44,730 --> 01:38:49,440
So you give it a dog
barking and a picture.

2303
01:38:49,440 --> 01:38:51,170
What can you expect?

2304
01:38:51,170 --> 01:38:52,760
A dog on the beach.

2305
01:38:52,760 --> 01:38:55,130
That's the
multi-modal embedding.

2306
01:38:55,130 --> 01:38:58,350
The connecting tissue between
those different modalities.

2307
01:38:58,350 --> 01:39:01,370
And that's probably one
of the biggest innovation

2308
01:39:01,370 --> 01:39:04,670
of the last few years
connecting those shared spaces.

2309
01:39:04,670 --> 01:39:05,240
OK.

2310
01:39:05,240 --> 01:39:06,823
I'm not going to
cover the full paper,

2311
01:39:06,823 --> 01:39:11,180
but the core insight is that
there are shared spaces.

2312
01:39:11,180 --> 01:39:12,680
There are spaces
like text and image

2313
01:39:12,680 --> 01:39:15,180
that connect to most modalities
that can allow us to connect

2314
01:39:15,180 --> 01:39:17,150
those modalities together.

2315
01:39:17,150 --> 01:39:19,880
We learned a lot of
things here embeddings,

2316
01:39:19,880 --> 01:39:22,550
self-supervised learning,
contrastive learning, data

2317
01:39:22,550 --> 01:39:24,470
augmentation, next
token prediction,

2318
01:39:24,470 --> 01:39:26,030
weakly supervised learning.

2319
01:39:26,030 --> 01:39:29,390
And then the shared embedding
stays with the central pivot

2320
01:39:29,390 --> 01:39:30,800
usually being text.

2321
01:39:30,800 --> 01:39:33,320
OK.

2322
01:39:33,320 --> 01:39:34,220
That's all for today.

2323
01:39:34,220 --> 01:39:36,803
We're not going to have time to
cover the adversarial example,

2324
01:39:36,803 --> 01:39:39,050
but we're going to cover
it in two weeks together.

2325
01:39:39,050 --> 01:39:42,160
You're going to have more
neural network baggage.

