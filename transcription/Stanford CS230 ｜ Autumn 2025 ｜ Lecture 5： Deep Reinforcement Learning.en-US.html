<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Stanford CS230 ｜ Autumn 2025 ｜ Lecture 5： Deep Reinforcement Learning.en-US.html</title>
  <style>
    body{font-family:Inter, system-ui, -apple-system, Arial, sans-serif;line-height:1.6;padding:1rem;max-width:900px;margin:0 auto;color:#111}
    .meta{color:#666;font-size:0.95rem;margin-bottom:0.5rem}
    article{background:#fff;border-radius:8px;padding:1rem 1.2rem;box-shadow:0 6px 18px rgba(10,20,30,0.05)}
    p.cue{margin:0 0 0.6rem}
    span.time{color:#666;margin-right:8px;font-size:0.95rem}
  </style>
</head>
<body>
<article lang="en">
  <header>
    <h1>Stanford CS230 ｜ Autumn 2025 ｜ Lecture 5： Deep Reinforcement Learning</h1>
    <p class="meta">Source: Stanford CS230 ｜ Autumn 2025 ｜ Lecture 5： Deep Reinforcement Learning.en-US.srt</p>
  </header>
  <section class="transcript">
    <p class="cue"><span class="time">[00:05]</span>Welcome to our fifth lecture in person for Stanford</p>
    <p class="cue"><span class="time">[00:10]</span>deep learning CS230.</p>
    <p class="cue"><span class="time">[00:13]</span>Today&#x27;s lecture is going to be about deep reinforcement</p>
    <p class="cue"><span class="time">[00:15]</span>learning.</p>
    <p class="cue"><span class="time">[00:16]</span>I actually switched the original plan</p>
    <p class="cue"><span class="time">[00:19]</span>of talking about neural network interpretability and LLM</p>
    <p class="cue"><span class="time">[00:24]</span>visualization, simply because you haven&#x27;t</p>
    <p class="cue"><span class="time">[00:27]</span>had the chance to study attention maps,</p>
    <p class="cue"><span class="time">[00:31]</span>convolutional neural networks.</p>
    <p class="cue"><span class="time">[00:32]</span>And so it would have been an overkill to do that week five.</p>
    <p class="cue"><span class="time">[00:35]</span>So we&#x27;re going to talk about neural network interpretability</p>
    <p class="cue"><span class="time">[00:38]</span>and visualization in a later lecture actually.</p>
    <p class="cue"><span class="time">[00:42]</span>But today, our focus will be on deep reinforcement learning,</p>
    <p class="cue"><span class="time">[00:46]</span>which is probably my favorite lecture of the class.</p>
    <p class="cue"><span class="time">[00:50]</span>I feel like I say that every week, but it&#x27;s OK, I like it.</p>
    <p class="cue"><span class="time">[00:54]</span>The agenda is pretty packed.</p>
    <p class="cue"><span class="time">[00:57]</span>We&#x27;re going to start with deep reinforcement learning, which</p>
    <p class="cue"><span class="time">[01:02]</span>you can think of as the marriage between deep learning</p>
    <p class="cue"><span class="time">[01:06]</span>and reinforcement learning.</p>
    <p class="cue"><span class="time">[01:08]</span>Together, the baby is called deep reinforcement learning.</p>
    <p class="cue"><span class="time">[01:10]</span>And we&#x27;re going to see how reinforcement learning works</p>
    <p class="cue"><span class="time">[01:14]</span>and how neural networks can play a part in building</p>
    <p class="cue"><span class="time">[01:19]</span>reinforcement learning agent.</p>
    <p class="cue"><span class="time">[01:22]</span>In the second half of the class, we</p>
    <p class="cue"><span class="time">[01:25]</span>will focus on a very specific concept called reinforcement</p>
    <p class="cue"><span class="time">[01:32]</span>learning from human feedback that you might have heard of.</p>
    <p class="cue"><span class="time">[01:36]</span>It&#x27;s one of the core concepts that</p>
    <p class="cue"><span class="time">[01:39]</span>really made the difference between what</p>
    <p class="cue"><span class="time">[01:43]</span>you might have remembered as GPT 2 and ChatGPT.</p>
    <p class="cue"><span class="time">[01:47]</span>That&#x27;s the leap.</p>
    <p class="cue"><span class="time">[01:49]</span>That&#x27;s really the technique that has democratized access</p>
    <p class="cue"><span class="time">[01:56]</span>to LLM because of the performance improvements</p>
    <p class="cue"><span class="time">[01:59]</span>and the alignment with humans.</p>
    <p class="cue"><span class="time">[02:01]</span>So we&#x27;re going to see what is this concept of RLHF</p>
    <p class="cue"><span class="time">[02:05]</span>and how does it work, and why does it</p>
    <p class="cue"><span class="time">[02:09]</span>allow us to align a language model to human preferences.</p>
    <p class="cue"><span class="time">[02:14]</span>Ready to go.</p>
    <p class="cue"><span class="time">[02:15]</span>As always, let&#x27;s try to make it interactive.</p>
    <p class="cue"><span class="time">[02:19]</span>So the motivation behind deep reinforcement</p>
    <p class="cue"><span class="time">[02:21]</span>learning-- and as usual, you&#x27;re going</p>
    <p class="cue"><span class="time">[02:22]</span>to have all the most important papers that</p>
    <p class="cue"><span class="time">[02:25]</span>are covered in the class listed at the bottom of each slide.</p>
    <p class="cue"><span class="time">[02:30]</span>Reinforcement learning has grown in popularity.</p>
    <p class="cue"><span class="time">[02:33]</span>One of the very popular papers, called Human Level Control</p>
    <p class="cue"><span class="time">[02:39]</span>through Deep Reinforcement Learning,</p>
    <p class="cue"><span class="time">[02:41]</span>is the work from DeepMind, has showed us</p>
    <p class="cue"><span class="time">[02:47]</span>that a single algorithm/training method</p>
    <p class="cue"><span class="time">[02:51]</span>can allow us to train AI that can play many, many Atari games</p>
    <p class="cue"><span class="time">[02:58]</span>better than humans.</p>
    <p class="cue"><span class="time">[03:00]</span>Single algorithm, over 40, 50 games</p>
    <p class="cue"><span class="time">[03:03]</span>where it exceeds human capability, which</p>
    <p class="cue"><span class="time">[03:06]</span>is quite impressive when you thought about the fact</p>
    <p class="cue"><span class="time">[03:09]</span>that machine learning used to be niche,</p>
    <p class="cue"><span class="time">[03:11]</span>and you would have to train a really niche algorithm</p>
    <p class="cue"><span class="time">[03:13]</span>to perform different tasks.</p>
    <p class="cue"><span class="time">[03:15]</span>Here&#x27;s an algorithm that can just learn every Atari game.</p>
    <p class="cue"><span class="time">[03:19]</span>A little later, you might have heard of AlphaGo.</p>
    <p class="cue"><span class="time">[03:23]</span>AlphaGo is an algorithm that was developed</p>
    <p class="cue"><span class="time">[03:27]</span>to beat and exceed human performance in the game of Go.</p>
    <p class="cue"><span class="time">[03:31]</span>We&#x27;ll talk about it a little more.</p>
    <p class="cue"><span class="time">[03:32]</span>The game of Go is a very complex game.</p>
    <p class="cue"><span class="time">[03:35]</span>Some would argue way more complex than chess</p>
    <p class="cue"><span class="time">[03:39]</span>from a decision making standpoint</p>
    <p class="cue"><span class="time">[03:41]</span>and from the possibilities that can happen on the board.</p>
    <p class="cue"><span class="time">[03:46]</span>And so it actually got solved in 2017, again by the DeepMind team</p>
    <p class="cue"><span class="time">[03:53]</span>and David Silver&#x27;s lab.</p>
    <p class="cue"><span class="time">[03:56]</span>Later on-- and again, another great paper from DeepMind</p>
    <p class="cue"><span class="time">[04:00]</span>had showed us that reinforcement learning can also</p>
    <p class="cue"><span class="time">[04:03]</span>be used for strategy game.</p>
    <p class="cue"><span class="time">[04:07]</span>That might be a touch more complex than chess or Go.</p>
    <p class="cue"><span class="time">[04:12]</span>That might actually involve multiple players</p>
    <p class="cue"><span class="time">[04:15]</span>playing with each other or against each other.</p>
    <p class="cue"><span class="time">[04:18]</span>Some of you might have played StarCraft, for example.</p>
    <p class="cue"><span class="time">[04:21]</span>That&#x27;s an example of a game where</p>
    <p class="cue"><span class="time">[04:23]</span>it requires a lot of long-term thinking, short-term thinking.</p>
    <p class="cue"><span class="time">[04:26]</span>Another one is DOTA.</p>
    <p class="cue"><span class="time">[04:28]</span>Some of you might have played DOTA or league</p>
    <p class="cue"><span class="time">[04:30]</span>of legends, where you have a team playing</p>
    <p class="cue"><span class="time">[04:31]</span>against another team.</p>
    <p class="cue"><span class="time">[04:33]</span>Those are examples of games that involve multiple agents playing</p>
    <p class="cue"><span class="time">[04:36]</span>collaboratively.</p>
    <p class="cue"><span class="time">[04:37]</span>And it&#x27;s pretty hard to develop systems</p>
    <p class="cue"><span class="time">[04:40]</span>that can play with each other against multiple opponents.</p>
    <p class="cue"><span class="time">[04:45]</span>And finally, most recently, this is</p>
    <p class="cue"><span class="time">[04:47]</span>2022, so alongside the release of ChatGPT,</p>
    <p class="cue"><span class="time">[04:51]</span>this paper that introduces the concept of reinforcement</p>
    <p class="cue"><span class="time">[04:55]</span>learning with human feedback applied to aligning language</p>
    <p class="cue"><span class="time">[04:58]</span>models with human preferences.</p>
    <p class="cue"><span class="time">[05:01]</span>And we&#x27;ll talk about that later.</p>
    <p class="cue"><span class="time">[05:02]</span>So all this to say that reinforcement learning allowed</p>
    <p class="cue"><span class="time">[05:08]</span>us to exceed human performance in a variety of tasks.</p>
    <p class="cue"><span class="time">[05:12]</span>The first one I want us to think about is the game of Go.</p>
    <p class="cue"><span class="time">[05:16]</span>So let&#x27;s say that you were asked to solve the game of Go</p>
    <p class="cue"><span class="time">[05:22]</span>with classic supervised learning.</p>
    <p class="cue"><span class="time">[05:25]</span>Everything we&#x27;ve seen together so far, labeled data,</p>
    <p class="cue"><span class="time">[05:28]</span>how would you solve the game of Go</p>
    <p class="cue"><span class="time">[05:30]</span>with classic supervised learning?</p>
    <p class="cue"><span class="time">[05:35]</span>What data would you collect?</p>
    <p class="cue"><span class="time">[05:37]</span>What would be the label, et cetera?</p>
    <p class="cue"><span class="time">[05:45]</span>Yes?</p>
    <p class="cue"><span class="time">[05:46]</span>[INAUDIBLE] data that&#x27;s the current [INAUDIBLE].</p>
    <p class="cue"><span class="time">[05:54]</span>OK, good point.</p>
    <p class="cue"><span class="time">[05:55]</span>Yeah, you look at history of plenty of games,</p>
    <p class="cue"><span class="time">[05:57]</span>hopefully from good players.</p>
    <p class="cue"><span class="time">[06:00]</span>You want the algorithm to work.</p>
    <p class="cue"><span class="time">[06:02]</span>And you look at x as the input being</p>
    <p class="cue"><span class="time">[06:06]</span>the current state of the board and y</p>
    <p class="cue"><span class="time">[06:09]</span>as the next state of the board.</p>
    <p class="cue"><span class="time">[06:11]</span>And this would tell you what move was selected,</p>
    <p class="cue"><span class="time">[06:13]</span>and you learn the move essentially.</p>
    <p class="cue"><span class="time">[06:15]</span>And hopefully, if you do that across many, many games,</p>
    <p class="cue"><span class="time">[06:18]</span>you might see the agent become more attuned to the game</p>
    <p class="cue"><span class="time">[06:23]</span>and develop better strategies.</p>
    <p class="cue"><span class="time">[06:27]</span>So really hopefully it&#x27;s a professional player.</p>
    <p class="cue"><span class="time">[06:30]</span>What are the disadvantages of that or the shortcomings</p>
    <p class="cue"><span class="time">[06:33]</span>that you can anticipate?</p>
    <p class="cue"><span class="time">[06:37]</span>Yes?</p>
    <p class="cue"><span class="time">[06:39]</span>So the two space types of moves that the players use maybe</p>
    <p class="cue"><span class="time">[06:44]</span>[INAUDIBLE] some other set of moves that [INAUDIBLE].</p>
    <p class="cue"><span class="time">[06:51]</span>Yeah, great point.</p>
    <p class="cue"><span class="time">[06:52]</span>You might not see the entire space</p>
    <p class="cue"><span class="time">[06:54]</span>of possible states of the board, which is what you said.</p>
    <p class="cue"><span class="time">[06:58]</span>So you might miss out on a lot of different strategies.</p>
    <p class="cue"><span class="time">[07:00]</span>So the game of Go is actually a game</p>
    <p class="cue"><span class="time">[07:03]</span>with two players, one player that</p>
    <p class="cue"><span class="time">[07:05]</span>uses the black stones and one player that</p>
    <p class="cue"><span class="time">[07:08]</span>uses the White stones.</p>
    <p class="cue"><span class="time">[07:09]</span>And iteratively, they&#x27;re going to place</p>
    <p class="cue"><span class="time">[07:11]</span>those stones on the grid, a 13 by 13 grid</p>
    <p class="cue"><span class="time">[07:16]</span>that you can see on screen with the goal of surrounding</p>
    <p class="cue"><span class="time">[07:19]</span>their opponents.</p>
    <p class="cue"><span class="time">[07:20]</span>So you&#x27;re constantly trying to surround</p>
    <p class="cue"><span class="time">[07:22]</span>the stones of the opponent, and the opponent</p>
    <p class="cue"><span class="time">[07:24]</span>is trying to surround your stones.</p>
    <p class="cue"><span class="time">[07:26]</span>And so you can imagine that for every intersection on the grid,</p>
    <p class="cue"><span class="time">[07:32]</span>there is multiple possibilities, either there&#x27;s a black stone</p>
    <p class="cue"><span class="time">[07:35]</span>or a white stone or nothing.</p>
    <p class="cue"><span class="time">[07:38]</span>And on a 13 by 13 grid, you can imagine how many possibilities</p>
    <p class="cue"><span class="time">[07:42]</span>of a board state there are.</p>
    <p class="cue"><span class="time">[07:44]</span>It&#x27;s impossible to capture all of that with historical moves</p>
    <p class="cue"><span class="time">[07:48]</span>from professional players.</p>
    <p class="cue"><span class="time">[07:49]</span>You will just never cover that.</p>
    <p class="cue"><span class="time">[07:51]</span>The same thing could be said in chess as well.</p>
    <p class="cue"><span class="time">[07:54]</span>You know that even the professional players can</p>
    <p class="cue"><span class="time">[07:55]</span>plan x number of steps in advance,</p>
    <p class="cue"><span class="time">[07:58]</span>but nobody knows where the game takes you.</p>
    <p class="cue"><span class="time">[08:01]</span>And in the late stages of the games or the end games,</p>
    <p class="cue"><span class="time">[08:05]</span>players always find themselves playing a different game.</p>
    <p class="cue"><span class="time">[08:07]</span>And that&#x27;s part of the magic of being good at chess.</p>
    <p class="cue"><span class="time">[08:11]</span>So yeah, that&#x27;s a problem.</p>
    <p class="cue"><span class="time">[08:12]</span>What&#x27;s another problem or shortcoming</p>
    <p class="cue"><span class="time">[08:14]</span>beyond the fact that we can&#x27;t observe possibly all the states?</p>
    <p class="cue"><span class="time">[08:21]</span>Yes?</p>
    <p class="cue"><span class="time">[08:22]</span>You also can&#x27;t anticipate what that action would lead to</p>
    <p class="cue"><span class="time">[08:27]</span>in the future.</p>
    <p class="cue"><span class="time">[08:28]</span>You might not make the best decision.</p>
    <p class="cue"><span class="time">[08:34]</span>Correct.</p>
    <p class="cue"><span class="time">[08:35]</span>If I repeat what you said, well, first, you don&#x27;t even</p>
    <p class="cue"><span class="time">[08:39]</span>know if this was a good move.</p>
    <p class="cue"><span class="time">[08:40]</span>So maybe it was not even a good move.</p>
    <p class="cue"><span class="time">[08:42]</span>And you&#x27;re learning something that was not a good move,</p>
    <p class="cue"><span class="time">[08:44]</span>and you&#x27;re labeling it as a good move.</p>
    <p class="cue"><span class="time">[08:46]</span>And second, you&#x27;re actually only getting partial information,</p>
    <p class="cue"><span class="time">[08:49]</span>meaning you don&#x27;t have the information of what&#x27;s</p>
    <p class="cue"><span class="time">[08:51]</span>in the person&#x27;s mind and what strategy they&#x27;re</p>
    <p class="cue"><span class="time">[08:54]</span>trying to execute.</p>
    <p class="cue"><span class="time">[08:55]</span>So your store-- you&#x27;re looking at a single example</p>
    <p class="cue"><span class="time">[08:59]</span>among a long-term strategy.</p>
    <p class="cue"><span class="time">[09:02]</span>And you can&#x27;t expect the model to guess what&#x27;s</p>
    <p class="cue"><span class="time">[09:04]</span>the long-term strategy, because it was just trained on x and y</p>
    <p class="cue"><span class="time">[09:08]</span>and matching the inputs to a possible output.</p>
    <p class="cue"><span class="time">[09:11]</span>So you don&#x27;t really have any concept</p>
    <p class="cue"><span class="time">[09:13]</span>of a strategy at that point.</p>
    <p class="cue"><span class="time">[09:14]</span>It looks one off at every decisions of the model.</p>
    <p class="cue"><span class="time">[09:18]</span>Those are really good points.</p>
    <p class="cue"><span class="time">[09:21]</span>The other one is the ground truth might be ill-defined.</p>
    <p class="cue"><span class="time">[09:26]</span>What I mean by that is even the best humans in the world</p>
    <p class="cue"><span class="time">[09:33]</span>do not play their best game every day,</p>
    <p class="cue"><span class="time">[09:36]</span>and even their best game is not the ground truth.</p>
    <p class="cue"><span class="time">[09:39]</span>And that creates an issue because you&#x27;re essentially</p>
    <p class="cue"><span class="time">[09:42]</span>training against a target that is off by a certain margin.</p>
    <p class="cue"><span class="time">[09:46]</span>You&#x27;re never going to get better than the best human,</p>
    <p class="cue"><span class="time">[09:48]</span>and the best human is not the best possible--</p>
    <p class="cue"><span class="time">[09:51]</span>executing the best possible strategy at every point.</p>
    <p class="cue"><span class="time">[09:54]</span>So you could argue, what if we get a panel of experts</p>
    <p class="cue"><span class="time">[09:58]</span>that we&#x27;re monitoring, and those are</p>
    <p class="cue"><span class="time">[09:59]</span>the best players in the world.</p>
    <p class="cue"><span class="time">[10:01]</span>Even with a panel of experts that decides every move,</p>
    <p class="cue"><span class="time">[10:04]</span>you still have an ill-defined ground truth.</p>
    <p class="cue"><span class="time">[10:10]</span>So that&#x27;s a big issue.</p>
    <p class="cue"><span class="time">[10:11]</span>Too many states in the game, you mentioned.</p>
    <p class="cue"><span class="time">[10:13]</span>And we will likely not generalize,</p>
    <p class="cue"><span class="time">[10:14]</span>which is what you said, meaning we&#x27;re</p>
    <p class="cue"><span class="time">[10:16]</span>looking at one off situations.</p>
    <p class="cue"><span class="time">[10:17]</span>We&#x27;re not looking at entire strategies.</p>
    <p class="cue"><span class="time">[10:19]</span>And so when we face a board state</p>
    <p class="cue"><span class="time">[10:22]</span>that we&#x27;ve never seen before, because the model was not</p>
    <p class="cue"><span class="time">[10:25]</span>trained on strategy, it will get stuck.</p>
    <p class="cue"><span class="time">[10:28]</span>Yeah.</p>
    <p class="cue"><span class="time">[10:31]</span>OK.</p>
    <p class="cue"><span class="time">[10:31]</span>And this is an example of a perfect application</p>
    <p class="cue"><span class="time">[10:34]</span>for reinforcement learning, because reinforcement</p>
    <p class="cue"><span class="time">[10:37]</span>learning is all about delayed labels</p>
    <p class="cue"><span class="time">[10:41]</span>and making sequences of good decisions.</p>
    <p class="cue"><span class="time">[10:45]</span>So if you had to remember in one sentence what&#x27;s RL,</p>
    <p class="cue"><span class="time">[10:49]</span>RL is making good sequences of decisions-- sequences</p>
    <p class="cue"><span class="time">[10:54]</span>of good decisions.</p>
    <p class="cue"><span class="time">[10:55]</span>Sorry.</p>
    <p class="cue"><span class="time">[11:01]</span>And do that automatically.</p>
    <p class="cue"><span class="time">[11:05]</span>Another way to look at it is the difference</p>
    <p class="cue"><span class="time">[11:07]</span>between classic supervised learning in RL is--</p>
    <p class="cue"><span class="time">[11:12]</span>in classic supervised learning, you teach by example.</p>
    <p class="cue"><span class="time">[11:15]</span>In reinforcement learning, you teach</p>
    <p class="cue"><span class="time">[11:17]</span>by experience, which is also a different concept.</p>
    <p class="cue"><span class="time">[11:20]</span>You&#x27;re not just showing cats and non-cats to a model,</p>
    <p class="cue"><span class="time">[11:25]</span>you&#x27;re actually letting the model experience an environment</p>
    <p class="cue"><span class="time">[11:29]</span>until it figures out what were the best decisions it</p>
    <p class="cue"><span class="time">[11:33]</span>made and learns from them.</p>
    <p class="cue"><span class="time">[11:37]</span>Some examples of reinforcement learning applications,</p>
    <p class="cue"><span class="time">[11:40]</span>I&#x27;m going to mention them.</p>
    <p class="cue"><span class="time">[11:41]</span>We have gaming, of course, that we already covered.</p>
    <p class="cue"><span class="time">[11:44]</span>What are other applications of AI where we need</p>
    <p class="cue"><span class="time">[11:48]</span>good sequences of decisions?</p>
    <p class="cue"><span class="time">[11:53]</span>Yes?</p>
    <p class="cue"><span class="time">[11:53]</span>Autonomous driving.</p>
    <p class="cue"><span class="time">[11:55]</span>Autonomous driving.</p>
    <p class="cue"><span class="time">[11:56]</span>Yeah, correct.</p>
    <p class="cue"><span class="time">[11:57]</span>I mean, in driving, you could argue RL could work,</p>
    <p class="cue"><span class="time">[12:00]</span>and there&#x27;s some RL going on.</p>
    <p class="cue"><span class="time">[12:02]</span>But what you mean, I think, is you</p>
    <p class="cue"><span class="time">[12:04]</span>have some of a dynamic planning algorithm that</p>
    <p class="cue"><span class="time">[12:07]</span>allows you to strategize.</p>
    <p class="cue"><span class="time">[12:09]</span>If you see a red light ahead, you</p>
    <p class="cue"><span class="time">[12:12]</span>might start slowing down over time.</p>
    <p class="cue"><span class="time">[12:14]</span>But maybe it will turn green, so you might not</p>
    <p class="cue"><span class="time">[12:16]</span>slow down completely.</p>
    <p class="cue"><span class="time">[12:17]</span>This is an example of a strategy that you need, of course.</p>
    <p class="cue"><span class="time">[12:22]</span>Yeah?</p>
    <p class="cue"><span class="time">[12:22]</span>Robotic controlling.</p>
    <p class="cue"><span class="time">[12:24]</span>Robotic controlling.</p>
    <p class="cue"><span class="time">[12:25]</span>That&#x27;s a great example, also related to autonomous driving.</p>
    <p class="cue"><span class="time">[12:27]</span>But imagine you want to teach your robot to move from point A</p>
    <p class="cue"><span class="time">[12:32]</span>to point B, the number of good decisions</p>
    <p class="cue"><span class="time">[12:36]</span>that the robot needs to make in terms of moving each</p>
    <p class="cue"><span class="time">[12:39]</span>of their joints is tremendous.</p>
    <p class="cue"><span class="time">[12:41]</span>It&#x27;s actually super unlikely that a robot</p>
    <p class="cue"><span class="time">[12:44]</span>would move from A to B if it&#x27;s not</p>
    <p class="cue"><span class="time">[12:46]</span>trained to make good sequences of decisions.</p>
    <p class="cue"><span class="time">[12:51]</span>What else?</p>
    <p class="cue"><span class="time">[12:56]</span>The biggest one nobody mentioned yet.</p>
    <p class="cue"><span class="time">[13:02]</span>It&#x27;s not a great application.</p>
    <p class="cue"><span class="time">[13:03]</span>I don&#x27;t like it, but it happens to be the biggest</p>
    <p class="cue"><span class="time">[13:05]</span>one of reinforcement learning.</p>
    <p class="cue"><span class="time">[13:10]</span>Yeah?</p>
    <p class="cue"><span class="time">[13:10]</span>[INAUDIBLE] where they suggest [INAUDIBLE].</p>
    <p class="cue"><span class="time">[13:14]</span>Yeah, advertisement.</p>
    <p class="cue"><span class="time">[13:15]</span>Yeah, marketing.</p>
    <p class="cue"><span class="time">[13:16]</span>You&#x27;re right.</p>
    <p class="cue"><span class="time">[13:16]</span>So yeah, we talked about robotics.</p>
    <p class="cue"><span class="time">[13:18]</span>Advertisement is another example.</p>
    <p class="cue"><span class="time">[13:21]</span>Advertisement is a long game.</p>
    <p class="cue"><span class="time">[13:25]</span>Companies are showing you multiple ads before you buy.</p>
    <p class="cue"><span class="time">[13:29]</span>And in fact, the reason reinforcement learning</p>
    <p class="cue"><span class="time">[13:32]</span>is important is because they&#x27;re planning a strategy that</p>
    <p class="cue"><span class="time">[13:35]</span>might lead a buyer to execute a purchase over time,</p>
    <p class="cue"><span class="time">[13:39]</span>and it requires a long-term thinking.</p>
    <p class="cue"><span class="time">[13:42]</span>So there&#x27;s a lot of reinforcement learning applied</p>
    <p class="cue"><span class="time">[13:44]</span>to marketing, advertisement, real-time bidding processes, et</p>
    <p class="cue"><span class="time">[13:49]</span>cetera.</p>
    <p class="cue"><span class="time">[13:52]</span>OK.</p>
    <p class="cue"><span class="time">[13:53]</span>Clear on what RL is and how it differs</p>
    <p class="cue"><span class="time">[13:55]</span>from classic supervised learning?</p>
    <p class="cue"><span class="time">[13:58]</span>OK.</p>
    <p class="cue"><span class="time">[13:59]</span>So let&#x27;s put some vocabulary around that concept.</p>
    <p class="cue"><span class="time">[14:02]</span>In reinforcement learning, you have an agent and the agent</p>
    <p class="cue"><span class="time">[14:06]</span>interacts with an environment.</p>
    <p class="cue"><span class="time">[14:11]</span>As the agent interacts with the environment,</p>
    <p class="cue"><span class="time">[14:14]</span>the agent will perform certain actions</p>
    <p class="cue"><span class="time">[14:16]</span>that we will denote at, where t is the time step.</p>
    <p class="cue"><span class="time">[14:22]</span>And the environment will show you</p>
    <p class="cue"><span class="time">[14:25]</span>states that transition from time step t to time step t plus 1.</p>
    <p class="cue"><span class="time">[14:31]</span>So subject to an action at, an environment</p>
    <p class="cue"><span class="time">[14:34]</span>may transition from st to st plus 1.</p>
    <p class="cue"><span class="time">[14:37]</span>You can think of the game of Go.</p>
    <p class="cue"><span class="time">[14:39]</span>I take the action of putting my black stone on a certain grid</p>
    <p class="cue"><span class="time">[14:42]</span>intersection, and the environment has changed.</p>
    <p class="cue"><span class="time">[14:45]</span>It moved from-- the state has changed.</p>
    <p class="cue"><span class="time">[14:46]</span>It moved from state at time step t to time step t plus 1,</p>
    <p class="cue"><span class="time">[14:49]</span>where my stone is on the grid.</p>
    <p class="cue"><span class="time">[14:52]</span>After that state update happens, there&#x27;s two things</p>
    <p class="cue"><span class="time">[14:57]</span>that the agent observes.</p>
    <p class="cue"><span class="time">[14:58]</span>The agent observes an observation</p>
    <p class="cue"><span class="time">[15:00]</span>that we will note ot and a reward rt.</p>
    <p class="cue"><span class="time">[15:08]</span>OK.</p>
    <p class="cue"><span class="time">[15:09]</span>So those are the vocabulary words.</p>
    <p class="cue"><span class="time">[15:10]</span>And of course, the goal of the agent</p>
    <p class="cue"><span class="time">[15:12]</span>will be to maximize the rewards.</p>
    <p class="cue"><span class="time">[15:17]</span>One thing to about the observation--</p>
    <p class="cue"><span class="time">[15:19]</span>we&#x27;ll talk about it a little more.</p>
    <p class="cue"><span class="time">[15:21]</span>The observation sometimes is equal to the states.</p>
    <p class="cue"><span class="time">[15:25]</span>Can someone guess why we might need two concepts instead</p>
    <p class="cue"><span class="time">[15:28]</span>of a single concept?</p>
    <p class="cue"><span class="time">[15:30]</span>Why is it important to have a state and an observation?</p>
    <p class="cue"><span class="time">[15:37]</span>Yes?</p>
    <p class="cue"><span class="time">[15:38]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[15:41]</span>Yes, correct.</p>
    <p class="cue"><span class="time">[15:42]</span>So in some cases, the environment</p>
    <p class="cue"><span class="time">[15:45]</span>may not be fully transparent to the user.</p>
    <p class="cue"><span class="time">[15:50]</span>And so for example, in chess or in Go,</p>
    <p class="cue"><span class="time">[15:53]</span>the observation is actually equal to the state.</p>
    <p class="cue"><span class="time">[15:55]</span>You see everything on your board.</p>
    <p class="cue"><span class="time">[15:57]</span>All the information is available to you.</p>
    <p class="cue"><span class="time">[15:59]</span>If you play League of Legends or StarCraft,</p>
    <p class="cue"><span class="time">[16:03]</span>you know the concept of--</p>
    <p class="cue"><span class="time">[16:05]</span>I think in English, it&#x27;s called a cloud or a fog.</p>
    <p class="cue"><span class="time">[16:08]</span>I think it&#x27;s the fog.</p>
    <p class="cue"><span class="time">[16:09]</span>You only see certain parts of the map</p>
    <p class="cue"><span class="time">[16:12]</span>until you have explored everything,</p>
    <p class="cue"><span class="time">[16:13]</span>or until your friends are visiting</p>
    <p class="cue"><span class="time">[16:16]</span>the other parts of the map.</p>
    <p class="cue"><span class="time">[16:17]</span>And so the observation is actually</p>
    <p class="cue"><span class="time">[16:19]</span>less information than the states of the environment.</p>
    <p class="cue"><span class="time">[16:24]</span>OK.</p>
    <p class="cue"><span class="time">[16:25]</span>And then the last piece of vocabulary is the transition.</p>
    <p class="cue"><span class="time">[16:28]</span>When I refer to a transition, I&#x27;ll</p>
    <p class="cue"><span class="time">[16:29]</span>refer of the process of getting from state t</p>
    <p class="cue"><span class="time">[16:32]</span>to state t plus 1, which means we&#x27;re in state t,</p>
    <p class="cue"><span class="time">[16:35]</span>the agent takes an action at, it observes ot and a reward rt,</p>
    <p class="cue"><span class="time">[16:40]</span>and it transitions to the next state st plus 1.</p>
    <p class="cue"><span class="time">[16:42]</span>Question?</p>
    <p class="cue"><span class="time">[16:44]</span>Regarding [INAUDIBLE], are there any [INAUDIBLE]</p>
    <p class="cue"><span class="time">[16:47]</span>the state is too large to [INAUDIBLE]?</p>
    <p class="cue"><span class="time">[16:55]</span>Wait.</p>
    <p class="cue"><span class="time">[16:56]</span>What do you mean?</p>
    <p class="cue"><span class="time">[16:57]</span>You mean, is there-- are there examples of environment where</p>
    <p class="cue"><span class="time">[17:00]</span>the state is so large that the--</p>
    <p class="cue"><span class="time">[17:03]</span>Either the environment is too large or [INAUDIBLE].</p>
    <p class="cue"><span class="time">[17:08]</span>Yeah, possibly.</p>
    <p class="cue"><span class="time">[17:09]</span>For computational reasons?</p>
    <p class="cue"><span class="time">[17:10]</span>Yeah.</p>
    <p class="cue"><span class="time">[17:10]</span>Yeah, you might have games--</p>
    <p class="cue"><span class="time">[17:13]</span>I mean, look at open World Games.</p>
    <p class="cue"><span class="time">[17:15]</span>Truly, you could argue--</p>
    <p class="cue"><span class="time">[17:18]</span>I don&#x27;t know.</p>
    <p class="cue"><span class="time">[17:18]</span>There are some games where you might press start</p>
    <p class="cue"><span class="time">[17:20]</span>and you see the entire environment.</p>
    <p class="cue"><span class="time">[17:22]</span>But who cares of what&#x27;s happening 20,000 kilometers West</p>
    <p class="cue"><span class="time">[17:27]</span>of you if you&#x27;re in a certain location.</p>
    <p class="cue"><span class="time">[17:29]</span>That might not influence your strategy.</p>
    <p class="cue"><span class="time">[17:31]</span>So you might actually put some trust circle or some circle</p>
    <p class="cue"><span class="time">[17:38]</span>in which you observe, which you think</p>
    <p class="cue"><span class="time">[17:39]</span>has 99% of the information you need, possibly</p>
    <p class="cue"><span class="time">[17:42]</span>for computational reasons.</p>
    <p class="cue"><span class="time">[17:43]</span>That&#x27;s a good point.</p>
    <p class="cue"><span class="time">[17:45]</span>OK.</p>
    <p class="cue"><span class="time">[17:46]</span>Let&#x27;s get to a practical example of a reinforcement learning</p>
    <p class="cue"><span class="time">[17:49]</span>algorithm and develop it together.</p>
    <p class="cue"><span class="time">[17:53]</span>This example is called recycling is good because recycling</p>
    <p class="cue"><span class="time">[17:56]</span>is good, but also because it&#x27;s a simple example illustrative</p>
    <p class="cue"><span class="time">[18:00]</span>of reinforcement learning.</p>
    <p class="cue"><span class="time">[18:01]</span>So let&#x27;s say we have a small environment with five states.</p>
    <p class="cue"><span class="time">[18:08]</span>There is a starting state marked in brown, which is state 2.</p>
    <p class="cue"><span class="time">[18:13]</span>It&#x27;s our initial state.</p>
    <p class="cue"><span class="time">[18:16]</span>And then on the right side--</p>
    <p class="cue"><span class="time">[18:19]</span>sorry.</p>
    <p class="cue"><span class="time">[18:20]</span>On the left side, you have state 1, which is a garbage.</p>
    <p class="cue"><span class="time">[18:24]</span>And it&#x27;s great to get to the garbage</p>
    <p class="cue"><span class="time">[18:25]</span>because you&#x27;re going to be able to put in the garbage</p>
    <p class="cue"><span class="time">[18:29]</span>the stuff that you have in your hands.</p>
    <p class="cue"><span class="time">[18:33]</span>You&#x27;re trying to throw away some garbage.</p>
    <p class="cue"><span class="time">[18:35]</span>And the garbage can happens to be there.</p>
    <p class="cue"><span class="time">[18:37]</span>And so we would expect there to be a reward.</p>
    <p class="cue"><span class="time">[18:39]</span>On the other side, if you actually go to the right,</p>
    <p class="cue"><span class="time">[18:42]</span>you might pass by state 3, which is empty.</p>
    <p class="cue"><span class="time">[18:45]</span>You might pass by state 4, where there is a chocolate</p>
    <p class="cue"><span class="time">[18:49]</span>packaging that is left on the ground that you can pick up,</p>
    <p class="cue"><span class="time">[18:53]</span>and it&#x27;s good to pick it up.</p>
    <p class="cue"><span class="time">[18:56]</span>And then on stage 5--</p>
    <p class="cue"><span class="time">[18:57]</span>state 5, you have the recycle bin,</p>
    <p class="cue"><span class="time">[18:59]</span>which is more valuable than the garbage can</p>
    <p class="cue"><span class="time">[19:02]</span>because you can recycle and you should</p>
    <p class="cue"><span class="time">[19:03]</span>get better rewards for that.</p>
    <p class="cue"><span class="time">[19:06]</span>So that&#x27;s our game.</p>
    <p class="cue"><span class="time">[19:07]</span>In this game, we define a reward that</p>
    <p class="cue"><span class="time">[19:09]</span>is associated with the type of behaviors</p>
    <p class="cue"><span class="time">[19:12]</span>that we want the agents to learn.</p>
    <p class="cue"><span class="time">[19:14]</span>And the reward is as follows.</p>
    <p class="cue"><span class="time">[19:16]</span>That&#x27;s just one example.</p>
    <p class="cue"><span class="time">[19:17]</span>Plus 2 for throwing your garbage in the normal can,</p>
    <p class="cue"><span class="time">[19:21]</span>plus 1 for picking up the chocolate packaging,</p>
    <p class="cue"><span class="time">[19:24]</span>and plus 10 if you manage to make it to the recycle bin.</p>
    <p class="cue"><span class="time">[19:29]</span>Is it clear?</p>
    <p class="cue"><span class="time">[19:31]</span>Now, the goal will be--</p>
    <p class="cue"><span class="time">[19:34]</span>and that&#x27;s the case in reinforcement learning often</p>
    <p class="cue"><span class="time">[19:37]</span>time to maximize the return.</p>
    <p class="cue"><span class="time">[19:41]</span>We&#x27;ll define formally the return.</p>
    <p class="cue"><span class="time">[19:43]</span>But think about it as maximize the amount of rewards</p>
    <p class="cue"><span class="time">[19:45]</span>that you get as you go through this journey,</p>
    <p class="cue"><span class="time">[19:47]</span>and you make your decisions.</p>
    <p class="cue"><span class="time">[19:49]</span>In this specific game, we have five states,</p>
    <p class="cue"><span class="time">[19:52]</span>and there&#x27;s three types of states.</p>
    <p class="cue"><span class="time">[19:53]</span>In brown is the initial states.</p>
    <p class="cue"><span class="time">[19:56]</span>We have normal states, and we have in blue, terminal states.</p>
    <p class="cue"><span class="time">[20:00]</span>When you get to a terminal state in reinforcement learning,</p>
    <p class="cue"><span class="time">[20:04]</span>it will typically end the game.</p>
    <p class="cue"><span class="time">[20:07]</span>It will end one episode of the game.</p>
    <p class="cue"><span class="time">[20:11]</span>You&#x27;ll move to another episode.</p>
    <p class="cue"><span class="time">[20:13]</span>You&#x27;ll get back to the starting state or initial state</p>
    <p class="cue"><span class="time">[20:15]</span>and you&#x27;ll redo another episode.</p>
    <p class="cue"><span class="time">[20:18]</span>The possible actions for agent here</p>
    <p class="cue"><span class="time">[20:20]</span>are going to be fairly simple, left and right.</p>
    <p class="cue"><span class="time">[20:25]</span>And we&#x27;re going to add an additional rule that</p>
    <p class="cue"><span class="time">[20:27]</span>is important, which is that the garbage collector comes</p>
    <p class="cue"><span class="time">[20:30]</span>in three minutes.</p>
    <p class="cue"><span class="time">[20:32]</span>And it takes a minute to get from one state to the other.</p>
    <p class="cue"><span class="time">[20:36]</span>Why is that an important rule to add to the game?</p>
    <p class="cue"><span class="time">[20:41]</span>Can you guess?</p>
    <p class="cue"><span class="time">[20:44]</span>Yeah?</p>
    <p class="cue"><span class="time">[20:45]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[20:47]</span>Yeah.</p>
    <p class="cue"><span class="time">[20:47]</span>Otherwise, you just go back and forth between stage 3</p>
    <p class="cue"><span class="time">[20:51]</span>and stage 4.</p>
    <p class="cue"><span class="time">[20:51]</span>You just collect a bunch of chocolate packaging</p>
    <p class="cue"><span class="time">[20:53]</span>and you never make it to the bin.</p>
    <p class="cue"><span class="time">[20:55]</span>And so it&#x27;s not what we want.</p>
    <p class="cue"><span class="time">[20:58]</span>Yeah.</p>
    <p class="cue"><span class="time">[21:00]</span>OK.</p>
    <p class="cue"><span class="time">[21:01]</span>So how do we define the long-term return?</p>
    <p class="cue"><span class="time">[21:03]</span>The long-term return is going to be</p>
    <p class="cue"><span class="time">[21:04]</span>defined as capital R, which is the sum of rewards</p>
    <p class="cue"><span class="time">[21:12]</span>with a discount.</p>
    <p class="cue"><span class="time">[21:14]</span>Discount is a very important concept</p>
    <p class="cue"><span class="time">[21:17]</span>in reinforcement learning.</p>
    <p class="cue"><span class="time">[21:19]</span>It&#x27;s also a very natural concept to think about.</p>
    <p class="cue"><span class="time">[21:22]</span>Can you think of what the discount would represent in</p>
    <p class="cue"><span class="time">[21:26]</span>for humans?</p>
    <p class="cue"><span class="time">[21:27]</span>Do you have an example of what it could be?</p>
    <p class="cue"><span class="time">[21:29]</span>Yeah?</p>
    <p class="cue"><span class="time">[21:30]</span>The value of money and time.</p>
    <p class="cue"><span class="time">[21:31]</span>Huh?</p>
    <p class="cue"><span class="time">[21:31]</span>The value of money and time.</p>
    <p class="cue"><span class="time">[21:33]</span>Yeah, the value of money and time.</p>
    <p class="cue"><span class="time">[21:35]</span>Exactly.</p>
    <p class="cue"><span class="time">[21:36]</span>Or the energy that a robot might have.</p>
    <p class="cue"><span class="time">[21:38]</span>Things like that.</p>
    <p class="cue"><span class="time">[21:39]</span>Yeah.</p>
    <p class="cue"><span class="time">[21:39]</span>You would rather get a $1 now than $1</p>
    <p class="cue"><span class="time">[21:43]</span>in 10 years knowing that there is some inflation, for example.</p>
    <p class="cue"><span class="time">[21:47]</span>That&#x27;s the example of a discount.</p>
    <p class="cue"><span class="time">[21:48]</span>And reinforcement learning is the same.</p>
    <p class="cue"><span class="time">[21:50]</span>Let&#x27;s say you have a strategy that takes so much time.</p>
    <p class="cue"><span class="time">[21:53]</span>You need to discount it because your robot might lose energy</p>
    <p class="cue"><span class="time">[21:56]</span>as it going through it, for example.</p>
    <p class="cue"><span class="time">[21:59]</span>Discounts can vary, but they stay between 0 and 1.</p>
    <p class="cue"><span class="time">[22:05]</span>So what is the best strategy to follow if gamma, the discount,</p>
    <p class="cue"><span class="time">[22:10]</span>is equal to 1, meaning time doesn&#x27;t matter here</p>
    <p class="cue"><span class="time">[22:15]</span>if it&#x27;s longer or shorter, just want to maximize the return?</p>
    <p class="cue"><span class="time">[22:20]</span>Best strategy to follow?</p>
    <p class="cue"><span class="time">[22:27]</span>And let&#x27;s give it a try.</p>
    <p class="cue"><span class="time">[22:29]</span>Someone who hasn&#x27;t spoken yet.</p>
    <p class="cue"><span class="time">[22:40]</span>Yes?</p>
    <p class="cue"><span class="time">[22:41]</span>It just bounce around.</p>
    <p class="cue"><span class="time">[22:47]</span>Bounce around.</p>
    <p class="cue"><span class="time">[22:48]</span>But remember, the rule of three minutes.</p>
    <p class="cue"><span class="time">[22:51]</span>You can&#x27;t bounce around because you will not</p>
    <p class="cue"><span class="time">[22:53]</span>get to the terminal state before the time allotted is done.</p>
    <p class="cue"><span class="time">[22:58]</span>But that would be a good idea if this rule was not true.</p>
    <p class="cue"><span class="time">[23:03]</span>What else could you do?</p>
    <p class="cue"><span class="time">[23:11]</span>Any idea?</p>
    <p class="cue"><span class="time">[23:12]</span>It&#x27;s an easy one, though, not too hard.</p>
    <p class="cue"><span class="time">[23:19]</span>Best strategy for gamma equals 1.</p>
    <p class="cue"><span class="time">[23:22]</span>And give me also the maximum reward you would get.</p>
    <p class="cue"><span class="time">[23:30]</span>People are sleepy today.</p>
    <p class="cue"><span class="time">[23:31]</span>Yeah?</p>
    <p class="cue"><span class="time">[23:31]</span>Recycle.</p>
    <p class="cue"><span class="time">[23:32]</span>Go recycle.</p>
    <p class="cue"><span class="time">[23:34]</span>Go to the recycle.</p>
    <p class="cue"><span class="time">[23:35]</span>Right.</p>
    <p class="cue"><span class="time">[23:38]</span>Yeah.</p>
    <p class="cue"><span class="time">[23:38]</span>That&#x27;s right.</p>
    <p class="cue"><span class="time">[23:39]</span>Thank you.</p>
    <p class="cue"><span class="time">[23:42]</span>Right.</p>
    <p class="cue"><span class="time">[23:43]</span>And then what&#x27;s your-- sorry.</p>
    <p class="cue"><span class="time">[23:44]</span>What&#x27;s your total reward?</p>
    <p class="cue"><span class="time">[23:48]</span>11.</p>
    <p class="cue"><span class="time">[23:49]</span>Yeah, that&#x27;s right, 11.</p>
    <p class="cue"><span class="time">[23:51]</span>So that&#x27;s where we get terminal state,</p>
    <p class="cue"><span class="time">[23:53]</span>and we grab our reward of 11.</p>
    <p class="cue"><span class="time">[23:56]</span>Very good.</p>
    <p class="cue"><span class="time">[23:57]</span>Now, assuming 0.9 for gamma, we&#x27;re</p>
    <p class="cue"><span class="time">[24:01]</span>going to complexify things a little bit.</p>
    <p class="cue"><span class="time">[24:03]</span>I&#x27;m going to walk you through a very simple algorithm that</p>
    <p class="cue"><span class="time">[24:06]</span>allows us to determine the best strategy.</p>
    <p class="cue"><span class="time">[24:09]</span>And we will put our numbers in a matrix.</p>
    <p class="cue"><span class="time">[24:12]</span>So for instance, we&#x27;ll define a Q-table.</p>
    <p class="cue"><span class="time">[24:17]</span>And Q stands-- it&#x27;s a value function where</p>
    <p class="cue"><span class="time">[24:22]</span>the name Q-learning, Q-star.</p>
    <p class="cue"><span class="time">[24:25]</span>You might have heard all of these things come</p>
    <p class="cue"><span class="time">[24:28]</span>from Q-learning.</p>
    <p class="cue"><span class="time">[24:30]</span>And so let&#x27;s say we have a Q-table, which</p>
    <p class="cue"><span class="time">[24:32]</span>has the size of number of states times number of actions.</p>
    <p class="cue"><span class="time">[24:38]</span>So five rows, two columns, in our case.</p>
    <p class="cue"><span class="time">[24:42]</span>Every entry of the Q-table is essentially</p>
    <p class="cue"><span class="time">[24:46]</span>representing how good it is to take action A in state B.</p>
    <p class="cue"><span class="time">[24:55]</span>Do you agree that if we had a table with these numbers,</p>
    <p class="cue"><span class="time">[24:59]</span>essentially we solved the problem?</p>
    <p class="cue"><span class="time">[25:01]</span>Meaning at any point, the agent can just look in the table.</p>
    <p class="cue"><span class="time">[25:05]</span>I am in state 3.</p>
    <p class="cue"><span class="time">[25:07]</span>Let&#x27;s look at column 1.</p>
    <p class="cue"><span class="time">[25:08]</span>That would tell me the value of action 1.</p>
    <p class="cue"><span class="time">[25:10]</span>And let&#x27;s look at column 2.</p>
    <p class="cue"><span class="time">[25:11]</span>It would tell me the value of action 2.</p>
    <p class="cue"><span class="time">[25:13]</span>So I have everything I need to make my decisions.</p>
    <p class="cue"><span class="time">[25:18]</span>So that table is really the thing</p>
    <p class="cue"><span class="time">[25:20]</span>you want to find in this exercise.</p>
    <p class="cue"><span class="time">[25:23]</span>Now, the way we will find the table</p>
    <p class="cue"><span class="time">[25:25]</span>is using backtracking algorithm, where we might actually</p>
    <p class="cue"><span class="time">[25:31]</span>codify the environment as a tree and traverse the tree.</p>
    <p class="cue"><span class="time">[25:34]</span>So here&#x27;s what it looks like.</p>
    <p class="cue"><span class="time">[25:36]</span>I start in S2 and I have two options ahead of me.</p>
    <p class="cue"><span class="time">[25:39]</span>I can go to the left where I will get a reward of 2.</p>
    <p class="cue"><span class="time">[25:42]</span>It&#x27;s an immediate reward.</p>
    <p class="cue"><span class="time">[25:44]</span>The immediate reward is not discounted.</p>
    <p class="cue"><span class="time">[25:46]</span>It&#x27;s an immediate reward.</p>
    <p class="cue"><span class="time">[25:48]</span>Remember the formula for R. The immediate reward R0</p>
    <p class="cue"><span class="time">[25:52]</span>is not discounted.</p>
    <p class="cue"><span class="time">[25:54]</span>That would take me to S1.</p>
    <p class="cue"><span class="time">[25:57]</span>It&#x27;s a terminal state, so there&#x27;s nothing to do after.</p>
    <p class="cue"><span class="time">[26:02]</span>Second option, I go to the right, and I get a reward of 0.</p>
    <p class="cue"><span class="time">[26:06]</span>That&#x27;s my immediate reward, and I end up in state 3.</p>
    <p class="cue"><span class="time">[26:09]</span>State 3 is not a terminal state.</p>
    <p class="cue"><span class="time">[26:12]</span>So I can go and do the same exercise from state 3.</p>
    <p class="cue"><span class="time">[26:15]</span>In state 3, I have two options.</p>
    <p class="cue"><span class="time">[26:17]</span>I can go to the left where I would see a reward of 0,</p>
    <p class="cue"><span class="time">[26:20]</span>and I will end up in S2, or I will go to the right,</p>
    <p class="cue"><span class="time">[26:24]</span>and I will get an immediate reward of plus 1.</p>
    <p class="cue"><span class="time">[26:27]</span>It&#x27;s an immediate reward.</p>
    <p class="cue"><span class="time">[26:28]</span>We&#x27;re not discounting it.</p>
    <p class="cue"><span class="time">[26:30]</span>I will end up in S4.</p>
    <p class="cue"><span class="time">[26:31]</span>And from S4, again, I have two options, back</p>
    <p class="cue"><span class="time">[26:34]</span>to the left to S3 with 0 reward, or</p>
    <p class="cue"><span class="time">[26:38]</span>to the right with the amazing reward of plus 10</p>
    <p class="cue"><span class="time">[26:42]</span>and the terminal state of S5.</p>
    <p class="cue"><span class="time">[26:45]</span>So that&#x27;s my map of immediate rewards.</p>
    <p class="cue"><span class="time">[26:49]</span>That&#x27;s not my discounted return.</p>
    <p class="cue"><span class="time">[26:50]</span>So what we&#x27;re going to do now is we&#x27;re</p>
    <p class="cue"><span class="time">[26:52]</span>going to backtrack up the tree in order</p>
    <p class="cue"><span class="time">[26:54]</span>to compute the discounted returns.</p>
    <p class="cue"><span class="time">[26:57]</span>Actually, if I&#x27;m in S3 right here,</p>
    <p class="cue"><span class="time">[27:02]</span>I see that I can get an immediate reward in S4</p>
    <p class="cue"><span class="time">[27:07]</span>of plus 1, and I want to compute my maximum return that I</p>
    <p class="cue"><span class="time">[27:11]</span>can get from when I&#x27;m in S3.</p>
    <p class="cue"><span class="time">[27:14]</span>My maximum return is that in S4, I could get a plus 10.</p>
    <p class="cue"><span class="time">[27:19]</span>But I need to discount that.</p>
    <p class="cue"><span class="time">[27:21]</span>My discount is 0.9, so I multiply 10 by 0.9.</p>
    <p class="cue"><span class="time">[27:26]</span>What it tells me is that from S4,</p>
    <p class="cue"><span class="time">[27:28]</span>I can expect 9 plus 1, which I get as an immediate reward</p>
    <p class="cue"><span class="time">[27:32]</span>from moving from S3 to S4.</p>
    <p class="cue"><span class="time">[27:34]</span>I can update this number to 10.</p>
    <p class="cue"><span class="time">[27:36]</span>Meaning from S3, the best you can hope for</p>
    <p class="cue"><span class="time">[27:40]</span>is a discounted return of 10, which is 1 plus 0.9 times 10.</p>
    <p class="cue"><span class="time">[27:46]</span>Everyone follows?</p>
    <p class="cue"><span class="time">[27:48]</span>Now, let&#x27;s do the same exercise one step before in S2.</p>
    <p class="cue"><span class="time">[27:54]</span>In S2, I have an immediate reward of 0 for going to S3,</p>
    <p class="cue"><span class="time">[28:02]</span>or an immediate reward of 2 for going to S1.</p>
    <p class="cue"><span class="time">[28:06]</span>S1 is not going to be worth it.</p>
    <p class="cue"><span class="time">[28:08]</span>We already know that because when I&#x27;m in S3,</p>
    <p class="cue"><span class="time">[28:11]</span>I can actually expect 10, which I have to discount.</p>
    <p class="cue"><span class="time">[28:16]</span>0.9 times 10 gives me 9 plus 0 immediate reward from S2 to S3.</p>
    <p class="cue"><span class="time">[28:21]</span>That tells me that the discounted return from state 2,</p>
    <p class="cue"><span class="time">[28:25]</span>which is our initial state, is 9.</p>
    <p class="cue"><span class="time">[28:30]</span>Everyone follow?</p>
    <p class="cue"><span class="time">[28:31]</span>Just a simple backtracking.</p>
    <p class="cue"><span class="time">[28:34]</span>Now, I can copy back this, so S3.</p>
    <p class="cue"><span class="time">[28:37]</span>I know that when I&#x27;m in S3, I can expect a 0 immediate reward</p>
    <p class="cue"><span class="time">[28:42]</span>to--</p>
    <p class="cue"><span class="time">[28:45]</span>sorry.</p>
    <p class="cue"><span class="time">[28:46]</span>If I&#x27;m in S2, I can expect 0 immediate reward</p>
    <p class="cue"><span class="time">[28:52]</span>plus a discount times the plus 9 that I could expect in S3.</p>
    <p class="cue"><span class="time">[28:57]</span>And so that gives me values that should cover everything</p>
    <p class="cue"><span class="time">[29:02]</span>that we have in this Q-table.</p>
    <p class="cue"><span class="time">[29:05]</span>So I do that backtracking.</p>
    <p class="cue"><span class="time">[29:07]</span>I copy paste all of that into my Q-table all the way up here,</p>
    <p class="cue"><span class="time">[29:11]</span>and this is what I get.</p>
    <p class="cue"><span class="time">[29:14]</span>We essentially finished the game at this point.</p>
    <p class="cue"><span class="time">[29:16]</span>We [INAUDIBLE] at a certain role.</p>
    <p class="cue"><span class="time">[29:21]</span>So let&#x27;s say I&#x27;m in state number 3.</p>
    <p class="cue"><span class="time">[29:24]</span>I look on the third row of that Q-table.</p>
    <p class="cue"><span class="time">[29:26]</span>And I see that I have two options.</p>
    <p class="cue"><span class="time">[29:28]</span>If I go back to S2, ultimately my discounted return</p>
    <p class="cue"><span class="time">[29:34]</span>will be 8.1.</p>
    <p class="cue"><span class="time">[29:37]</span>If I actually go to S4 on the right,</p>
    <p class="cue"><span class="time">[29:42]</span>I will get 10 because I will get 1</p>
    <p class="cue"><span class="time">[29:44]</span>plus 0.9 times 10, which is 10.</p>
    <p class="cue"><span class="time">[29:48]</span>So this is a toy example, but it tells you</p>
    <p class="cue"><span class="time">[29:51]</span>that if you were able to backtrack</p>
    <p class="cue"><span class="time">[29:53]</span>through the entire environment, you</p>
    <p class="cue"><span class="time">[29:55]</span>will be able to build a massive Q-table</p>
    <p class="cue"><span class="time">[29:57]</span>and you will be able to give it to your agent</p>
    <p class="cue"><span class="time">[30:00]</span>to make its decisions.</p>
    <p class="cue"><span class="time">[30:02]</span>Yeah?</p>
    <p class="cue"><span class="time">[30:03]</span>What is the [INAUDIBLE]?</p>
    <p class="cue"><span class="time">[30:06]</span>Sorry.</p>
    <p class="cue"><span class="time">[30:06]</span>Can you repeat?</p>
    <p class="cue"><span class="time">[30:07]</span>What is the time remaining from the [INAUDIBLE]?</p>
    <p class="cue"><span class="time">[30:10]</span>Yeah.</p>
    <p class="cue"><span class="time">[30:10]</span>Here, I&#x27;m simplifying.</p>
    <p class="cue"><span class="time">[30:11]</span>I&#x27;m not considering the time remaining.</p>
    <p class="cue"><span class="time">[30:14]</span>But in practice, you--</p>
    <p class="cue"><span class="time">[30:16]</span>if I remove the time component, so I remove the fact</p>
    <p class="cue"><span class="time">[30:20]</span>that there&#x27;s a three-minute deadline before the garbage</p>
    <p class="cue"><span class="time">[30:22]</span>collector comes, then this would be slightly more difficult,</p>
    <p class="cue"><span class="time">[30:26]</span>because you would have to do a time series, essentially,</p>
    <p class="cue"><span class="time">[30:29]</span>of adding the discount times the reward that you collect.</p>
    <p class="cue"><span class="time">[30:33]</span>Yeah, but I&#x27;m simplifying here, and that&#x27;s</p>
    <p class="cue"><span class="time">[30:35]</span>why I use the three-minute rule.</p>
    <p class="cue"><span class="time">[30:38]</span>Any question on the Q-table?</p>
    <p class="cue"><span class="time">[30:44]</span>Super.</p>
    <p class="cue"><span class="time">[30:45]</span>OK.</p>
    <p class="cue"><span class="time">[30:45]</span>So this was the Q-table.</p>
    <p class="cue"><span class="time">[30:49]</span>And in fact, we can put together our strategy</p>
    <p class="cue"><span class="time">[30:51]</span>for gamma equals 0.9.</p>
    <p class="cue"><span class="time">[30:54]</span>The best strategy is still the same.</p>
    <p class="cue"><span class="time">[30:56]</span>You go to the right.</p>
    <p class="cue"><span class="time">[30:57]</span>And you can expect a return of nine.</p>
    <p class="cue"><span class="time">[31:02]</span>Now, one of the most important concepts in reinforcement</p>
    <p class="cue"><span class="time">[31:06]</span>learning is this equation on the board called the Bellman</p>
    <p class="cue"><span class="time">[31:10]</span>optimality equation.</p>
    <p class="cue"><span class="time">[31:13]</span>Oftentimes, you&#x27;ll see it noted as Q star of state s and action</p>
    <p class="cue"><span class="time">[31:19]</span>a equals r plus gamma times the max of that same function</p>
    <p class="cue"><span class="time">[31:26]</span>applied to s prime, a prime.</p>
    <p class="cue"><span class="time">[31:31]</span>Let me explain this equation for you,</p>
    <p class="cue"><span class="time">[31:32]</span>because it&#x27;s super important.</p>
    <p class="cue"><span class="time">[31:35]</span>This equation is called the optimality equation</p>
    <p class="cue"><span class="time">[31:39]</span>because your optimal Q-table will follow this equation.</p>
    <p class="cue"><span class="time">[31:43]</span>If you have finished the game, this equation</p>
    <p class="cue"><span class="time">[31:46]</span>can be applied to any state action pair,</p>
    <p class="cue"><span class="time">[31:49]</span>and it will still be true.</p>
    <p class="cue"><span class="time">[31:52]</span>The intuition behind why the Bellman equation</p>
    <p class="cue"><span class="time">[31:55]</span>is the optimality equation is that if you&#x27;re</p>
    <p class="cue"><span class="time">[32:00]</span>in a-- if you have the perfect Q function, Q-table,</p>
    <p class="cue"><span class="time">[32:04]</span>and you&#x27;re in a certain state and you perform a certain action</p>
    <p class="cue"><span class="time">[32:08]</span>a, you will observe a reward.</p>
    <p class="cue"><span class="time">[32:10]</span>And this reward will--</p>
    <p class="cue"><span class="time">[32:13]</span>you have taken an action.</p>
    <p class="cue"><span class="time">[32:15]</span>So you would be in a new state.</p>
    <p class="cue"><span class="time">[32:16]</span>And from that new state, you could repeat what you just did.</p>
    <p class="cue"><span class="time">[32:20]</span>And because you&#x27;ve done the backtracking and stuff</p>
    <p class="cue"><span class="time">[32:23]</span>like that, you will get this equation</p>
    <p class="cue"><span class="time">[32:26]</span>to be true because it&#x27;s the reward</p>
    <p class="cue"><span class="time">[32:28]</span>plus discount times the best next action</p>
    <p class="cue"><span class="time">[32:30]</span>that you could be taking.</p>
    <p class="cue"><span class="time">[32:35]</span>Does that make sense?</p>
    <p class="cue"><span class="time">[32:36]</span>Any questions on that?</p>
    <p class="cue"><span class="time">[32:38]</span>That&#x27;s exactly the backtracking that we did by the way.</p>
    <p class="cue"><span class="time">[32:42]</span>Immediate reward plus discount times the best possible action</p>
    <p class="cue"><span class="time">[32:47]</span>that you can take in the next state, s prime.</p>
    <p class="cue"><span class="time">[32:55]</span>The last concept I&#x27;ll cover in terms of vocabulary</p>
    <p class="cue"><span class="time">[32:58]</span>is the policy.</p>
    <p class="cue"><span class="time">[32:59]</span>The policy is the function that given your state,</p>
    <p class="cue"><span class="time">[33:02]</span>is going to tell you what to do.</p>
    <p class="cue"><span class="time">[33:04]</span>And in Q-learning, the way this policy is defined</p>
    <p class="cue"><span class="time">[33:08]</span>is argmax of Q star across the action.</p>
    <p class="cue"><span class="time">[33:13]</span>So essentially what it says is, look in the table</p>
    <p class="cue"><span class="time">[33:17]</span>and look at a certain state s.</p>
    <p class="cue"><span class="time">[33:19]</span>You want the policy, which is what you should do.</p>
    <p class="cue"><span class="time">[33:22]</span>It&#x27;s the function that tells you our best strategy.</p>
    <p class="cue"><span class="time">[33:25]</span>You just look at the two possible actions,</p>
    <p class="cue"><span class="time">[33:27]</span>which one has the highest Q value, and select that action.</p>
    <p class="cue"><span class="time">[33:32]</span>That&#x27;s it.</p>
    <p class="cue"><span class="time">[33:37]</span>This is a very simple example, but it&#x27;s</p>
    <p class="cue"><span class="time">[33:39]</span>the core of Q-learning that later on you</p>
    <p class="cue"><span class="time">[33:44]</span>will use policies widely.</p>
    <p class="cue"><span class="time">[33:46]</span>There&#x27;s a lot of reinforcement learning algorithms.</p>
    <p class="cue"><span class="time">[33:48]</span>But this concept of understanding the policies,</p>
    <p class="cue"><span class="time">[33:50]</span>the function telling us our best strategy in Q-learning,</p>
    <p class="cue"><span class="time">[33:53]</span>it&#x27;s the argmax of the best q-value in a given state.</p>
    <p class="cue"><span class="time">[33:56]</span>It tells you which action to take.</p>
    <p class="cue"><span class="time">[33:58]</span>That&#x27;s the core thing you need to understand.</p>
    <p class="cue"><span class="time">[34:01]</span>So remember this Bellman equation</p>
    <p class="cue"><span class="time">[34:03]</span>because we&#x27;re going to reuse it in a bit.</p>
    <p class="cue"><span class="time">[34:07]</span>The main issue with this approach of a Q table</p>
    <p class="cue"><span class="time">[34:13]</span>is that state and action spaces can be super large.</p>
    <p class="cue"><span class="time">[34:19]</span>And having a matrix that you discover through backtracking</p>
    <p class="cue"><span class="time">[34:25]</span>and where every time you want to do an action,</p>
    <p class="cue"><span class="time">[34:27]</span>you have to look up the given states, the possible action,</p>
    <p class="cue"><span class="time">[34:31]</span>it becomes impossible.</p>
    <p class="cue"><span class="time">[34:32]</span>Imagine you using this algorithm for the game of Go, where</p>
    <p class="cue"><span class="time">[34:38]</span>there&#x27;s so many states, there are so many possible actions.</p>
    <p class="cue"><span class="time">[34:41]</span>You can put your stone anywhere on the board.</p>
    <p class="cue"><span class="time">[34:44]</span>You can imagine how big this matrix becomes</p>
    <p class="cue"><span class="time">[34:46]</span>and how impossible it is to use.</p>
    <p class="cue"><span class="time">[34:49]</span>So that&#x27;s our problem.</p>
    <p class="cue"><span class="time">[34:51]</span>And that&#x27;s the moment where deep learning comes into play.</p>
    <p class="cue"><span class="time">[34:57]</span>So let&#x27;s look at it.</p>
    <p class="cue"><span class="time">[35:02]</span>Actually, before I go there, I&#x27;m just</p>
    <p class="cue"><span class="time">[35:04]</span>going to cover some vocabulary.</p>
    <p class="cue"><span class="time">[35:05]</span>We said the environment, the agent, the state, the action,</p>
    <p class="cue"><span class="time">[35:08]</span>the reward, the total return, and the discount factor.</p>
    <p class="cue"><span class="time">[35:10]</span>We learned all of that.</p>
    <p class="cue"><span class="time">[35:11]</span>We saw that the Q-table is the matrix of entries representing</p>
    <p class="cue"><span class="time">[35:14]</span>how good is it to take action a in state s.</p>
    <p class="cue"><span class="time">[35:18]</span>And the policy is the function that</p>
    <p class="cue"><span class="time">[35:19]</span>tells us what&#x27;s the best strategy to adopt.</p>
    <p class="cue"><span class="time">[35:21]</span>And the Bellman equation is satisfied</p>
    <p class="cue"><span class="time">[35:23]</span>by the optimal Q-table.</p>
    <p class="cue"><span class="time">[35:29]</span>So let&#x27;s get to deep Q-learning, which</p>
    <p class="cue"><span class="time">[35:31]</span>is what I was about to say, is we</p>
    <p class="cue"><span class="time">[35:33]</span>are going to frame the problem slightly differently.</p>
    <p class="cue"><span class="time">[35:36]</span>So instead of using a Q-table, we&#x27;re</p>
    <p class="cue"><span class="time">[35:39]</span>going to use the fact that neural networks are</p>
    <p class="cue"><span class="time">[35:42]</span>universal function approximators.</p>
    <p class="cue"><span class="time">[35:44]</span>And we&#x27;re going to define a Q function that&#x27;s essentially</p>
    <p class="cue"><span class="time">[35:47]</span>a neural network, so that the function can</p>
    <p class="cue"><span class="time">[35:50]</span>take a state s and an action a and tell you how</p>
    <p class="cue"><span class="time">[35:55]</span>good that action is in state s.</p>
    <p class="cue"><span class="time">[35:59]</span>So instead of a lookup in a matrix,</p>
    <p class="cue"><span class="time">[36:01]</span>you just run a forward pass in a neural network,</p>
    <p class="cue"><span class="time">[36:04]</span>and it gives you the answer.</p>
    <p class="cue"><span class="time">[36:06]</span>That feels like a better solution for games</p>
    <p class="cue"><span class="time">[36:08]</span>where there&#x27;s a lot of states and a lot of actions.</p>
    <p class="cue"><span class="time">[36:14]</span>So here is a same problem statement.</p>
    <p class="cue"><span class="time">[36:17]</span>In the past, we looked for a Q-table.</p>
    <p class="cue"><span class="time">[36:18]</span>And this time, we will look for a neural network.</p>
    <p class="cue"><span class="time">[36:22]</span>One of the things we&#x27;re going to do</p>
    <p class="cue"><span class="time">[36:24]</span>is to define the output layer to have two outputs.</p>
    <p class="cue"><span class="time">[36:27]</span>So given a certain state as input,</p>
    <p class="cue"><span class="time">[36:29]</span>think about it as a one hot vector encoding the state.</p>
    <p class="cue"><span class="time">[36:33]</span>So this one is the example of state 2, 01000.</p>
    <p class="cue"><span class="time">[36:37]</span>If you pass state 2 in this Q function with multiple layers,</p>
    <p class="cue"><span class="time">[36:42]</span>it will give you two outputs, one output that corresponds</p>
    <p class="cue"><span class="time">[36:46]</span>to Q of s action right, and the other one Q of s action</p>
    <p class="cue"><span class="time">[36:53]</span>left, because it&#x27;s the two actions.</p>
    <p class="cue"><span class="time">[36:56]</span>If we had more actions to take, we</p>
    <p class="cue"><span class="time">[36:58]</span>would just increase the output layer,</p>
    <p class="cue"><span class="time">[36:59]</span>and we might have many more neurons in the output layer.</p>
    <p class="cue"><span class="time">[37:07]</span>So the big question is, how the hell</p>
    <p class="cue"><span class="time">[37:10]</span>are we going to train that network?</p>
    <p class="cue"><span class="time">[37:13]</span>Because we&#x27;re not in classic supervised learning.</p>
    <p class="cue"><span class="time">[37:15]</span>We don&#x27;t have labels.</p>
    <p class="cue"><span class="time">[37:20]</span>So this one is a hard question, but what would you</p>
    <p class="cue"><span class="time">[37:25]</span>do given-- we don&#x27;t have traditional x and y pairs.</p>
    <p class="cue"><span class="time">[37:29]</span>How are you going to train these neural network?</p>
    <p class="cue"><span class="time">[37:35]</span>Because remember at the beginning,</p>
    <p class="cue"><span class="time">[37:37]</span>this neural network will give you garbage.</p>
    <p class="cue"><span class="time">[37:38]</span>It will take a state s and it might tell you go to the left</p>
    <p class="cue"><span class="time">[37:42]</span>or to the right, but it&#x27;s completely random.</p>
    <p class="cue"><span class="time">[37:44]</span>So how are you going to tune it to the level</p>
    <p class="cue"><span class="time">[37:46]</span>where it makes really good decisions?</p>
    <p class="cue"><span class="time">[37:53]</span>Yes?</p>
    <p class="cue"><span class="time">[37:54]</span>We assume based on prior knowledge.</p>
    <p class="cue"><span class="time">[37:58]</span>Assume based on some prior knowledge.</p>
    <p class="cue"><span class="time">[38:01]</span>Tell me more.</p>
    <p class="cue"><span class="time">[38:03]</span>If you have-- people will [INAUDIBLE]</p>
    <p class="cue"><span class="time">[38:06]</span>if you have some idea [INAUDIBLE].</p>
    <p class="cue"><span class="time">[38:09]</span>So what are the things we know about this problem right now?</p>
    <p class="cue"><span class="time">[38:12]</span>What are the rules of the game that we could use in order to--</p>
    <p class="cue"><span class="time">[38:19]</span>I&#x27;m seeing what you say.</p>
    <p class="cue"><span class="time">[38:20]</span>You say we could estimate what good looks like,</p>
    <p class="cue"><span class="time">[38:22]</span>but based on what?</p>
    <p class="cue"><span class="time">[38:27]</span>We reduce [INAUDIBLE].</p>
    <p class="cue"><span class="time">[38:32]</span>OK, so reward structure.</p>
    <p class="cue"><span class="time">[38:34]</span>You&#x27;re saying that&#x27;s one thing we have in every game.</p>
    <p class="cue"><span class="time">[38:36]</span>We have a reward structure for every state.</p>
    <p class="cue"><span class="time">[38:38]</span>That definitely should be used in order</p>
    <p class="cue"><span class="time">[38:41]</span>to estimate the good-- what a good decision looks like.</p>
    <p class="cue"><span class="time">[38:44]</span>Yeah, the problem is not in every state</p>
    <p class="cue"><span class="time">[38:46]</span>you will see a reward.</p>
    <p class="cue"><span class="time">[38:48]</span>And if you look at many games like Go,</p>
    <p class="cue"><span class="time">[38:52]</span>you might not see a reward until 50 moves.</p>
    <p class="cue"><span class="time">[38:56]</span>So what do you do in this case?</p>
    <p class="cue"><span class="time">[38:59]</span>Yes?</p>
    <p class="cue"><span class="time">[39:00]</span>Can we run through a bunch of actions</p>
    <p class="cue"><span class="time">[39:05]</span>and see what the output is, and get more data</p>
    <p class="cue"><span class="time">[39:07]</span>to train the neural net?</p>
    <p class="cue"><span class="time">[39:09]</span>Yeah.</p>
    <p class="cue"><span class="time">[39:09]</span>So you could.</p>
    <p class="cue"><span class="time">[39:10]</span>You&#x27;re actually bringing up a tree search.</p>
    <p class="cue"><span class="time">[39:15]</span>You go down the tree, you do every possible action,</p>
    <p class="cue"><span class="time">[39:18]</span>and then you backtrack.</p>
    <p class="cue"><span class="time">[39:19]</span>Not every possible action.</p>
    <p class="cue"><span class="time">[39:21]</span>So which actions?</p>
    <p class="cue"><span class="time">[39:23]</span>Trying to spread it out.</p>
    <p class="cue"><span class="time">[39:25]</span>OK.</p>
    <p class="cue"><span class="time">[39:26]</span>That&#x27;s-- we&#x27;re getting there.</p>
    <p class="cue"><span class="time">[39:27]</span>So first possibility is we just go down the tree.</p>
    <p class="cue"><span class="time">[39:30]</span>In the game of Go, you could put your stone everywhere.</p>
    <p class="cue"><span class="time">[39:33]</span>So the tree already start by a 13 by 13 options.</p>
    <p class="cue"><span class="time">[39:36]</span>And then it&#x27;s exponentially grows.</p>
    <p class="cue"><span class="time">[39:39]</span>Impossible.</p>
    <p class="cue"><span class="time">[39:40]</span>It&#x27;s intractable.</p>
    <p class="cue"><span class="time">[39:41]</span>But what you said is, what if there are certain actions that</p>
    <p class="cue"><span class="time">[39:43]</span>are more likely than others?</p>
    <p class="cue"><span class="time">[39:45]</span>Do we need actually to explore the entire tree?</p>
    <p class="cue"><span class="time">[39:47]</span>What&#x27;s this?</p>
    <p class="cue"><span class="time">[39:48]</span>What are you using when you&#x27;re saying that?</p>
    <p class="cue"><span class="time">[39:50]</span>How do you determine what action might</p>
    <p class="cue"><span class="time">[39:52]</span>be better than another one?</p>
    <p class="cue"><span class="time">[39:53]</span>Expected return.</p>
    <p class="cue"><span class="time">[39:55]</span>Expected return.</p>
    <p class="cue"><span class="time">[39:56]</span>We&#x27;re getting close.</p>
    <p class="cue"><span class="time">[39:57]</span>Yeah.</p>
    <p class="cue"><span class="time">[39:58]</span>But how do you know the expected return</p>
    <p class="cue"><span class="time">[39:59]</span>without going through the tree ones, at least?</p>
    <p class="cue"><span class="time">[40:03]</span>You can estimate [INAUDIBLE].</p>
    <p class="cue"><span class="time">[40:05]</span>OK, you can estimate it using what?</p>
    <p class="cue"><span class="time">[40:08]</span>Is it an equation?</p>
    <p class="cue"><span class="time">[40:10]</span>Yeah, maybe.</p>
    <p class="cue"><span class="time">[40:13]</span>So that&#x27;s exactly what we&#x27;re going to do actually,</p>
    <p class="cue"><span class="time">[40:15]</span>but we&#x27;re going to use the Bellman equation.</p>
    <p class="cue"><span class="time">[40:19]</span>Because there are two things we know about this problem.</p>
    <p class="cue"><span class="time">[40:21]</span>We know the reward structure, which you brought up.</p>
    <p class="cue"><span class="time">[40:24]</span>And we also know that the perfect Q function will</p>
    <p class="cue"><span class="time">[40:27]</span>follow the Bellman equation.</p>
    <p class="cue"><span class="time">[40:29]</span>That we know as well.</p>
    <p class="cue"><span class="time">[40:30]</span>At the end, the Bellman equation should be respected.</p>
    <p class="cue"><span class="time">[40:33]</span>Meaning for every state, if you want</p>
    <p class="cue"><span class="time">[40:36]</span>to know the Q value of that state given an action, the way</p>
    <p class="cue"><span class="time">[40:41]</span>you will get that is you will look at the immediate reward</p>
    <p class="cue"><span class="time">[40:44]</span>plus a discount times the best Q value from the next state</p>
    <p class="cue"><span class="time">[40:48]</span>across all actions.</p>
    <p class="cue"><span class="time">[40:50]</span>That equation will be respected.</p>
    <p class="cue"><span class="time">[40:53]</span>So those are the only information</p>
    <p class="cue"><span class="time">[40:54]</span>we have, and we&#x27;re going to use them drastically</p>
    <p class="cue"><span class="time">[40:57]</span>to define our labels and mimic a classic supervised learning</p>
    <p class="cue"><span class="time">[41:01]</span>approach.</p>
    <p class="cue"><span class="time">[41:02]</span>So here&#x27;s what we have.</p>
    <p class="cue"><span class="time">[41:03]</span>We have our neural network.</p>
    <p class="cue"><span class="time">[41:04]</span>We have Qs to the left and Qs to the right that</p>
    <p class="cue"><span class="time">[41:08]</span>represent how good it is to go to the left</p>
    <p class="cue"><span class="time">[41:10]</span>in that state versus the right.</p>
    <p class="cue"><span class="time">[41:12]</span>And then I&#x27;ve pasted the Bellman equation</p>
    <p class="cue"><span class="time">[41:15]</span>on top right of the screen.</p>
    <p class="cue"><span class="time">[41:17]</span>We&#x27;re going to define a loss function.</p>
    <p class="cue"><span class="time">[41:19]</span>So let&#x27;s say for the sake of simplicity</p>
    <p class="cue"><span class="time">[41:20]</span>because those are scalar values that we&#x27;ll</p>
    <p class="cue"><span class="time">[41:23]</span>use L2 loss, quadratic loss, that compares a certain label</p>
    <p class="cue"><span class="time">[41:30]</span>y to a certain Q value of a state in a certain action.</p>
    <p class="cue"><span class="time">[41:37]</span>So what we would like is to minimize this loss function,</p>
    <p class="cue"><span class="time">[41:40]</span>meaning y, and the Q value for a given action in a given state</p>
    <p class="cue"><span class="time">[41:44]</span>is as close as possible to each other.</p>
    <p class="cue"><span class="time">[41:48]</span>And we&#x27;re going to leverage the reward and the Bellman equation.</p>
    <p class="cue"><span class="time">[41:51]</span>So let&#x27;s do two things.</p>
    <p class="cue"><span class="time">[41:53]</span>Right now, we don&#x27;t have a y.</p>
    <p class="cue"><span class="time">[41:55]</span>So in supervised learning, you will have a picture of a cat.</p>
    <p class="cue"><span class="time">[41:57]</span>If there&#x27;s a cat, the y is 1 or 0.</p>
    <p class="cue"><span class="time">[42:00]</span>Here, we don&#x27;t have a y.</p>
    <p class="cue"><span class="time">[42:01]</span>So we have to come up with an estimate of a good y, at least</p>
    <p class="cue"><span class="time">[42:06]</span>better than random.</p>
    <p class="cue"><span class="time">[42:07]</span>So let&#x27;s say at this point in time,</p>
    <p class="cue"><span class="time">[42:11]</span>when I send a state s in the network,</p>
    <p class="cue"><span class="time">[42:14]</span>it turns out that Q of going to the left</p>
    <p class="cue"><span class="time">[42:17]</span>is higher than Q of going to the right, which means that today,</p>
    <p class="cue"><span class="time">[42:21]</span>at that moment, the Q function tells me</p>
    <p class="cue"><span class="time">[42:24]</span>it&#x27;s better to go to the left than to go to the right.</p>
    <p class="cue"><span class="time">[42:27]</span>That is random at the beginning.</p>
    <p class="cue"><span class="time">[42:29]</span>It&#x27;s completely random.</p>
    <p class="cue"><span class="time">[42:31]</span>So what I&#x27;m going to do is I&#x27;m going</p>
    <p class="cue"><span class="time">[42:33]</span>to use as my target value y the immediate reward that I observe</p>
    <p class="cue"><span class="time">[42:39]</span>on the left, plus gamma times the best Q value that I can get.</p>
    <p class="cue"><span class="time">[42:48]</span>So the best action that I could take in the next step</p>
    <p class="cue"><span class="time">[42:51]</span>based on my current Q value.</p>
    <p class="cue"><span class="time">[42:57]</span>That&#x27;s very important.</p>
    <p class="cue"><span class="time">[42:58]</span>So remember, this target is off.</p>
    <p class="cue"><span class="time">[43:01]</span>It&#x27;s not a perfect target, but it&#x27;s better than nothing.</p>
    <p class="cue"><span class="time">[43:05]</span>Meaning not only it tells us, hey, there</p>
    <p class="cue"><span class="time">[43:09]</span>is a good reward to the left.</p>
    <p class="cue"><span class="time">[43:11]</span>We should consider that in saying</p>
    <p class="cue"><span class="time">[43:13]</span>that might be a good move because we&#x27;re</p>
    <p class="cue"><span class="time">[43:15]</span>seeing an immediate reward.</p>
    <p class="cue"><span class="time">[43:17]</span>But on top of that, we also know that at the end of training,</p>
    <p class="cue"><span class="time">[43:21]</span>the Q value should follow the Bellman equation.</p>
    <p class="cue"><span class="time">[43:23]</span>So why don&#x27;t we set the target as the Bellman equation?</p>
    <p class="cue"><span class="time">[43:27]</span>So we add the discounted maximum future reward</p>
    <p class="cue"><span class="time">[43:30]</span>when you are in the next state.</p>
    <p class="cue"><span class="time">[43:32]</span>So you were in state s.</p>
    <p class="cue"><span class="time">[43:33]</span>You go to the left.</p>
    <p class="cue"><span class="time">[43:34]</span>Now, you&#x27;re in state s next left.</p>
    <p class="cue"><span class="time">[43:37]</span>And you look again at your Q values,</p>
    <p class="cue"><span class="time">[43:40]</span>and you select the best one.</p>
    <p class="cue"><span class="time">[43:42]</span>Then you add that number here.</p>
    <p class="cue"><span class="time">[43:44]</span>So there is actually two forward paths in that process.</p>
    <p class="cue"><span class="time">[43:52]</span>There&#x27;s one forward path where you send the state s in Q.</p>
    <p class="cue"><span class="time">[43:57]</span>And you look at the two options, left or right.</p>
    <p class="cue"><span class="time">[43:59]</span>And you&#x27;re like, OK, I&#x27;m going to the left.</p>
    <p class="cue"><span class="time">[44:01]</span>And then you&#x27;re like, I&#x27;m going to compare that value</p>
    <p class="cue"><span class="time">[44:03]</span>to a target y.</p>
    <p class="cue"><span class="time">[44:05]</span>But to get that target y, I need to do another forward path.</p>
    <p class="cue"><span class="time">[44:08]</span>So I take my action left.</p>
    <p class="cue"><span class="time">[44:10]</span>I perform it.</p>
    <p class="cue"><span class="time">[44:11]</span>I get an s prime state, s next.</p>
    <p class="cue"><span class="time">[44:14]</span>And I send that s next into the Q-network.</p>
    <p class="cue"><span class="time">[44:17]</span>I look at the two options I have.</p>
    <p class="cue"><span class="time">[44:18]</span>I pick the best one and I add it here with the discount.</p>
    <p class="cue"><span class="time">[44:25]</span>So fundamentally, what&#x27;s happening is the following,</p>
    <p class="cue"><span class="time">[44:29]</span>is we have a Q-network that&#x27;s random at the beginning.</p>
    <p class="cue"><span class="time">[44:33]</span>It has never observed the rewards.</p>
    <p class="cue"><span class="time">[44:36]</span>We just know that at some point, it will get to the Q--</p>
    <p class="cue"><span class="time">[44:39]</span>it will get to a perfect policy.</p>
    <p class="cue"><span class="time">[44:43]</span>It will get to a perfect Q function.</p>
    <p class="cue"><span class="time">[44:45]</span>But the best we can do right now is to say as a guide</p>
    <p class="cue"><span class="time">[44:50]</span>to our agent, we will look at the immediate reward,</p>
    <p class="cue"><span class="time">[44:53]</span>and we will look at the Bellman equation, which</p>
    <p class="cue"><span class="time">[44:55]</span>should tell us a better estimate than where we are right now.</p>
    <p class="cue"><span class="time">[44:59]</span>And we will try to catch up to that estimate.</p>
    <p class="cue"><span class="time">[45:02]</span>And then we do that again and again.</p>
    <p class="cue"><span class="time">[45:04]</span>So remember, every time your Q gets better,</p>
    <p class="cue"><span class="time">[45:07]</span>it gets better for the next state as well.</p>
    <p class="cue"><span class="time">[45:09]</span>So the Bellman equation tells you estimated</p>
    <p class="cue"><span class="time">[45:12]</span>with the second forward path.</p>
    <p class="cue"><span class="time">[45:14]</span>And you just keep getting better and better as you&#x27;re</p>
    <p class="cue"><span class="time">[45:16]</span>observing more rewards.</p>
    <p class="cue"><span class="time">[45:20]</span>Can I just go into a loop in which</p>
    <p class="cue"><span class="time">[45:24]</span>the next state [INAUDIBLE]?</p>
    <p class="cue"><span class="time">[45:35]</span>How would you describe the loop?</p>
    <p class="cue"><span class="time">[45:37]</span>Right now, imagine this going to the right,</p>
    <p class="cue"><span class="time">[45:41]</span>like the next, state going to the right.</p>
    <p class="cue"><span class="time">[45:43]</span>Yeah.</p>
    <p class="cue"><span class="time">[45:44]</span>For going to the right, you again need the target.</p>
    <p class="cue"><span class="time">[45:47]</span>Yeah, you would stop at that point.</p>
    <p class="cue"><span class="time">[45:49]</span>So what-- yeah, that&#x27;s a good question.</p>
    <p class="cue"><span class="time">[45:51]</span>I&#x27;ll show you how we fix certain things,</p>
    <p class="cue"><span class="time">[45:53]</span>but you do only one step.</p>
    <p class="cue"><span class="time">[45:55]</span>Meaning you have your Q-value at this point.</p>
    <p class="cue"><span class="time">[45:59]</span>And it tells you go to the left.</p>
    <p class="cue"><span class="time">[46:01]</span>And you just want to target y.</p>
    <p class="cue"><span class="time">[46:03]</span>So what you do is you put left and you look at your next state.</p>
    <p class="cue"><span class="time">[46:07]</span>You forward propagate your next state.</p>
    <p class="cue"><span class="time">[46:09]</span>You look at the two options.</p>
    <p class="cue"><span class="time">[46:10]</span>You pick the best.</p>
    <p class="cue"><span class="time">[46:11]</span>You don&#x27;t go further.</p>
    <p class="cue"><span class="time">[46:12]</span>You just use that one step.</p>
    <p class="cue"><span class="time">[46:14]</span>You look one step ahead essentially.</p>
    <p class="cue"><span class="time">[46:16]</span>You don&#x27;t look multiple steps ahead.</p>
    <p class="cue"><span class="time">[46:17]</span>You could, but it would be more computationally heavy to do</p>
    <p class="cue"><span class="time">[46:20]</span>one more step again, and so on.</p>
    <p class="cue"><span class="time">[46:24]</span>Yeah?</p>
    <p class="cue"><span class="time">[46:26]</span>It seems like you&#x27;re learning to function locally.</p>
    <p class="cue"><span class="time">[46:29]</span>Some [INAUDIBLE] information about the reward</p>
    <p class="cue"><span class="time">[46:32]</span>is coming from what the reward would think [INAUDIBLE].</p>
    <p class="cue"><span class="time">[46:36]</span>So if people know how fast that [INAUDIBLE] is, the [INAUDIBLE].</p>
    <p class="cue"><span class="time">[46:44]</span>It will typically be a function of the environment, the state</p>
    <p class="cue"><span class="time">[46:47]</span>space, how long it will take to converge.</p>
    <p class="cue"><span class="time">[46:50]</span>But you&#x27;re perfectly right that as the Q function gets better,</p>
    <p class="cue"><span class="time">[46:56]</span>the estimate y also gets better.</p>
    <p class="cue"><span class="time">[46:58]</span>So the two things get better together,</p>
    <p class="cue"><span class="time">[47:00]</span>because the y is based on the Q function.</p>
    <p class="cue"><span class="time">[47:03]</span>And if the state space is massive,</p>
    <p class="cue"><span class="time">[47:05]</span>you might have a very difficult time training this model.</p>
    <p class="cue"><span class="time">[47:09]</span>There&#x27;s better approaches that we&#x27;ll see later.</p>
    <p class="cue"><span class="time">[47:13]</span>Yeah.</p>
    <p class="cue"><span class="time">[47:14]</span>Right.</p>
    <p class="cue"><span class="time">[47:15]</span>There was a question there.</p>
    <p class="cue"><span class="time">[47:16]</span>So if you-- I&#x27;m sorry.</p>
    <p class="cue"><span class="time">[47:19]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[47:23]</span>Yeah.</p>
    <p class="cue"><span class="time">[47:23]</span>So here I&#x27;m just saying-- let&#x27;s say when you send state s in Q,</p>
    <p class="cue"><span class="time">[47:27]</span>the left happens to be higher than right.</p>
    <p class="cue"><span class="time">[47:29]</span>But the same happens on the other side.</p>
    <p class="cue"><span class="time">[47:31]</span>Let&#x27;s say the left is worse than right.</p>
    <p class="cue"><span class="time">[47:34]</span>Then what you will do is you will define your target</p>
    <p class="cue"><span class="time">[47:37]</span>y as the reward that you observe on the right.</p>
    <p class="cue"><span class="time">[47:41]</span>Plus, from the next state of having</p>
    <p class="cue"><span class="time">[47:43]</span>gone to the right, what&#x27;s the best action and what&#x27;s</p>
    <p class="cue"><span class="time">[47:46]</span>the Q value for that pair?</p>
    <p class="cue"><span class="time">[47:48]</span>And then it will give you the target for that scenario.</p>
    <p class="cue"><span class="time">[47:55]</span>So one complication with this training</p>
    <p class="cue"><span class="time">[47:58]</span>is that when you want to differentiate L,</p>
    <p class="cue"><span class="time">[48:01]</span>so you want to perform a backup propagation,</p>
    <p class="cue"><span class="time">[48:03]</span>you want to take the derivative of L</p>
    <p class="cue"><span class="time">[48:05]</span>with respect to the parameters of the network,</p>
    <p class="cue"><span class="time">[48:07]</span>you want y to be a fixed thing.</p>
    <p class="cue"><span class="time">[48:09]</span>Because in supervised learning, y is not differentiable.</p>
    <p class="cue"><span class="time">[48:12]</span>It&#x27;s just a fixed number 0 or 1 or a certain number.</p>
    <p class="cue"><span class="time">[48:15]</span>So here, we&#x27;re going to simplify.</p>
    <p class="cue"><span class="time">[48:17]</span>And we&#x27;re going to say this term that</p>
    <p class="cue"><span class="time">[48:19]</span>is dependent on the Q network.</p>
    <p class="cue"><span class="time">[48:20]</span>So technically, this term has parameters.</p>
    <p class="cue"><span class="time">[48:23]</span>So if you actually differentiate it, it will give you a value.</p>
    <p class="cue"><span class="time">[48:25]</span>We&#x27;ll just hold it fixed.</p>
    <p class="cue"><span class="time">[48:29]</span>So we say, we do use our Q network</p>
    <p class="cue"><span class="time">[48:32]</span>to perform an estimate of our y, but we will not</p>
    <p class="cue"><span class="time">[48:35]</span>differentiate it.</p>
    <p class="cue"><span class="time">[48:36]</span>We will say it&#x27;s not--</p>
    <p class="cue"><span class="time">[48:37]</span>it&#x27;s fixed.</p>
    <p class="cue"><span class="time">[48:38]</span>Yeah?</p>
    <p class="cue"><span class="time">[48:39]</span>Is that why we discount this [INAUDIBLE]?</p>
    <p class="cue"><span class="time">[48:44]</span>Because, going back to the reason we discount,</p>
    <p class="cue"><span class="time">[48:47]</span>it&#x27;s like the value of time.</p>
    <p class="cue"><span class="time">[48:49]</span>It&#x27;s like you probably want to say</p>
    <p class="cue"><span class="time">[48:50]</span>if you can win the game in 10 moves,</p>
    <p class="cue"><span class="time">[48:53]</span>win it in 10 moves rather than 100 moves.</p>
    <p class="cue"><span class="time">[48:56]</span>Or if you can get $1 today, get $1 today</p>
    <p class="cue"><span class="time">[48:59]</span>rather than in 10 years.</p>
    <p class="cue"><span class="time">[49:00]</span>All of that is why we have a discount here.</p>
    <p class="cue"><span class="time">[49:03]</span>And the discount is a hyperparameter</p>
    <p class="cue"><span class="time">[49:05]</span>that you would define as well, that would influence</p>
    <p class="cue"><span class="time">[49:09]</span>the strategy of your agent.</p>
    <p class="cue"><span class="time">[49:10]</span>Can you show once more how the 4 pass the [INAUDIBLE]?</p>
    <p class="cue"><span class="time">[49:14]</span>We&#x27;re going to see that after.</p>
    <p class="cue"><span class="time">[49:15]</span>I&#x27;m going to do a concrete example, because it&#x27;s</p>
    <p class="cue"><span class="time">[49:17]</span>a little complicated.</p>
    <p class="cue"><span class="time">[49:18]</span>Yeah?</p>
    <p class="cue"><span class="time">[49:18]</span>It&#x27;s the law function, right?</p>
    <p class="cue"><span class="time">[49:20]</span>Qo s to the y?</p>
    <p class="cue"><span class="time">[49:23]</span>No.</p>
    <p class="cue"><span class="time">[49:24]</span>It&#x27;s a good point.</p>
    <p class="cue"><span class="time">[49:26]</span>It&#x27;s not that.</p>
    <p class="cue"><span class="time">[49:26]</span>It&#x27;s just of Q of--</p>
    <p class="cue"><span class="time">[49:28]</span>it&#x27;s a 2 by 2.</p>
    <p class="cue"><span class="time">[49:30]</span>It&#x27;s a 1 by 2.</p>
    <p class="cue"><span class="time">[49:31]</span>So you have left and right.</p>
    <p class="cue"><span class="time">[49:32]</span>I was just going down the first case.</p>
    <p class="cue"><span class="time">[49:34]</span>So I put the state left.</p>
    <p class="cue"><span class="time">[49:35]</span>Yeah?</p>
    <p class="cue"><span class="time">[49:36]</span>What if there&#x27;s law that&#x27;s not fixed [INAUDIBLE]?</p>
    <p class="cue"><span class="time">[49:44]</span>What if the rewards are not fixed?</p>
    <p class="cue"><span class="time">[49:46]</span>I mean, in most games we&#x27;re going to see right now,</p>
    <p class="cue"><span class="time">[49:48]</span>the rewards are going to be fixed</p>
    <p class="cue"><span class="time">[49:49]</span>by the designer of the game, the human that&#x27;s designing the game.</p>
    <p class="cue"><span class="time">[49:53]</span>In practice, you could have a separate function that actually</p>
    <p class="cue"><span class="time">[49:58]</span>comes up with the reward.</p>
    <p class="cue"><span class="time">[49:59]</span>We&#x27;re going to see an example later</p>
    <p class="cue"><span class="time">[50:00]</span>in the lecture, where the reward might</p>
    <p class="cue"><span class="time">[50:02]</span>be different in different scenarios.</p>
    <p class="cue"><span class="time">[50:04]</span>And there&#x27;s a function or a sometimes</p>
    <p class="cue"><span class="time">[50:06]</span>called a critic that determines what&#x27;s</p>
    <p class="cue"><span class="time">[50:09]</span>the reward in a certain state.</p>
    <p class="cue"><span class="time">[50:12]</span>OK, this is the-- one last question.</p>
    <p class="cue"><span class="time">[50:14]</span>And then we&#x27;ll move because we&#x27;re going</p>
    <p class="cue"><span class="time">[50:15]</span>to see a concrete example.</p>
    <p class="cue"><span class="time">[50:16]</span>It&#x27;s going to be clear.</p>
    <p class="cue"><span class="time">[50:18]</span>So when we like hold it fixed for backprop,</p>
    <p class="cue"><span class="time">[50:21]</span>is that what differentiates this from iterating through all</p>
    <p class="cue"><span class="time">[50:25]</span>of the possible [INAUDIBLE]?</p>
    <p class="cue"><span class="time">[50:28]</span>Yeah.</p>
    <p class="cue"><span class="time">[50:29]</span>So instead of doing the backtracking down the tree</p>
    <p class="cue"><span class="time">[50:32]</span>and going over everything, we&#x27;re saying--</p>
    <p class="cue"><span class="time">[50:35]</span>we&#x27;re going to limit ourselves to just picking the best</p>
    <p class="cue"><span class="time">[50:38]</span>action based on our current understanding of the network.</p>
    <p class="cue"><span class="time">[50:42]</span>You see, my network is kind of intelligent, not great.</p>
    <p class="cue"><span class="time">[50:45]</span>We&#x27;re in the middle of training.</p>
    <p class="cue"><span class="time">[50:47]</span>It says that I should go to the left.</p>
    <p class="cue"><span class="time">[50:49]</span>And then if I look at the next state when I&#x27;m in the left,</p>
    <p class="cue"><span class="time">[50:52]</span>it says, I should go to the right.</p>
    <p class="cue"><span class="time">[50:54]</span>I will trust it because it&#x27;s the best I have,</p>
    <p class="cue"><span class="time">[50:56]</span>best estimate I have, but I will discount that.</p>
    <p class="cue"><span class="time">[50:59]</span>And then if you keep repeating that,</p>
    <p class="cue"><span class="time">[51:00]</span>it turns out that not only your estimate gets better,</p>
    <p class="cue"><span class="time">[51:03]</span>but your model gets trained, and then ultimately both together</p>
    <p class="cue"><span class="time">[51:06]</span>get to an optimality equation.</p>
    <p class="cue"><span class="time">[51:10]</span>So it&#x27;s a funky concept, right?</p>
    <p class="cue"><span class="time">[51:13]</span>But you get it.</p>
    <p class="cue"><span class="time">[51:15]</span>We&#x27;re going to see examples.</p>
    <p class="cue"><span class="time">[51:18]</span>OK.</p>
    <p class="cue"><span class="time">[51:18]</span>So then once you have been able to use the Bellman</p>
    <p class="cue"><span class="time">[51:22]</span>equation to estimate your targets,</p>
    <p class="cue"><span class="time">[51:26]</span>you perform a classic back propagation.</p>
    <p class="cue"><span class="time">[51:29]</span>And you update the parameters of the network,</p>
    <p class="cue"><span class="time">[51:32]</span>and you repeat that process.</p>
    <p class="cue"><span class="time">[51:38]</span>Here is-- concretely, if you were to code it in pseudocode,</p>
    <p class="cue"><span class="time">[51:41]</span>here is what it would look like to train a neural agent using</p>
    <p class="cue"><span class="time">[51:46]</span>Q-learning.</p>
    <p class="cue"><span class="time">[51:47]</span>We start by initializing our Q-network parameters.</p>
    <p class="cue"><span class="time">[51:51]</span>So initialization.</p>
    <p class="cue"><span class="time">[51:53]</span>It&#x27;s random at first.</p>
    <p class="cue"><span class="time">[51:55]</span>Then we will loop over episodes.</p>
    <p class="cue"><span class="time">[51:57]</span>As a reminder, episodes are one full game from start</p>
    <p class="cue"><span class="time">[51:59]</span>to terminal state.</p>
    <p class="cue"><span class="time">[52:03]</span>Within an episode, we&#x27;re going to start from the initial state</p>
    <p class="cue"><span class="time">[52:06]</span>s and we&#x27;re going to loop over time steps</p>
    <p class="cue"><span class="time">[52:08]</span>until we reach a terminal state.</p>
    <p class="cue"><span class="time">[52:11]</span>So within one time step, here is what we will do.</p>
    <p class="cue"><span class="time">[52:15]</span>We&#x27;ll forward propagate the state s in the Q-network.</p>
    <p class="cue"><span class="time">[52:19]</span>We will execute the action a that has the maximum Q value.</p>
    <p class="cue"><span class="time">[52:25]</span>We will observe a reward and we will also</p>
    <p class="cue"><span class="time">[52:28]</span>observe a next state s prime.</p>
    <p class="cue"><span class="time">[52:31]</span>We will use that s prime to compute our target y by forward</p>
    <p class="cue"><span class="time">[52:35]</span>propagating s prime in the Q-network</p>
    <p class="cue"><span class="time">[52:38]</span>and then computing our loss function.</p>
    <p class="cue"><span class="time">[52:40]</span>And based on that, we will use gradient descent</p>
    <p class="cue"><span class="time">[52:43]</span>to update the parameters of the network.</p>
    <p class="cue"><span class="time">[52:48]</span>It should be simpler looked at like that, right?</p>
    <p class="cue"><span class="time">[52:52]</span>All right.</p>
    <p class="cue"><span class="time">[52:52]</span>So this is the vanilla Q-learning.</p>
    <p class="cue"><span class="time">[52:57]</span>So to summarize, again, the one--</p>
    <p class="cue"><span class="time">[52:59]</span>the main difference is that we don&#x27;t have a target.</p>
    <p class="cue"><span class="time">[53:01]</span>And we use our own network to estimate the targets.</p>
    <p class="cue"><span class="time">[53:04]</span>And the rewards are what&#x27;s going to help us get better over time.</p>
    <p class="cue"><span class="time">[53:12]</span>By the way, it&#x27;s OK if you don&#x27;t understand everything.</p>
    <p class="cue"><span class="time">[53:15]</span>This is an entire class at Stanford, an entire quarter</p>
    <p class="cue"><span class="time">[53:18]</span>of studying that type of stuff.</p>
    <p class="cue"><span class="time">[53:20]</span>So we&#x27;re trying to get the basics within an hour</p>
    <p class="cue"><span class="time">[53:22]</span>and a half, two hours.</p>
    <p class="cue"><span class="time">[53:26]</span>OK, let&#x27;s go a little further now together</p>
    <p class="cue"><span class="time">[53:32]</span>and apply that to an actual game.</p>
    <p class="cue"><span class="time">[53:34]</span>So here&#x27;s the game.</p>
    <p class="cue"><span class="time">[53:35]</span>It&#x27;s called Breakout.</p>
    <p class="cue"><span class="time">[53:36]</span>We want to destroy all the bricks.</p>
    <p class="cue"><span class="time">[53:38]</span>Who has played breakout in the past?</p>
    <p class="cue"><span class="time">[53:39]</span>Only a few.</p>
    <p class="cue"><span class="time">[53:41]</span>OK, good.</p>
    <p class="cue"><span class="time">[53:41]</span>So you have a paddle that you control and you are</p>
    <p class="cue"><span class="time">[53:45]</span>trying to destroy the bricks.</p>
    <p class="cue"><span class="time">[53:48]</span>If the ball gets past your paddle, you lost.</p>
    <p class="cue"><span class="time">[53:51]</span>And if the bricks are all destroyed, you won.</p>
    <p class="cue"><span class="time">[53:54]</span>That&#x27;s it.</p>
    <p class="cue"><span class="time">[53:57]</span>Let&#x27;s do it together.</p>
    <p class="cue"><span class="time">[54:00]</span>What is the input of our Q-network?</p>
    <p class="cue"><span class="time">[54:04]</span>What would you use as input?</p>
    <p class="cue"><span class="time">[54:07]</span>Remember-- yeah?</p>
    <p class="cue"><span class="time">[54:16]</span>The entire screen.</p>
    <p class="cue"><span class="time">[54:18]</span>Entire screen.</p>
    <p class="cue"><span class="time">[54:18]</span>OK, let&#x27;s do that.</p>
    <p class="cue"><span class="time">[54:20]</span>So I take-- I define that as the state s,</p>
    <p class="cue"><span class="time">[54:22]</span>which is the input to my Q network.</p>
    <p class="cue"><span class="time">[54:25]</span>What&#x27;s the output of the Q-network?</p>
    <p class="cue"><span class="time">[54:29]</span>Yes?</p>
    <p class="cue"><span class="time">[54:30]</span>For the input, do we have to do that on screen?</p>
    <p class="cue"><span class="time">[54:33]</span>Good question.</p>
    <p class="cue"><span class="time">[54:33]</span>We get there.</p>
    <p class="cue"><span class="time">[54:34]</span>I&#x27;m going to ask you.</p>
    <p class="cue"><span class="time">[54:36]</span>But do we have to look at the full screen?</p>
    <p class="cue"><span class="time">[54:39]</span>The answer is no, but we&#x27;ll see why.</p>
    <p class="cue"><span class="time">[54:42]</span>What&#x27;s the output?</p>
    <p class="cue"><span class="time">[54:43]</span>Yeah?</p>
    <p class="cue"><span class="time">[54:44]</span>The game score.</p>
    <p class="cue"><span class="time">[54:45]</span>The game score.</p>
    <p class="cue"><span class="time">[54:47]</span>No, but we&#x27;re going to talk about the game</p>
    <p class="cue"><span class="time">[54:49]</span>score in the back.</p>
    <p class="cue"><span class="time">[54:50]</span>Like the cool fix gradient polymer.</p>
    <p class="cue"><span class="time">[54:54]</span>Yeah, let&#x27;s talk about the output first,</p>
    <p class="cue"><span class="time">[54:55]</span>and then we&#x27;ll talk about the stuff we</p>
    <p class="cue"><span class="time">[54:57]</span>can get rid of on the inputs.</p>
    <p class="cue"><span class="time">[54:59]</span>But what&#x27;s the output?</p>
    <p class="cue"><span class="time">[55:00]</span>Yeah?</p>
    <p class="cue"><span class="time">[55:01]</span>It&#x27;s the movement of the fix points.</p>
    <p class="cue"><span class="time">[55:04]</span>The actions?</p>
    <p class="cue"><span class="time">[55:04]</span>Yes.</p>
    <p class="cue"><span class="time">[55:05]</span>Yeah, the actions.</p>
    <p class="cue"><span class="time">[55:07]</span>So yeah, it will be the Q values associated</p>
    <p class="cue"><span class="time">[55:11]</span>with the actions in state s.</p>
    <p class="cue"><span class="time">[55:12]</span>Remember, it&#x27;s a Q function.</p>
    <p class="cue"><span class="time">[55:14]</span>So the output is-- we need one value for left,</p>
    <p class="cue"><span class="time">[55:17]</span>one value for right and one value for idle.</p>
    <p class="cue"><span class="time">[55:20]</span>You could make this game more complicated and say,</p>
    <p class="cue"><span class="time">[55:23]</span>we have eight actions.</p>
    <p class="cue"><span class="time">[55:24]</span>We have a little bit to the left, a lot to the left,</p>
    <p class="cue"><span class="time">[55:27]</span>a lot more to the left, if you had multiple buttons.</p>
    <p class="cue"><span class="time">[55:30]</span>But let&#x27;s simplify and say three actions.</p>
    <p class="cue"><span class="time">[55:31]</span>Either you don&#x27;t move, you move to the left,</p>
    <p class="cue"><span class="time">[55:33]</span>or you move to the right.</p>
    <p class="cue"><span class="time">[55:34]</span>So these are the outputs.</p>
    <p class="cue"><span class="time">[55:36]</span>So now, let&#x27;s get to the question of the screen.</p>
    <p class="cue"><span class="time">[55:38]</span>Do we need the entire screen?</p>
    <p class="cue"><span class="time">[55:42]</span>So you were saying something earlier.</p>
    <p class="cue"><span class="time">[55:45]</span>Right.</p>
    <p class="cue"><span class="time">[55:45]</span>So you will get [INAUDIBLE].</p>
    <p class="cue"><span class="time">[55:51]</span>OK.</p>
    <p class="cue"><span class="time">[55:52]</span>So you say you need the tray and the bricks.</p>
    <p class="cue"><span class="time">[55:54]</span>I would argue you need more because there&#x27;s the walls.</p>
    <p class="cue"><span class="time">[55:57]</span>And I guess that you could-- if you&#x27;re an expert player,</p>
    <p class="cue"><span class="time">[56:00]</span>you could where the walls are.</p>
    <p class="cue"><span class="time">[56:01]</span>But generally, you need a little more than that.</p>
    <p class="cue"><span class="time">[56:03]</span>What would be obviously things we can get rid of</p>
    <p class="cue"><span class="time">[56:06]</span>and why would we do that?</p>
    <p class="cue"><span class="time">[56:09]</span>The background, probably the walkthrough at the top.</p>
    <p class="cue"><span class="time">[56:13]</span>OK, the score at the top.</p>
    <p class="cue"><span class="time">[56:18]</span>Who would remove the score at the top?</p>
    <p class="cue"><span class="time">[56:23]</span>But half.</p>
    <p class="cue"><span class="time">[56:26]</span>Why would you not remove it?</p>
    <p class="cue"><span class="time">[56:32]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[56:33]</span>Why would you remove it?</p>
    <p class="cue"><span class="time">[56:35]</span>Because the point [INAUDIBLE].</p>
    <p class="cue"><span class="time">[56:39]</span>OK.</p>
    <p class="cue"><span class="time">[56:39]</span>You want to always win.</p>
    <p class="cue"><span class="time">[56:41]</span>So the score doesn&#x27;t matter.</p>
    <p class="cue"><span class="time">[56:43]</span>It&#x27;s true.</p>
    <p class="cue"><span class="time">[56:44]</span>We would remove the score.</p>
    <p class="cue"><span class="time">[56:45]</span>So you could actually crop the top.</p>
    <p class="cue"><span class="time">[56:47]</span>You could also crop the bottom.</p>
    <p class="cue"><span class="time">[56:49]</span>I mean, if it passed the paddle, you</p>
    <p class="cue"><span class="time">[56:50]</span>don&#x27;t care about the few pixels at the bottom.</p>
    <p class="cue"><span class="time">[56:52]</span>You could get rid of them.</p>
    <p class="cue"><span class="time">[56:54]</span>It is not always true.</p>
    <p class="cue"><span class="time">[56:56]</span>There are games where the score matters.</p>
    <p class="cue"><span class="time">[56:59]</span>And in fact, I like football, soccer.</p>
    <p class="cue"><span class="time">[57:04]</span>In soccer, if you&#x27;re 1-0 up, you can park the bus.</p>
    <p class="cue"><span class="time">[57:08]</span>So your strategy is dependent of the score that you have.</p>
    <p class="cue"><span class="time">[57:12]</span>You wouldn&#x27;t park the bus if you&#x27;re losing 1-0.</p>
    <p class="cue"><span class="time">[57:14]</span>Parking the bus, meaning you ask every player</p>
    <p class="cue"><span class="time">[57:16]</span>to come back and defend.</p>
    <p class="cue"><span class="time">[57:18]</span>If you&#x27;re losing, you would actually do the opposite.</p>
    <p class="cue"><span class="time">[57:20]</span>You would go all out attack.</p>
    <p class="cue"><span class="time">[57:22]</span>So in certain games, you want the scores.</p>
    <p class="cue"><span class="time">[57:24]</span>In others, you don&#x27;t want.</p>
    <p class="cue"><span class="time">[57:26]</span>And so it&#x27;s part of the designer, the AI engineer that&#x27;s</p>
    <p class="cue"><span class="time">[57:29]</span>working on that to determine what information we need</p>
    <p class="cue"><span class="time">[57:31]</span>and what we don&#x27;t need.</p>
    <p class="cue"><span class="time">[57:32]</span>What else could we do to reduce the dimensionality</p>
    <p class="cue"><span class="time">[57:34]</span>of the problem and make our computation faster?</p>
    <p class="cue"><span class="time">[57:36]</span>Probably just remove the channels.</p>
    <p class="cue"><span class="time">[57:39]</span>Yeah, remove the RGB channels.</p>
    <p class="cue"><span class="time">[57:41]</span>So we do grayscale, essentially.</p>
    <p class="cue"><span class="time">[57:43]</span>That&#x27;s true.</p>
    <p class="cue"><span class="time">[57:44]</span>Here, you actually don&#x27;t need the colors.</p>
    <p class="cue"><span class="time">[57:46]</span>It&#x27;s just nice as a user for user experience purposes.</p>
    <p class="cue"><span class="time">[57:49]</span>You don&#x27;t need-- I don&#x27;t think there&#x27;s different points based</p>
    <p class="cue"><span class="time">[57:52]</span>on the bricks that you destroy.</p>
    <p class="cue"><span class="time">[57:54]</span>It&#x27;s all the same.</p>
    <p class="cue"><span class="time">[57:56]</span>Actually, funny enough, this algorithm</p>
    <p class="cue"><span class="time">[57:59]</span>was used by DeepMind to play a lot of Atari games.</p>
    <p class="cue"><span class="time">[58:04]</span>And they did a single pre-processing</p>
    <p class="cue"><span class="time">[58:05]</span>where they removed the channels because they</p>
    <p class="cue"><span class="time">[58:07]</span>said it doesn&#x27;t matter.</p>
    <p class="cue"><span class="time">[58:09]</span>Turns out in one of the games, I think it was SeaQuest,</p>
    <p class="cue"><span class="time">[58:12]</span>I forgot which one, the fish disappeared when you do that.</p>
    <p class="cue"><span class="time">[58:16]</span>And so that game didn&#x27;t work.</p>
    <p class="cue"><span class="time">[58:18]</span>The agent couldn&#x27;t crack it.</p>
    <p class="cue"><span class="time">[58:20]</span>Because they thought that the same pre-processing could</p>
    <p class="cue"><span class="time">[58:22]</span>apply to every game, but actually,</p>
    <p class="cue"><span class="time">[58:24]</span>they had to make a slight tweak.</p>
    <p class="cue"><span class="time">[58:26]</span>Question.</p>
    <p class="cue"><span class="time">[58:27]</span>If you remove RGB to reduce the [INAUDIBLE],</p>
    <p class="cue"><span class="time">[58:36]</span>and then you also didn&#x27;t want to [INAUDIBLE].</p>
    <p class="cue"><span class="time">[58:39]</span>Correct.</p>
    <p class="cue"><span class="time">[58:39]</span>All right.</p>
    <p class="cue"><span class="time">[58:41]</span>So just to recap, you could do it</p>
    <p class="cue"><span class="time">[58:42]</span>even better by using a low-dimensional representation</p>
    <p class="cue"><span class="time">[58:46]</span>of this game that describes the game.</p>
    <p class="cue"><span class="time">[58:48]</span>It&#x27;s true.</p>
    <p class="cue"><span class="time">[58:49]</span>But because we want to use a single algorithm for 50</p>
    <p class="cue"><span class="time">[58:52]</span>plus Atari games, we&#x27;ll say the human sees the screen,</p>
    <p class="cue"><span class="time">[58:55]</span>we&#x27;ll just give the screen, and it will probably</p>
    <p class="cue"><span class="time">[58:57]</span>scale better, essentially.</p>
    <p class="cue"><span class="time">[58:58]</span>But you&#x27;re perfectly right, if you</p>
    <p class="cue"><span class="time">[58:59]</span>were working on only that game.</p>
    <p class="cue"><span class="time">[59:01]</span>OK.</p>
    <p class="cue"><span class="time">[59:02]</span>So let&#x27;s do that.</p>
    <p class="cue"><span class="time">[59:03]</span>We&#x27;ll do pre-processing.</p>
    <p class="cue"><span class="time">[59:04]</span>There&#x27;s one last thing that nobody mentioned,</p>
    <p class="cue"><span class="time">[59:06]</span>which is history.</p>
    <p class="cue"><span class="time">[59:07]</span>Because in fact, if you get only one screen,</p>
    <p class="cue"><span class="time">[59:10]</span>you don&#x27;t know where the ball is going.</p>
    <p class="cue"><span class="time">[59:12]</span>So actually, you can solve the game.</p>
    <p class="cue"><span class="time">[59:14]</span>And the way you fix that is by giving</p>
    <p class="cue"><span class="time">[59:16]</span>a history of multiple screens.</p>
    <p class="cue"><span class="time">[59:18]</span>For example, four screens, so that you see the direction</p>
    <p class="cue"><span class="time">[59:20]</span>that the ball is going in.</p>
    <p class="cue"><span class="time">[59:22]</span>So our preprocessing function is called phi of s, let&#x27;s say.</p>
    <p class="cue"><span class="time">[59:27]</span>And phi of s is a mix of--</p>
    <p class="cue"><span class="time">[59:30]</span>you might do-- convert to grayscale, reduce</p>
    <p class="cue"><span class="time">[59:33]</span>the dimension, the height and width</p>
    <p class="cue"><span class="time">[59:34]</span>and also add the history of four frames.</p>
    <p class="cue"><span class="time">[59:37]</span>And that should be enough.</p>
    <p class="cue"><span class="time">[59:39]</span>It turns out in most games you will</p>
    <p class="cue"><span class="time">[59:41]</span>need the history, a little bit of history</p>
    <p class="cue"><span class="time">[59:43]</span>to where the ball is going.</p>
    <p class="cue"><span class="time">[59:45]</span>In this example, the opposite would be the velocity vector</p>
    <p class="cue"><span class="time">[59:49]</span>of the ball.</p>
    <p class="cue"><span class="time">[59:50]</span>Yeah, you could replace--</p>
    <p class="cue"><span class="time">[59:52]</span>exactly.</p>
    <p class="cue"><span class="time">[59:52]</span>You could replace the history, so multiple screen,</p>
    <p class="cue"><span class="time">[59:56]</span>by just adding the gradients or the velocity of where</p>
    <p class="cue"><span class="time">[59:59]</span>the ball is going.</p>
    <p class="cue"><span class="time">[60:00]</span>That&#x27;s true.</p>
    <p class="cue"><span class="time">[60:00]</span>But would it scale to every game?</p>
    <p class="cue"><span class="time">[60:02]</span>It turns out this-- because we know humans look at the Atari</p>
    <p class="cue"><span class="time">[60:05]</span>machine, and they look at pixels,</p>
    <p class="cue"><span class="time">[60:07]</span>this would be more likely to scale to every game.</p>
    <p class="cue"><span class="time">[60:13]</span>Think about a game where--</p>
    <p class="cue"><span class="time">[60:15]</span>actually SeaQuest is a good game or Space Invaders,</p>
    <p class="cue"><span class="time">[60:17]</span>where you have multiple enemies coming at you.</p>
    <p class="cue"><span class="time">[60:20]</span>Then you would need to change your pre-processing</p>
    <p class="cue"><span class="time">[60:23]</span>to take into account the velocity of all these enemies,</p>
    <p class="cue"><span class="time">[60:25]</span>so it wouldn&#x27;t work the same way.</p>
    <p class="cue"><span class="time">[60:27]</span>While if you actually give the pixels,</p>
    <p class="cue"><span class="time">[60:29]</span>you actually, from the pixels, get</p>
    <p class="cue"><span class="time">[60:30]</span>the velocity of all your enemies and the direction</p>
    <p class="cue"><span class="time">[60:32]</span>they&#x27;re going in.</p>
    <p class="cue"><span class="time">[60:35]</span>So this is our pre-processing.</p>
    <p class="cue"><span class="time">[60:36]</span>I&#x27;m going to refer to it as phi of s.</p>
    <p class="cue"><span class="time">[60:38]</span>And our deep Q-network architecture,</p>
    <p class="cue"><span class="time">[60:41]</span>because we&#x27;re working with pixels,</p>
    <p class="cue"><span class="time">[60:43]</span>is going to be a convolutional neural network.</p>
    <p class="cue"><span class="time">[60:46]</span>Don&#x27;t worry if you haven&#x27;t learned it yet in the class,</p>
    <p class="cue"><span class="time">[60:48]</span>but it&#x27;s a bunch of conv and ReLU activations.</p>
    <p class="cue"><span class="time">[60:52]</span>And then we end with a fully connected layer</p>
    <p class="cue"><span class="time">[60:57]</span>that gives us the three Q values for the different actions.</p>
    <p class="cue"><span class="time">[61:03]</span>So nothing special here.</p>
    <p class="cue"><span class="time">[61:04]</span>Now, we&#x27;re going to go back to our vanilla training.</p>
    <p class="cue"><span class="time">[61:08]</span>So this one that we saw together earlier.</p>
    <p class="cue"><span class="time">[61:10]</span>And we&#x27;re going to look at tips to train reinforcement learning</p>
    <p class="cue"><span class="time">[61:13]</span>algorithms.</p>
    <p class="cue"><span class="time">[61:14]</span>Those tips are not specific to Q-learning.</p>
    <p class="cue"><span class="time">[61:16]</span>Some of them are applied to a lot more than Q-learning.</p>
    <p class="cue"><span class="time">[61:20]</span>And they&#x27;re very important to know.</p>
    <p class="cue"><span class="time">[61:22]</span>And they&#x27;re part of the reason reinforcement learning</p>
    <p class="cue"><span class="time">[61:24]</span>has worked better in the last few years.</p>
    <p class="cue"><span class="time">[61:28]</span>So one of the things that&#x27;s pretty simple</p>
    <p class="cue"><span class="time">[61:30]</span>that we forgot to do is the pre-processing that we just did.</p>
    <p class="cue"><span class="time">[61:34]</span>So anywhere I had an s, I&#x27;m going</p>
    <p class="cue"><span class="time">[61:36]</span>to instead run s through the pre-processing step.</p>
    <p class="cue"><span class="time">[61:39]</span>I&#x27;m going to use phi of s.</p>
    <p class="cue"><span class="time">[61:41]</span>So I initialize instead of s with phi of s.</p>
    <p class="cue"><span class="time">[61:45]</span>I start from the initial state phi of s.</p>
    <p class="cue"><span class="time">[61:47]</span>And then I forward propagate phi of s.</p>
    <p class="cue"><span class="time">[61:51]</span>I get the Q of that pre-processed state in action a</p>
    <p class="cue"><span class="time">[61:56]</span>and et cetera, et cetera.</p>
    <p class="cue"><span class="time">[61:57]</span>And then when I get my next state--</p>
    <p class="cue"><span class="time">[62:00]</span>so let&#x27;s say I look at my current pre-processed state.</p>
    <p class="cue"><span class="time">[62:04]</span>I forward propagated once.</p>
    <p class="cue"><span class="time">[62:07]</span>I see the three Q values, in our case, the two Q values.</p>
    <p class="cue"><span class="time">[62:12]</span>One of them was better than the other action, right?</p>
    <p class="cue"><span class="time">[62:15]</span>Then I get my next state S prime.</p>
    <p class="cue"><span class="time">[62:17]</span>I want to pre-process that state as well.</p>
    <p class="cue"><span class="time">[62:20]</span>OK.</p>
    <p class="cue"><span class="time">[62:21]</span>So that&#x27;s pretty straightforward.</p>
    <p class="cue"><span class="time">[62:23]</span>You just replace all of that.</p>
    <p class="cue"><span class="time">[62:26]</span>The second thing we forgot to do is</p>
    <p class="cue"><span class="time">[62:27]</span>to keep track of the terminal states.</p>
    <p class="cue"><span class="time">[62:29]</span>In our pseudocode, there is no concept of terminal state.</p>
    <p class="cue"><span class="time">[62:32]</span>It&#x27;s pretty easy to add.</p>
    <p class="cue"><span class="time">[62:33]</span>You would probably just do an if else statement.</p>
    <p class="cue"><span class="time">[62:36]</span>You would create a Boolean to detect terminal state.</p>
    <p class="cue"><span class="time">[62:39]</span>So let&#x27;s say your Boolean is terminal equals false.</p>
    <p class="cue"><span class="time">[62:42]</span>And then as you loop over the time step of a single episode,</p>
    <p class="cue"><span class="time">[62:45]</span>every time you&#x27;re going to check,</p>
    <p class="cue"><span class="time">[62:47]</span>is the state that I&#x27;m going in based on the action I&#x27;m</p>
    <p class="cue"><span class="time">[62:50]</span>taking a terminal state?</p>
    <p class="cue"><span class="time">[62:52]</span>If it&#x27;s a terminal state, then get out of the loop.</p>
    <p class="cue"><span class="time">[62:55]</span>There&#x27;s nothing else after.</p>
    <p class="cue"><span class="time">[62:57]</span>The one thing that you need to be careful of</p>
    <p class="cue"><span class="time">[62:59]</span>is if it&#x27;s a terminal state, then</p>
    <p class="cue"><span class="time">[63:01]</span>your target is not the Bellman equation,</p>
    <p class="cue"><span class="time">[63:04]</span>it&#x27;s just the immediate reward.</p>
    <p class="cue"><span class="time">[63:06]</span>Remember, you get to the terminal state.</p>
    <p class="cue"><span class="time">[63:07]</span>You get a reward of 10.</p>
    <p class="cue"><span class="time">[63:09]</span>There is no Bellman equation to apply it.</p>
    <p class="cue"><span class="time">[63:11]</span>It&#x27;s just 10.</p>
    <p class="cue"><span class="time">[63:12]</span>It&#x27;s immediate reward.</p>
    <p class="cue"><span class="time">[63:13]</span>There&#x27;s no discount, et cetera.</p>
    <p class="cue"><span class="time">[63:20]</span>OK.</p>
    <p class="cue"><span class="time">[63:20]</span>So these are fairly easy changes.</p>
    <p class="cue"><span class="time">[63:23]</span>Now, we&#x27;re going to look at a new method that will</p>
    <p class="cue"><span class="time">[63:26]</span>enable more data efficiency.</p>
    <p class="cue"><span class="time">[63:29]</span>It&#x27;s called experience replay.</p>
    <p class="cue"><span class="time">[63:31]</span>One of the-- a couple of issues with the way we&#x27;ve</p>
    <p class="cue"><span class="time">[63:34]</span>been training so far is one, the correlation</p>
    <p class="cue"><span class="time">[63:39]</span>of successive screens.</p>
    <p class="cue"><span class="time">[63:42]</span>So imagine in the Atari game, you</p>
    <p class="cue"><span class="time">[63:44]</span>have the ball that&#x27;s in the top left corner</p>
    <p class="cue"><span class="time">[63:46]</span>and it&#x27;s traveling to the bottom right of the screen.</p>
    <p class="cue"><span class="time">[63:50]</span>You have many, many time steps that are essentially the same.</p>
    <p class="cue"><span class="time">[63:53]</span>It&#x27;s all the ball traveling in the same place.</p>
    <p class="cue"><span class="time">[63:57]</span>So you&#x27;re actually training repetitively</p>
    <p class="cue"><span class="time">[64:01]</span>on something that is not that meaningful.</p>
    <p class="cue"><span class="time">[64:03]</span>You don&#x27;t need to just train on a batch.</p>
    <p class="cue"><span class="time">[64:05]</span>The equivalent in supervised learning</p>
    <p class="cue"><span class="time">[64:06]</span>is let&#x27;s say you&#x27;re trying to differentiate cats and dogs,</p>
    <p class="cue"><span class="time">[64:09]</span>and you train on a mini batch of cats.</p>
    <p class="cue"><span class="time">[64:11]</span>Then you train on a mini batch of dogs.</p>
    <p class="cue"><span class="time">[64:13]</span>Then you train on a mini batch of cats.</p>
    <p class="cue"><span class="time">[64:15]</span>Then you will never converge.</p>
    <p class="cue"><span class="time">[64:16]</span>It will just index too much on cats and then index</p>
    <p class="cue"><span class="time">[64:19]</span>too much on dogs.</p>
    <p class="cue"><span class="time">[64:21]</span>So you want to add some experience replay concept</p>
    <p class="cue"><span class="time">[64:25]</span>that we&#x27;ll see in order to create more mixes in the data</p>
    <p class="cue"><span class="time">[64:29]</span>and get more diversity.</p>
    <p class="cue"><span class="time">[64:31]</span>The other thing that is important</p>
    <p class="cue"><span class="time">[64:33]</span>is in our current training process,</p>
    <p class="cue"><span class="time">[64:37]</span>we are not reusing our data.</p>
    <p class="cue"><span class="time">[64:39]</span>Like you experience something.</p>
    <p class="cue"><span class="time">[64:40]</span>You immediately train on it.</p>
    <p class="cue"><span class="time">[64:42]</span>You never see it again, unless you re-experience the same thing</p>
    <p class="cue"><span class="time">[64:45]</span>sometimes in the future, which might or might not happen.</p>
    <p class="cue"><span class="time">[64:48]</span>Experience replay is going to help</p>
    <p class="cue"><span class="time">[64:50]</span>us to keep experiences in memory,</p>
    <p class="cue"><span class="time">[64:53]</span>and maybe retrain on them on a regular basis,</p>
    <p class="cue"><span class="time">[64:56]</span>so that one experience might be useful multiple times, which</p>
    <p class="cue"><span class="time">[65:00]</span>intuitively makes sense.</p>
    <p class="cue"><span class="time">[65:01]</span>Maybe you do an experience, you get an amazing reward,</p>
    <p class="cue"><span class="time">[65:03]</span>and you don&#x27;t want to forget it.</p>
    <p class="cue"><span class="time">[65:05]</span>You want to retrain the model on a regular basis.</p>
    <p class="cue"><span class="time">[65:08]</span>It&#x27;s more data efficiency.</p>
    <p class="cue"><span class="time">[65:10]</span>So here&#x27;s what it looks like.</p>
    <p class="cue"><span class="time">[65:12]</span>The current way we were training was we&#x27;re in a state--</p>
    <p class="cue"><span class="time">[65:15]</span>I&#x27;m just going to state instead of pre-process state,</p>
    <p class="cue"><span class="time">[65:17]</span>but it&#x27;s pre-processed.</p>
    <p class="cue"><span class="time">[65:18]</span>We&#x27;re in a state s.</p>
    <p class="cue"><span class="time">[65:20]</span>We perform action a.</p>
    <p class="cue"><span class="time">[65:21]</span>We get a reward r and we get into the next state.</p>
    <p class="cue"><span class="time">[65:25]</span>From that next state, we perform another action a prime.</p>
    <p class="cue"><span class="time">[65:28]</span>We get a reward r prime and we get into a second,</p>
    <p class="cue"><span class="time">[65:32]</span>and so on and so on.</p>
    <p class="cue"><span class="time">[65:34]</span>And each of these would be called one experience.</p>
    <p class="cue"><span class="time">[65:37]</span>It&#x27;s one iteration of gradient descent.</p>
    <p class="cue"><span class="time">[65:39]</span>It&#x27;s one experience.</p>
    <p class="cue"><span class="time">[65:41]</span>So right now we&#x27;re training on these experiences.</p>
    <p class="cue"><span class="time">[65:44]</span>So the training looks like I train on E1.</p>
    <p class="cue"><span class="time">[65:47]</span>I update my parameters.</p>
    <p class="cue"><span class="time">[65:49]</span>Then I train on E2.</p>
    <p class="cue"><span class="time">[65:50]</span>I update my parameters.</p>
    <p class="cue"><span class="time">[65:51]</span>Then I train on E3.</p>
    <p class="cue"><span class="time">[65:52]</span>I update my parameters.</p>
    <p class="cue"><span class="time">[65:54]</span>Those are highly correlated because they&#x27;re</p>
    <p class="cue"><span class="time">[65:56]</span>part of the same episode.</p>
    <p class="cue"><span class="time">[65:58]</span>And as I was saying, with the ball traveling in one direction,</p>
    <p class="cue"><span class="time">[66:01]</span>that might actually not be that helpful to train</p>
    <p class="cue"><span class="time">[66:03]</span>on all of these.</p>
    <p class="cue"><span class="time">[66:05]</span>So instead, what we&#x27;ll do is we&#x27;ll use experience replay</p>
    <p class="cue"><span class="time">[66:08]</span>where we will collect our first experience,</p>
    <p class="cue"><span class="time">[66:11]</span>but instead of training on it, we</p>
    <p class="cue"><span class="time">[66:13]</span>will put it in a memory called the replay memory</p>
    <p class="cue"><span class="time">[66:16]</span>D. We&#x27;ll put it in there.</p>
    <p class="cue"><span class="time">[66:19]</span>And then at every step, we will sample from that memory</p>
    <p class="cue"><span class="time">[66:22]</span>to decide what to train on.</p>
    <p class="cue"><span class="time">[66:24]</span>So of course, at the beginning, if we just</p>
    <p class="cue"><span class="time">[66:27]</span>have one experience in the memory,</p>
    <p class="cue"><span class="time">[66:30]</span>we will train on that experience.</p>
    <p class="cue"><span class="time">[66:31]</span>But over time, you will see that we get more diversity</p>
    <p class="cue"><span class="time">[66:35]</span>and reuse out of our experiences.</p>
    <p class="cue"><span class="time">[66:38]</span>So for example, let&#x27;s say I experience E2.</p>
    <p class="cue"><span class="time">[66:41]</span>I put it into memory.</p>
    <p class="cue"><span class="time">[66:42]</span>And then instead of training on E2,</p>
    <p class="cue"><span class="time">[66:44]</span>I&#x27;m going to randomly sample from the memory.</p>
    <p class="cue"><span class="time">[66:46]</span>I might get E1 or I might get E2.</p>
    <p class="cue"><span class="time">[66:49]</span>Then I experience E3, and I put it in the memory,</p>
    <p class="cue"><span class="time">[66:52]</span>and I might get one of the three.</p>
    <p class="cue"><span class="time">[66:56]</span>This is the vanilla experience replay.</p>
    <p class="cue"><span class="time">[66:58]</span>In practice, there is more methods</p>
    <p class="cue"><span class="time">[67:00]</span>like prioritized sweeping, which might tell you</p>
    <p class="cue"><span class="time">[67:03]</span>which experience you want to weigh.</p>
    <p class="cue"><span class="time">[67:04]</span>Maybe some experiences had a higher gradient.</p>
    <p class="cue"><span class="time">[67:07]</span>So you want to prioritize them more often.</p>
    <p class="cue"><span class="time">[67:09]</span>Things like that.</p>
    <p class="cue"><span class="time">[67:12]</span>So all in all, this is what our training looks</p>
    <p class="cue"><span class="time">[67:14]</span>like with experience replay.</p>
    <p class="cue"><span class="time">[67:16]</span>We experience E1.</p>
    <p class="cue"><span class="time">[67:19]</span>We train on E1.</p>
    <p class="cue"><span class="time">[67:20]</span>Then the next training iteration is not on E2,</p>
    <p class="cue"><span class="time">[67:24]</span>it&#x27;s on a sample from E1 and E2, either or.</p>
    <p class="cue"><span class="time">[67:27]</span>The third experience is then put in the replay memory,</p>
    <p class="cue"><span class="time">[67:30]</span>but we don&#x27;t train on it.</p>
    <p class="cue"><span class="time">[67:31]</span>We train on a sample from whatever is in the replay</p>
    <p class="cue"><span class="time">[67:33]</span>memory, and we repeat.</p>
    <p class="cue"><span class="time">[67:35]</span>And that is more sample--</p>
    <p class="cue"><span class="time">[67:37]</span>more efficient.</p>
    <p class="cue"><span class="time">[67:38]</span>It allows more reusability and less cross correlation</p>
    <p class="cue"><span class="time">[67:41]</span>in our training batch.</p>
    <p class="cue"><span class="time">[67:47]</span>OK.</p>
    <p class="cue"><span class="time">[67:49]</span>So that&#x27;s called replay memory.</p>
    <p class="cue"><span class="time">[67:52]</span>And you can use it with mini batch gradient descent.</p>
    <p class="cue"><span class="time">[67:54]</span>Note that you still experience in the direction</p>
    <p class="cue"><span class="time">[67:58]</span>that the game is played.</p>
    <p class="cue"><span class="time">[67:59]</span>We still go and take the action as expected.</p>
    <p class="cue"><span class="time">[68:03]</span>We just don&#x27;t necessarily update our model parameter</p>
    <p class="cue"><span class="time">[68:05]</span>based on the action that we ended up taking.</p>
    <p class="cue"><span class="time">[68:08]</span>We put it in the replay memory.</p>
    <p class="cue"><span class="time">[68:09]</span>We may train on it later.</p>
    <p class="cue"><span class="time">[68:13]</span>OK.</p>
    <p class="cue"><span class="time">[68:15]</span>So here is how it modifies our vanilla setup.</p>
    <p class="cue"><span class="time">[68:20]</span>We&#x27;ve added an experience from state</p>
    <p class="cue"><span class="time">[68:23]</span>s to state s prime to the replay memory.</p>
    <p class="cue"><span class="time">[68:27]</span>Let me walk you through it again.</p>
    <p class="cue"><span class="time">[68:29]</span>Within one time step, we forward propagate our state</p>
    <p class="cue"><span class="time">[68:33]</span>into the Q-network.</p>
    <p class="cue"><span class="time">[68:33]</span>We execute the best action given the Q-values.</p>
    <p class="cue"><span class="time">[68:37]</span>This gives us a reward and next state.</p>
    <p class="cue"><span class="time">[68:40]</span>The next state is pre-processed.</p>
    <p class="cue"><span class="time">[68:42]</span>And then instead of training on that,</p>
    <p class="cue"><span class="time">[68:44]</span>instead of training, we just add that transition</p>
    <p class="cue"><span class="time">[68:48]</span>to the replay memory.</p>
    <p class="cue"><span class="time">[68:49]</span>And instead, we sample randomly a mini batch of transition</p>
    <p class="cue"><span class="time">[68:53]</span>from the replay memory.</p>
    <p class="cue"><span class="time">[68:54]</span>And we train on those.</p>
    <p class="cue"><span class="time">[68:57]</span>And we redo the same thing again and again.</p>
    <p class="cue"><span class="time">[69:00]</span>Yes?</p>
    <p class="cue"><span class="time">[69:01]</span>As the replay memory play biased towards the start</p>
    <p class="cue"><span class="time">[69:05]</span>of the game for everything, are you</p>
    <p class="cue"><span class="time">[69:09]</span>more likely to replay start game?</p>
    <p class="cue"><span class="time">[69:11]</span>If you&#x27;re learning something, you would sometimes</p>
    <p class="cue"><span class="time">[69:14]</span>want [INAUDIBLE].</p>
    <p class="cue"><span class="time">[69:16]</span>Yeah.</p>
    <p class="cue"><span class="time">[69:19]</span>You would within one episode.</p>
    <p class="cue"><span class="time">[69:20]</span>But if you play multiple chess game,</p>
    <p class="cue"><span class="time">[69:25]</span>your replay memory will get already bigger.</p>
    <p class="cue"><span class="time">[69:27]</span>So then you would see some end game, some middle</p>
    <p class="cue"><span class="time">[69:30]</span>of the game, some early games.</p>
    <p class="cue"><span class="time">[69:35]</span>And in practice, it&#x27;s actually useful</p>
    <p class="cue"><span class="time">[69:36]</span>because you might imagine that in a chess game, all of us--</p>
    <p class="cue"><span class="time">[69:42]</span>let&#x27;s say, if you&#x27;re a beginner, you</p>
    <p class="cue"><span class="time">[69:45]</span>see a lot beginning of the games.</p>
    <p class="cue"><span class="time">[69:46]</span>You actually-- people that are beginners,</p>
    <p class="cue"><span class="time">[69:48]</span>they&#x27;re good at openings, but they&#x27;re bad at endgames</p>
    <p class="cue"><span class="time">[69:50]</span>because they don&#x27;t get to play a lot of endgames.</p>
    <p class="cue"><span class="time">[69:53]</span>Well, that type of approach could be useful.</p>
    <p class="cue"><span class="time">[69:56]</span>You can retrain on endgames more often.</p>
    <p class="cue"><span class="time">[69:58]</span>And a more advanced version of the replay memory</p>
    <p class="cue"><span class="time">[70:02]</span>would also weigh the experience in the replay memory</p>
    <p class="cue"><span class="time">[70:07]</span>based on how much the gradient is going to be.</p>
    <p class="cue"><span class="time">[70:09]</span>So if you have an experience that actually</p>
    <p class="cue"><span class="time">[70:11]</span>was super insightful, you can wait</p>
    <p class="cue"><span class="time">[70:13]</span>higher so that you prioritize grabbing it and retraining</p>
    <p class="cue"><span class="time">[70:17]</span>on it, essentially.</p>
    <p class="cue"><span class="time">[70:20]</span>So let&#x27;s say you blunder in chess.</p>
    <p class="cue"><span class="time">[70:22]</span>You might actually want to resee that blunder later so that you</p>
    <p class="cue"><span class="time">[70:24]</span>don&#x27;t do it again.</p>
    <p class="cue"><span class="time">[70:28]</span>OK, so these were all the different methods.</p>
    <p class="cue"><span class="time">[70:31]</span>Another one that&#x27;s very intuitive and very important</p>
    <p class="cue"><span class="time">[70:34]</span>is when--</p>
    <p class="cue"><span class="time">[70:35]</span>during the training process, our agent gets stuck,</p>
    <p class="cue"><span class="time">[70:40]</span>gets stuck in a local minima.</p>
    <p class="cue"><span class="time">[70:43]</span>Here is how it would work in practice.</p>
    <p class="cue"><span class="time">[70:46]</span>You start in initial state S1, and you</p>
    <p class="cue"><span class="time">[70:48]</span>have three states ahead of you.</p>
    <p class="cue"><span class="time">[70:50]</span>If you take action a1, you go to state 2,</p>
    <p class="cue"><span class="time">[70:53]</span>which is a terminal state.</p>
    <p class="cue"><span class="time">[70:54]</span>If you take-- and you get a reward of 0.</p>
    <p class="cue"><span class="time">[70:57]</span>If you take action a2, you get to S3, also a terminal state,</p>
    <p class="cue"><span class="time">[71:01]</span>and you get a reward of 1.</p>
    <p class="cue"><span class="time">[71:02]</span>And if you get action a3, you get to state 4 terminal state</p>
    <p class="cue"><span class="time">[71:08]</span>with a reward of 1,000.</p>
    <p class="cue"><span class="time">[71:10]</span>So of course, to us, it&#x27;s obvious</p>
    <p class="cue"><span class="time">[71:12]</span>that we would want to explore the state number 4.</p>
    <p class="cue"><span class="time">[71:15]</span>It&#x27;s pretty obvious.</p>
    <p class="cue"><span class="time">[71:16]</span>In practice, let&#x27;s say you update--</p>
    <p class="cue"><span class="time">[71:18]</span>you initialize your network.</p>
    <p class="cue"><span class="time">[71:20]</span>And in the first forward path, that&#x27;s what you get.</p>
    <p class="cue"><span class="time">[71:25]</span>First forward path, the network is random.</p>
    <p class="cue"><span class="time">[71:27]</span>You get Q value.</p>
    <p class="cue"><span class="time">[71:29]</span>For action 1, 0.5.</p>
    <p class="cue"><span class="time">[71:32]</span>For action 2, 0.4.</p>
    <p class="cue"><span class="time">[71:33]</span>For action 3, 0.3.</p>
    <p class="cue"><span class="time">[71:35]</span>What does that mean?</p>
    <p class="cue"><span class="time">[71:36]</span>It means the agent is saying, I&#x27;m going to go to action 1.</p>
    <p class="cue"><span class="time">[71:40]</span>So I take action 1 and I see an immediate reward of 0.</p>
    <p class="cue"><span class="time">[71:46]</span>Because it&#x27;s a terminal state, the Bellman equation thing</p>
    <p class="cue"><span class="time">[71:48]</span>doesn&#x27;t happen.</p>
    <p class="cue"><span class="time">[71:50]</span>I just have the immediate reward,</p>
    <p class="cue"><span class="time">[71:52]</span>which becomes my target y.</p>
    <p class="cue"><span class="time">[71:54]</span>And so I perform a gradient descent update</p>
    <p class="cue"><span class="time">[71:56]</span>to say this Q value should have been 0.</p>
    <p class="cue"><span class="time">[71:59]</span>So I convert this Q value to 0.</p>
    <p class="cue"><span class="time">[72:02]</span>Now, second try.</p>
    <p class="cue"><span class="time">[72:05]</span>This time, the Q value is saying take action 2.</p>
    <p class="cue"><span class="time">[72:09]</span>It&#x27;s the highest Q value.</p>
    <p class="cue"><span class="time">[72:11]</span>I take action 2.</p>
    <p class="cue"><span class="time">[72:13]</span>I have an immediate reward ahead of me.</p>
    <p class="cue"><span class="time">[72:15]</span>That&#x27;s one.</p>
    <p class="cue"><span class="time">[72:17]</span>Because it&#x27;s a terminal state, there</p>
    <p class="cue"><span class="time">[72:18]</span>is no second discounted future reward term.</p>
    <p class="cue"><span class="time">[72:22]</span>So I just take y equals 1.</p>
    <p class="cue"><span class="time">[72:26]</span>I perform my gradient descent update and this converts to 1.</p>
    <p class="cue"><span class="time">[72:30]</span>And then third time, the agent is still saying go to a2,</p>
    <p class="cue"><span class="time">[72:36]</span>go to the-- take action A2 reward of 1.</p>
    <p class="cue"><span class="time">[72:40]</span>Good.</p>
    <p class="cue"><span class="time">[72:41]</span>That&#x27;s what you predicted.</p>
    <p class="cue"><span class="time">[72:42]</span>Nothing to do.</p>
    <p class="cue"><span class="time">[72:43]</span>Just keep going.</p>
    <p class="cue"><span class="time">[72:44]</span>We&#x27;re done with training.</p>
    <p class="cue"><span class="time">[72:45]</span>We&#x27;re stuck.</p>
    <p class="cue"><span class="time">[72:46]</span>We never visit the state we actually wanted to visit.</p>
    <p class="cue"><span class="time">[72:52]</span>So that wouldn&#x27;t work for us.</p>
    <p class="cue"><span class="time">[72:54]</span>We will never visit that state using our current algorithm.</p>
    <p class="cue"><span class="time">[72:57]</span>Does that make sense why we wouldn&#x27;t ever visit that state?</p>
    <p class="cue"><span class="time">[73:01]</span>In practice, this is a big issue.</p>
    <p class="cue"><span class="time">[73:04]</span>The analogy of this concept of exploration versus exploitation</p>
    <p class="cue"><span class="time">[73:09]</span>is when every day you take your bike and you cross a campus,</p>
    <p class="cue"><span class="time">[73:14]</span>you have a favorite route.</p>
    <p class="cue"><span class="time">[73:16]</span>And it turns out that the more you take that route,</p>
    <p class="cue"><span class="time">[73:19]</span>the better you get every time.</p>
    <p class="cue"><span class="time">[73:20]</span>It get a little faster.</p>
    <p class="cue"><span class="time">[73:21]</span>Maybe your turn is faster or something,</p>
    <p class="cue"><span class="time">[73:23]</span>or you can predict how many people</p>
    <p class="cue"><span class="time">[73:25]</span>are going to be at that roundabout</p>
    <p class="cue"><span class="time">[73:26]</span>and you know how to take it in the wide way.</p>
    <p class="cue"><span class="time">[73:28]</span>So you go faster.</p>
    <p class="cue"><span class="time">[73:29]</span>We&#x27;ve all done that.</p>
    <p class="cue"><span class="time">[73:31]</span>That&#x27;s exploitation.</p>
    <p class="cue"><span class="time">[73:33]</span>You exploit what you already know and you get better at it.</p>
    <p class="cue"><span class="time">[73:35]</span>But maybe there&#x27;s another route that you&#x27;re not</p>
    <p class="cue"><span class="time">[73:38]</span>thinking of that&#x27;s pretty--</p>
    <p class="cue"><span class="time">[73:39]</span>instead of going North from campus, you go South.</p>
    <p class="cue"><span class="time">[73:42]</span>And maybe it might be better.</p>
    <p class="cue"><span class="time">[73:43]</span>You will never see it because you</p>
    <p class="cue"><span class="time">[73:45]</span>don&#x27;t have the courage or the patience to do it.</p>
    <p class="cue"><span class="time">[73:48]</span>That&#x27;s the difference between exploration and exploitation.</p>
    <p class="cue"><span class="time">[73:52]</span>In practice, a good model would be able to handle both,</p>
    <p class="cue"><span class="time">[73:55]</span>to exploit 22 to exploit, to explore</p>
    <p class="cue"><span class="time">[73:57]</span>when it needs to explore.</p>
    <p class="cue"><span class="time">[73:59]</span>The way we do it in practice in our pseudocode</p>
    <p class="cue"><span class="time">[74:02]</span>is to inject some randomness.</p>
    <p class="cue"><span class="time">[74:05]</span>So for example, when we are looping over time step</p>
    <p class="cue"><span class="time">[74:09]</span>with probability epsilon, let&#x27;s say 5%, take a random action.</p>
    <p class="cue"><span class="time">[74:13]</span>So from time to time, on average,</p>
    <p class="cue"><span class="time">[74:15]</span>one time every 20 times, you take a random action,</p>
    <p class="cue"><span class="time">[74:18]</span>it will allow you to visit maybe a new path.</p>
    <p class="cue"><span class="time">[74:22]</span>The analogy in chess is you might use a creative move</p>
    <p class="cue"><span class="time">[74:25]</span>from time to time that might be worse today,</p>
    <p class="cue"><span class="time">[74:28]</span>but might allow you to learn something</p>
    <p class="cue"><span class="time">[74:30]</span>and to get better over time.</p>
    <p class="cue"><span class="time">[74:33]</span>Yeah?</p>
    <p class="cue"><span class="time">[74:34]</span>Is that-- the example you&#x27;ve just covered,</p>
    <p class="cue"><span class="time">[74:38]</span>could we resolve that by just setting the initial Q</p>
    <p class="cue"><span class="time">[74:43]</span>values to infinity?</p>
    <p class="cue"><span class="time">[74:45]</span>Setting the-- couldn&#x27;t we resolve this problem</p>
    <p class="cue"><span class="time">[74:48]</span>by setting the initial values into infinity?</p>
    <p class="cue"><span class="time">[74:51]</span>Well, the problem if you set the initial values to infinity--</p>
    <p class="cue"><span class="time">[74:54]</span>so you would say, instead of randomly initializing</p>
    <p class="cue"><span class="time">[74:57]</span>your network, you initialize it in a way</p>
    <p class="cue"><span class="time">[74:59]</span>that the outputs are equal to infinity.</p>
    <p class="cue"><span class="time">[75:01]</span>Yeah.</p>
    <p class="cue"><span class="time">[75:01]</span>So that we wouldn&#x27;t get the issue</p>
    <p class="cue"><span class="time">[75:03]</span>of where the Q value of actionscript states</p>
    <p class="cue"><span class="time">[75:08]</span>are with that.</p>
    <p class="cue"><span class="time">[75:09]</span>Well, in practice, if the three Q values are infinity,</p>
    <p class="cue"><span class="time">[75:11]</span>then you can&#x27;t make a decision on the spot.</p>
    <p class="cue"><span class="time">[75:13]</span>So you&#x27;re saying just pick one randomly?</p>
    <p class="cue"><span class="time">[75:16]</span>Because if the three are infinity,</p>
    <p class="cue"><span class="time">[75:17]</span>you can&#x27;t decide which one to take, right?</p>
    <p class="cue"><span class="time">[75:21]</span>Right.</p>
    <p class="cue"><span class="time">[75:21]</span>And also, if it&#x27;s infinity, and the reward is 1--</p>
    <p class="cue"><span class="time">[75:25]</span>I mean, if it&#x27;s a really large number and the reward is 1.</p>
    <p class="cue"><span class="time">[75:27]</span>Your gradient is going to be massive, right?</p>
    <p class="cue"><span class="time">[75:30]</span>So it&#x27;s going to--</p>
    <p class="cue"><span class="time">[75:32]</span>I guess the loss function is going to be massive.</p>
    <p class="cue"><span class="time">[75:34]</span>And I don&#x27;t know.</p>
    <p class="cue"><span class="time">[75:35]</span>I imagine it will be really hard to train it.</p>
    <p class="cue"><span class="time">[75:38]</span>But in practice, you start with a random initialization</p>
    <p class="cue"><span class="time">[75:41]</span>because this might be one example.</p>
    <p class="cue"><span class="time">[75:43]</span>But if in the game of chess, actually, the reward</p>
    <p class="cue"><span class="time">[75:49]</span>is 1 at the end and 0 all the time.</p>
    <p class="cue"><span class="time">[75:52]</span>Or maybe the reward is 1,000 at the end.</p>
    <p class="cue"><span class="time">[75:54]</span>And when you lose your rook, it&#x27;s a negative reward.</p>
    <p class="cue"><span class="time">[75:57]</span>You can&#x27;t predict what the reward structure is going to be.</p>
    <p class="cue"><span class="time">[76:00]</span>You want an agent that is able to adapt to it.</p>
    <p class="cue"><span class="time">[76:02]</span>And it&#x27;s better to find a method that</p>
    <p class="cue"><span class="time">[76:05]</span>can scale to different environments, essentially.</p>
    <p class="cue"><span class="time">[76:11]</span>OK.</p>
    <p class="cue"><span class="time">[76:11]</span>So this was epsilon greedy action, which</p>
    <p class="cue"><span class="time">[76:16]</span>is adding some randomness with probability epsilon,</p>
    <p class="cue"><span class="time">[76:19]</span>take a random action.</p>
    <p class="cue"><span class="time">[76:21]</span>OK.</p>
    <p class="cue"><span class="time">[76:21]</span>So adding all our techniques because we</p>
    <p class="cue"><span class="time">[76:25]</span>get good at training reinforcement learning</p>
    <p class="cue"><span class="time">[76:27]</span>algorithms, this is what we have.</p>
    <p class="cue"><span class="time">[76:29]</span>We initialize our Q-network parameters.</p>
    <p class="cue"><span class="time">[76:31]</span>We have a random network.</p>
    <p class="cue"><span class="time">[76:33]</span>We initialize our replay memory D.</p>
    <p class="cue"><span class="time">[76:35]</span>And then we loop over episodes.</p>
    <p class="cue"><span class="time">[76:37]</span>We start from an initial state.</p>
    <p class="cue"><span class="time">[76:38]</span>We create a Boolean that allows us to detect terminal states.</p>
    <p class="cue"><span class="time">[76:42]</span>With probability epsilon, we&#x27;re going to take a random action.</p>
    <p class="cue"><span class="time">[76:45]</span>Otherwise, we&#x27;re going to follow what we know,</p>
    <p class="cue"><span class="time">[76:47]</span>which is forward propagate the state in the Q-network.</p>
    <p class="cue"><span class="time">[76:50]</span>Take the action that has the highest Q value.</p>
    <p class="cue"><span class="time">[76:52]</span>That allows you to observe a reward in the next state.</p>
    <p class="cue"><span class="time">[76:55]</span>Take that next state.</p>
    <p class="cue"><span class="time">[76:57]</span>Forward propagate it again.</p>
    <p class="cue"><span class="time">[77:03]</span>And then instead of--</p>
    <p class="cue"><span class="time">[77:05]</span>oh, no.</p>
    <p class="cue"><span class="time">[77:06]</span>Sorry, sorry.</p>
    <p class="cue"><span class="time">[77:07]</span>Observe that next state.</p>
    <p class="cue"><span class="time">[77:08]</span>Add it to the replay memory.</p>
    <p class="cue"><span class="time">[77:10]</span>Sample from the replay memory, and then train on that sample.</p>
    <p class="cue"><span class="time">[77:15]</span>And in the process, you will need</p>
    <p class="cue"><span class="time">[77:16]</span>to do another forward path because you</p>
    <p class="cue"><span class="time">[77:18]</span>need to estimate your target y using</p>
    <p class="cue"><span class="time">[77:20]</span>the immediate reward plus the Bellman equation</p>
    <p class="cue"><span class="time">[77:23]</span>plus the discounted future reward.</p>
    <p class="cue"><span class="time">[77:27]</span>OK, are you experts at Q-learning?</p>
    <p class="cue"><span class="time">[77:34]</span>OK, good.</p>
    <p class="cue"><span class="time">[77:35]</span>Sounds good.</p>
    <p class="cue"><span class="time">[77:36]</span>And here is where we get at the end.</p>
    <p class="cue"><span class="time">[77:38]</span>You can claim proudly, you have trained an Atari.</p>
    <p class="cue"><span class="time">[77:41]</span>It&#x27;s not that complicated as you can see, other than the Bellman</p>
    <p class="cue"><span class="time">[77:44]</span>equation piece.</p>
    <p class="cue"><span class="time">[77:45]</span>It turns out the agent has discovered</p>
    <p class="cue"><span class="time">[77:47]</span>that it can send the ball on the back,</p>
    <p class="cue"><span class="time">[77:50]</span>and it&#x27;s actually much easier to finish the game like that,</p>
    <p class="cue"><span class="time">[77:53]</span>which is quite interesting.</p>
    <p class="cue"><span class="time">[77:55]</span>A good player would know that you can dig a tunnel,</p>
    <p class="cue"><span class="time">[77:57]</span>and you can finish the game without too much issues.</p>
    <p class="cue"><span class="time">[77:59]</span>Yeah?</p>
    <p class="cue"><span class="time">[78:00]</span>How do you quantify the results of the edge of the repository?</p>
    <p class="cue"><span class="time">[78:04]</span>How do you quantify when the game has ended?</p>
    <p class="cue"><span class="time">[78:07]</span>How good the model becomes after the [INAUDIBLE]?</p>
    <p class="cue"><span class="time">[78:10]</span>Yeah.</p>
    <p class="cue"><span class="time">[78:11]</span>Well, first, you would start seeing</p>
    <p class="cue"><span class="time">[78:16]</span>the model get two good rewards as it play,</p>
    <p class="cue"><span class="time">[78:18]</span>like it manages to get really good rewards.</p>
    <p class="cue"><span class="time">[78:20]</span>While earlier, it might not.</p>
    <p class="cue"><span class="time">[78:23]</span>And so that&#x27;s probably your best guess for how good the model is.</p>
    <p class="cue"><span class="time">[78:27]</span>In practice, if you&#x27;re AlphaGo, you</p>
    <p class="cue"><span class="time">[78:29]</span>can also test it against the best humans in the world,</p>
    <p class="cue"><span class="time">[78:31]</span>and you can observe that they&#x27;re losing against the model.</p>
    <p class="cue"><span class="time">[78:34]</span>But then you have a bunch of different chess engines,</p>
    <p class="cue"><span class="time">[78:38]</span>and some of them are way, way better than others.</p>
    <p class="cue"><span class="time">[78:41]</span>They have different structure.</p>
    <p class="cue"><span class="time">[78:44]</span>Maybe they&#x27;re both based on reinforcement learning.</p>
    <p class="cue"><span class="time">[78:46]</span>And at the end, they maximize both of their rewards.</p>
    <p class="cue"><span class="time">[78:49]</span>Right.</p>
    <p class="cue"><span class="time">[78:49]</span>So how do you know which model is actually doing better?</p>
    <p class="cue"><span class="time">[78:53]</span>You can get them to play together.</p>
    <p class="cue"><span class="time">[78:55]</span>So you have no idea.</p>
    <p class="cue"><span class="time">[78:56]</span>You just have one.</p>
    <p class="cue"><span class="time">[78:57]</span>No.</p>
    <p class="cue"><span class="time">[78:58]</span>You could actually monitor the loss function.</p>
    <p class="cue"><span class="time">[79:00]</span>And look at, is the Bellman equation respected?</p>
    <p class="cue"><span class="time">[79:04]</span>If the Bellman equation is respected,</p>
    <p class="cue"><span class="time">[79:06]</span>then your model is really, really good.</p>
    <p class="cue"><span class="time">[79:08]</span>And then we&#x27;re going to see an example</p>
    <p class="cue"><span class="time">[79:11]</span>of competitive self-play, where you get the model to play</p>
    <p class="cue"><span class="time">[79:14]</span>against other models.</p>
    <p class="cue"><span class="time">[79:15]</span>And then over time, as you watch them</p>
    <p class="cue"><span class="time">[79:17]</span>play for thousands and thousands of time,</p>
    <p class="cue"><span class="time">[79:19]</span>you can tell which model is ahead of another one.</p>
    <p class="cue"><span class="time">[79:21]</span>You can then copy, paste the best model</p>
    <p class="cue"><span class="time">[79:25]</span>into the other models, and then make</p>
    <p class="cue"><span class="time">[79:27]</span>them play again for many times.</p>
    <p class="cue"><span class="time">[79:29]</span>And because you have the epsilon greedy approach,</p>
    <p class="cue"><span class="time">[79:31]</span>one of the models is naturally going</p>
    <p class="cue"><span class="time">[79:33]</span>to get better than the others because of the randomness</p>
    <p class="cue"><span class="time">[79:36]</span>that you add.</p>
    <p class="cue"><span class="time">[79:41]</span>OK, let&#x27;s look at a few examples,</p>
    <p class="cue"><span class="time">[79:42]</span>and then we&#x27;ll spend 20 minutes on the RLHF.</p>
    <p class="cue"><span class="time">[79:45]</span>Here are other examples.</p>
    <p class="cue"><span class="time">[79:47]</span>This is Pong, which is 1V1.</p>
    <p class="cue"><span class="time">[79:51]</span>SeaQuest, which is an underwater game.</p>
    <p class="cue"><span class="time">[79:57]</span>And then the one that&#x27;s maybe more of you</p>
    <p class="cue"><span class="time">[79:58]</span>know, Space Invaders, very popular game as well.</p>
    <p class="cue"><span class="time">[80:03]</span>So the impressive thing that they showed</p>
    <p class="cue"><span class="time">[80:05]</span>is that you can actually solve many games</p>
    <p class="cue"><span class="time">[80:09]</span>with the exact same algorithm.</p>
    <p class="cue"><span class="time">[80:11]</span>No tweaks, which is quite impressive.</p>
    <p class="cue"><span class="time">[80:18]</span>Let&#x27;s go a little further and talk about advanced topics.</p>
    <p class="cue"><span class="time">[80:23]</span>Here is a game called Montezuma&#x27;s Revenge.</p>
    <p class="cue"><span class="time">[80:27]</span>This game is particular because you&#x27;re controlling</p>
    <p class="cue"><span class="time">[80:29]</span>a little character right here.</p>
    <p class="cue"><span class="time">[80:31]</span>And this character is trying to go and grab,</p>
    <p class="cue"><span class="time">[80:34]</span>let&#x27;s say, this key right here.</p>
    <p class="cue"><span class="time">[80:36]</span>And it has some obstacles or some enemies</p>
    <p class="cue"><span class="time">[80:39]</span>that it needs to take care of.</p>
    <p class="cue"><span class="time">[80:43]</span>What do you think is going to be an issue if we apply what</p>
    <p class="cue"><span class="time">[80:47]</span>we just learned to this game?</p>
    <p class="cue"><span class="time">[80:52]</span>What makes this game especially hard in comparison</p>
    <p class="cue"><span class="time">[80:55]</span>to, let&#x27;s say, chess or Go?</p>
    <p class="cue"><span class="time">[80:58]</span>Yes?</p>
    <p class="cue"><span class="time">[80:58]</span>The reward is very delayed.</p>
    <p class="cue"><span class="time">[81:00]</span>Yeah.</p>
    <p class="cue"><span class="time">[81:01]</span>The reward is very delayed.</p>
    <p class="cue"><span class="time">[81:03]</span>If you start with a random network, what</p>
    <p class="cue"><span class="time">[81:06]</span>are the chances that the network is going to figure out</p>
    <p class="cue"><span class="time">[81:09]</span>that-- to get to the key, it actually</p>
    <p class="cue"><span class="time">[81:11]</span>should go in the opposite direction.</p>
    <p class="cue"><span class="time">[81:13]</span>It should go in the opposite direction.</p>
    <p class="cue"><span class="time">[81:15]</span>It should jump down here.</p>
    <p class="cue"><span class="time">[81:16]</span>It should catch the rope.</p>
    <p class="cue"><span class="time">[81:18]</span>The rope will probably allow the character to go to the ladder.</p>
    <p class="cue"><span class="time">[81:22]</span>It goes down the ladder.</p>
    <p class="cue"><span class="time">[81:23]</span>It has to go jump up this enemy.</p>
    <p class="cue"><span class="time">[81:26]</span>My guess is it&#x27;s in an enemy.</p>
    <p class="cue"><span class="time">[81:27]</span>I&#x27;m not sure, but I think it&#x27;s an enemy because of the color.</p>
    <p class="cue"><span class="time">[81:31]</span>And that in gaming, if it was green,</p>
    <p class="cue"><span class="time">[81:33]</span>it might not have been an enemy, but it&#x27;s gray or red,</p>
    <p class="cue"><span class="time">[81:36]</span>it might be an enemy.</p>
    <p class="cue"><span class="time">[81:37]</span>And then go up the ladder and grab the key.</p>
    <p class="cue"><span class="time">[81:40]</span>The chance is very low that the agent</p>
    <p class="cue"><span class="time">[81:44]</span>is going to make that successive good decisions to get there.</p>
    <p class="cue"><span class="time">[81:47]</span>You&#x27;re right.</p>
    <p class="cue"><span class="time">[81:49]</span>Why is it easier for a human to actually solve that game?</p>
    <p class="cue"><span class="time">[81:55]</span>Intuition.</p>
    <p class="cue"><span class="time">[81:56]</span>Intuition, prior knowledge.</p>
    <p class="cue"><span class="time">[81:59]</span>So for example, when you look at this game,</p>
    <p class="cue"><span class="time">[82:01]</span>even if you have never played it,</p>
    <p class="cue"><span class="time">[82:03]</span>my guess is you would know you can go down the ladder,</p>
    <p class="cue"><span class="time">[82:05]</span>because you what a ladder is.</p>
    <p class="cue"><span class="time">[82:07]</span>Or you can see this little rope and you&#x27;re like,</p>
    <p class="cue"><span class="time">[82:09]</span>I&#x27;m going to catch the rope.</p>
    <p class="cue"><span class="time">[82:10]</span>I&#x27;m going to jump and go to the other side.</p>
    <p class="cue"><span class="time">[82:12]</span>And you look at this little monster</p>
    <p class="cue"><span class="time">[82:13]</span>and you&#x27;re like, I better not touch this monster.</p>
    <p class="cue"><span class="time">[82:15]</span>Or if anything, I will jump on top of it,</p>
    <p class="cue"><span class="time">[82:18]</span>because you&#x27;ve played Mario, let&#x27;s say.</p>
    <p class="cue"><span class="time">[82:20]</span>So all of this is human intuition.</p>
    <p class="cue"><span class="time">[82:24]</span>Sometimes you would call as a baby survival instinct.</p>
    <p class="cue"><span class="time">[82:27]</span>You throw the baby in the water and suddenly it flips</p>
    <p class="cue"><span class="time">[82:29]</span>and it can swim.</p>
    <p class="cue"><span class="time">[82:32]</span>Those are things that are, to a certain extent,</p>
    <p class="cue"><span class="time">[82:34]</span>encoded in our DNA.</p>
    <p class="cue"><span class="time">[82:35]</span>But at the very least, encoded in our experience</p>
    <p class="cue"><span class="time">[82:37]</span>of doing other things that have nothing to do with this game.</p>
    <p class="cue"><span class="time">[82:40]</span>And so the problem here is called imitation learning is,</p>
    <p class="cue"><span class="time">[82:43]</span>is there a better way to start our network</p>
    <p class="cue"><span class="time">[82:47]</span>than a random initialization that allows the network to,</p>
    <p class="cue"><span class="time">[82:50]</span>for example, guess that this is a ladder?</p>
    <p class="cue"><span class="time">[82:52]</span>And it turns out that if the network</p>
    <p class="cue"><span class="time">[82:53]</span>knows that, it will be more likely to get to the reward</p>
    <p class="cue"><span class="time">[82:56]</span>first, and then learn from that reward</p>
    <p class="cue"><span class="time">[82:57]</span>and then get better over time.</p>
    <p class="cue"><span class="time">[83:01]</span>The other part that can also use human knowledge, which</p>
    <p class="cue"><span class="time">[83:03]</span>is what we&#x27;re going to see together,</p>
    <p class="cue"><span class="time">[83:05]</span>is reinforcement learning from human feedback, where you have</p>
    <p class="cue"><span class="time">[83:10]</span>an analogy here, which is you can train a language model,</p>
    <p class="cue"><span class="time">[83:13]</span>and it might be completely misaligned with what</p>
    <p class="cue"><span class="time">[83:15]</span>actually humans care about.</p>
    <p class="cue"><span class="time">[83:16]</span>How does reinforcement learning help in those situations?</p>
    <p class="cue"><span class="time">[83:19]</span>That&#x27;s going to be the next topic in the last part</p>
    <p class="cue"><span class="time">[83:21]</span>of the lecture.</p>
    <p class="cue"><span class="time">[83:23]</span>OK, let me show you a few other results quickly.</p>
    <p class="cue"><span class="time">[83:27]</span>Today, we talked about DQN, deep Q-learning.</p>
    <p class="cue"><span class="time">[83:30]</span>In practice, there is a lot more reinforcement</p>
    <p class="cue"><span class="time">[83:33]</span>learning algorithm, but you got the gist of it.</p>
    <p class="cue"><span class="time">[83:35]</span>You got the concept of making good sequences of decision,</p>
    <p class="cue"><span class="time">[83:38]</span>epsilon greedy, exploration, exploitation, terminal state,</p>
    <p class="cue"><span class="time">[83:42]</span>starting state.</p>
    <p class="cue"><span class="time">[83:43]</span>All of that, you got.</p>
    <p class="cue"><span class="time">[83:45]</span>The one algorithm that is very popular</p>
    <p class="cue"><span class="time">[83:47]</span>right now is called PPO, proximal policy optimization.</p>
    <p class="cue"><span class="time">[83:52]</span>There is one that is even more popular right now.</p>
    <p class="cue"><span class="time">[83:54]</span>That&#x27;s actually from a year ago at Stanford called DPO that we</p>
    <p class="cue"><span class="time">[83:58]</span>won&#x27;t study in the class.</p>
    <p class="cue"><span class="time">[84:00]</span>One of the things to know about PPO, just</p>
    <p class="cue"><span class="time">[84:02]</span>to go over it really quickly, and I</p>
    <p class="cue"><span class="time">[84:04]</span>pasted two important papers from Schulman et al a few years back,</p>
    <p class="cue"><span class="time">[84:08]</span>just TRPO and PPO, is that it is not a value-based algorithm.</p>
    <p class="cue"><span class="time">[84:13]</span>So in Q-learning, you learn the Q values</p>
    <p class="cue"><span class="time">[84:15]</span>and then you define your policy as the argmax of the Q values.</p>
    <p class="cue"><span class="time">[84:20]</span>In PPO, you&#x27;ll learn the policy directly,</p>
    <p class="cue"><span class="time">[84:23]</span>which is a more probabilistic method.</p>
    <p class="cue"><span class="time">[84:26]</span>It also works well with continuous spaces.</p>
    <p class="cue"><span class="time">[84:29]</span>If you look at Q-learning we learned,</p>
    <p class="cue"><span class="time">[84:31]</span>one output for one action.</p>
    <p class="cue"><span class="time">[84:33]</span>If you actually have a game that has continuous action,</p>
    <p class="cue"><span class="time">[84:38]</span>like autonomous driving, where it&#x27;s not</p>
    <p class="cue"><span class="time">[84:40]</span>like just turn the wheel to the right or to the left,</p>
    <p class="cue"><span class="time">[84:42]</span>it&#x27;s what degree you turn it, it&#x27;s continuous,</p>
    <p class="cue"><span class="time">[84:45]</span>then DQN would not work well, or you</p>
    <p class="cue"><span class="time">[84:49]</span>would have to granularized the number of actions a little bit</p>
    <p class="cue"><span class="time">[84:51]</span>to the right, a little bit more a little more,</p>
    <p class="cue"><span class="time">[84:53]</span>which would not be really useful.</p>
    <p class="cue"><span class="time">[84:55]</span>Instead you would use PPO.</p>
    <p class="cue"><span class="time">[84:58]</span>Yeah?</p>
    <p class="cue"><span class="time">[85:00]</span>We learn that the immediate reward for a game like Go</p>
    <p class="cue"><span class="time">[85:03]</span>is 0 for most steps.</p>
    <p class="cue"><span class="time">[85:05]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[85:06]</span>Yeah.</p>
    <p class="cue"><span class="time">[85:07]</span>So a question is, how do you define the reward in DQN?</p>
    <p class="cue"><span class="time">[85:12]</span>Different reward structure will lead to different types</p>
    <p class="cue"><span class="time">[85:15]</span>of agent strategies.</p>
    <p class="cue"><span class="time">[85:17]</span>But you&#x27;re right.</p>
    <p class="cue"><span class="time">[85:18]</span>For the game of Go, you could actually define the reward as 1</p>
    <p class="cue"><span class="time">[85:22]</span>if you win and 0 if you don&#x27;t win.</p>
    <p class="cue"><span class="time">[85:24]</span>That&#x27;s it.</p>
    <p class="cue"><span class="time">[85:25]</span>Every move will be 0 until the last move is a win.</p>
    <p class="cue"><span class="time">[85:28]</span>In chess, you might actually do intermediate reward</p>
    <p class="cue"><span class="time">[85:30]</span>because you want to tell the agent that it&#x27;s is</p>
    <p class="cue"><span class="time">[85:33]</span>good to kill the opponent&#x27;s pieces to get rid of them.</p>
    <p class="cue"><span class="time">[85:37]</span>You could also do end-to-end and say</p>
    <p class="cue"><span class="time">[85:38]</span>I don&#x27;t give any intermediate reward.</p>
    <p class="cue"><span class="time">[85:40]</span>I just give a final reward, which</p>
    <p class="cue"><span class="time">[85:42]</span>might be more complicated to train on,</p>
    <p class="cue"><span class="time">[85:43]</span>but it might actually lead to a more optimal strategy.</p>
    <p class="cue"><span class="time">[85:46]</span>Because, in fact, you could actually</p>
    <p class="cue"><span class="time">[85:48]</span>win without taking any piece from your opponent.</p>
    <p class="cue"><span class="time">[85:51]</span>So other things about PPO is it&#x27;s more probabilistic.</p>
    <p class="cue"><span class="time">[85:57]</span>It has a concept of an expected advantage, which at every step,</p>
    <p class="cue"><span class="time">[86:01]</span>instead of telling you how good that action is,</p>
    <p class="cue"><span class="time">[86:04]</span>it will tell you how much better it is than random--</p>
    <p class="cue"><span class="time">[86:07]</span>than the current state.</p>
    <p class="cue"><span class="time">[86:08]</span>How much better would it be to do certain things versus what</p>
    <p class="cue"><span class="time">[86:11]</span>you would have done otherwise?</p>
    <p class="cue"><span class="time">[86:12]</span>I&#x27;m not going to go into the details.</p>
    <p class="cue"><span class="time">[86:14]</span>It&#x27;s all in the paper, but those are things that are important.</p>
    <p class="cue"><span class="time">[86:17]</span>Here are a few examples of PPO.</p>
    <p class="cue"><span class="time">[86:19]</span>So this example on the left is from OpenAI a few years back,</p>
    <p class="cue"><span class="time">[86:24]</span>where you can see it&#x27;s a continuous space, where</p>
    <p class="cue"><span class="time">[86:27]</span>the agent is being bullied a little bit,</p>
    <p class="cue"><span class="time">[86:32]</span>but it&#x27;s trying to grab the rewards.</p>
    <p class="cue"><span class="time">[86:35]</span>But it&#x27;s also subject to external forces</p>
    <p class="cue"><span class="time">[86:37]</span>that are throwing balls at it.</p>
    <p class="cue"><span class="time">[86:40]</span>It&#x27;s a little bit mean.</p>
    <p class="cue"><span class="time">[86:44]</span>You can imagine that this is a continuous space, meaning</p>
    <p class="cue"><span class="time">[86:47]</span>you&#x27;re controlling the nodes, you&#x27;re</p>
    <p class="cue"><span class="time">[86:49]</span>controlling the joints of the agent,</p>
    <p class="cue"><span class="time">[86:51]</span>and you&#x27;re controlling the forces, the angles.</p>
    <p class="cue"><span class="time">[86:53]</span>And so that&#x27;s why PPO would be better in that case.</p>
    <p class="cue"><span class="time">[86:58]</span>Super.</p>
    <p class="cue"><span class="time">[86:59]</span>Here is a competitive self-play, which I really like,</p>
    <p class="cue"><span class="time">[87:05]</span>where you have agent play with each other.</p>
    <p class="cue"><span class="time">[87:08]</span>And this is the Sumo game.</p>
    <p class="cue"><span class="time">[87:09]</span>Push the opponent outside the ring and you get a reward.</p>
    <p class="cue"><span class="time">[87:12]</span>So actually, it&#x27;s interesting because you&#x27;re</p>
    <p class="cue"><span class="time">[87:14]</span>seeing some emergent behavior, which</p>
    <p class="cue"><span class="time">[87:16]</span>is they attack each other&#x27;s feet,</p>
    <p class="cue"><span class="time">[87:19]</span>or they lower their center of gravity to be more stable,</p>
    <p class="cue"><span class="time">[87:23]</span>for example.</p>
    <p class="cue"><span class="time">[87:24]</span>Yeah?</p>
    <p class="cue"><span class="time">[87:25]</span>Is this the exact same one from [INAUDIBLE]?</p>
    <p class="cue"><span class="time">[87:28]</span>Yeah.</p>
    <p class="cue"><span class="time">[87:28]</span>It&#x27;s versions.</p>
    <p class="cue"><span class="time">[87:29]</span>Sometimes different initializations, for example.</p>
    <p class="cue"><span class="time">[87:32]</span>But it has to be [INAUDIBLE].</p>
    <p class="cue"><span class="time">[87:34]</span>So no, but good question.</p>
    <p class="cue"><span class="time">[87:35]</span>So oftentimes what OpenAI would do back in that time</p>
    <p class="cue"><span class="time">[87:39]</span>is they would create copies of the same model.</p>
    <p class="cue"><span class="time">[87:43]</span>They would initialize them differently.</p>
    <p class="cue"><span class="time">[87:45]</span>And they would let them learn.</p>
    <p class="cue"><span class="time">[87:46]</span>And it turns out one of the models</p>
    <p class="cue"><span class="time">[87:48]</span>we get better than the others.</p>
    <p class="cue"><span class="time">[87:49]</span>And then they will copy again that model to the rest</p>
    <p class="cue"><span class="time">[87:52]</span>and do the same thing again and again, pretty much.</p>
    <p class="cue"><span class="time">[87:55]</span>Yeah.</p>
    <p class="cue"><span class="time">[87:56]</span>Yeah.</p>
    <p class="cue"><span class="time">[87:57]</span>It&#x27;s kind of funny, isn&#x27;t it?</p>
    <p class="cue"><span class="time">[87:59]</span>That&#x27;s a good catch.</p>
    <p class="cue"><span class="time">[88:05]</span>That&#x27;s a good goal.</p>
    <p class="cue"><span class="time">[88:08]</span>I could watch that for hours.</p>
    <p class="cue"><span class="time">[88:11]</span>OK.</p>
    <p class="cue"><span class="time">[88:13]</span>Very little awkward, you have to say, but it works.</p>
    <p class="cue"><span class="time">[88:17]</span>OK.</p>
    <p class="cue"><span class="time">[88:18]</span>So I&#x27;ll let you watch the video.</p>
    <p class="cue"><span class="time">[88:20]</span>It&#x27;s going to be shared.</p>
    <p class="cue"><span class="time">[88:21]</span>But here is another set of games that are even more complicated</p>
    <p class="cue"><span class="time">[88:25]</span>that I mentioned early on, OpenAI Five,</p>
    <p class="cue"><span class="time">[88:28]</span>which you can think of an equivalent of League of Legends,</p>
    <p class="cue"><span class="time">[88:31]</span>DOTA, where you have 5v5 game.</p>
    <p class="cue"><span class="time">[88:34]</span>So you have to collaborate, et cetera, which makes--</p>
    <p class="cue"><span class="time">[88:38]</span>adds literally one additional degree of complexity.</p>
    <p class="cue"><span class="time">[88:43]</span>And StarCraft-- AlphaStar from DeepMind</p>
    <p class="cue"><span class="time">[88:47]</span>is an example of where the observation is not</p>
    <p class="cue"><span class="time">[88:50]</span>the entire state.</p>
    <p class="cue"><span class="time">[88:51]</span>You have fog.</p>
    <p class="cue"><span class="time">[88:52]</span>And so that adds another layer of complexity.</p>
    <p class="cue"><span class="time">[88:54]</span>We&#x27;re not going to see that together today.</p>
    <p class="cue"><span class="time">[88:59]</span>I would encourage you to look at the AlphaGo documentary</p>
    <p class="cue"><span class="time">[89:03]</span>on Netflix if you haven&#x27;t.</p>
    <p class="cue"><span class="time">[89:04]</span>Who has seen it already?</p>
    <p class="cue"><span class="time">[89:06]</span>Nobody.</p>
    <p class="cue"><span class="time">[89:07]</span>OK.</p>
    <p class="cue"><span class="time">[89:07]</span>Well, you can now watch it with a different AI understanding</p>
    <p class="cue"><span class="time">[89:11]</span>reinforcement learning.</p>
    <p class="cue"><span class="time">[89:13]</span>And at some point in the documentary,</p>
    <p class="cue"><span class="time">[89:18]</span>you will see that AlphaGo makes a very odd move, a very creative</p>
    <p class="cue"><span class="time">[89:25]</span>move.</p>
    <p class="cue"><span class="time">[89:26]</span>And people are like, I don&#x27;t understand that move.</p>
    <p class="cue"><span class="time">[89:28]</span>Even the top researchers or the best players</p>
    <p class="cue"><span class="time">[89:32]</span>would say in the video, they don&#x27;t understand that move.</p>
    <p class="cue"><span class="time">[89:35]</span>It turns out that move is very unintuitive for humans.</p>
    <p class="cue"><span class="time">[89:38]</span>Because as humans, we are trained to maximize</p>
    <p class="cue"><span class="time">[89:41]</span>our chances of winning.</p>
    <p class="cue"><span class="time">[89:42]</span>Literally, if I can eat all your pieces in chess,</p>
    <p class="cue"><span class="time">[89:45]</span>I will eat all your pieces.</p>
    <p class="cue"><span class="time">[89:47]</span>And if I can surround your stones in go as much as I can,</p>
    <p class="cue"><span class="time">[89:50]</span>I will do it.</p>
    <p class="cue"><span class="time">[89:52]</span>The agent is just programmed to win.</p>
    <p class="cue"><span class="time">[89:54]</span>So that move actually looks counterintuitive,</p>
    <p class="cue"><span class="time">[89:56]</span>because the agent doesn&#x27;t care about winning by one</p>
    <p class="cue"><span class="time">[89:59]</span>or winning by a 20 stones.</p>
    <p class="cue"><span class="time">[90:01]</span>It just cares about winning.</p>
    <p class="cue"><span class="time">[90:03]</span>And that move specifically puts the agent in a good place</p>
    <p class="cue"><span class="time">[90:06]</span>to win by a small margin.</p>
    <p class="cue"><span class="time">[90:08]</span>So that&#x27;s an example of an insight that you will learn,</p>
    <p class="cue"><span class="time">[90:11]</span>you understand from this class, and you</p>
    <p class="cue"><span class="time">[90:13]</span>will see in the documentary.</p>
    <p class="cue"><span class="time">[90:18]</span>OK.</p>
    <p class="cue"><span class="time">[90:19]</span>I think we have 10 minutes.</p>
    <p class="cue"><span class="time">[90:20]</span>I&#x27;m just going to introduce a reinforcement learning</p>
    <p class="cue"><span class="time">[90:23]</span>from human feedback because it&#x27;s a more modern topic that</p>
    <p class="cue"><span class="time">[90:29]</span>is very trendy right now.</p>
    <p class="cue"><span class="time">[90:30]</span>It&#x27;s important to know.</p>
    <p class="cue"><span class="time">[90:31]</span>And so let&#x27;s look at it together.</p>
    <p class="cue"><span class="time">[90:33]</span>We&#x27;re going to start by recapping how language models</p>
    <p class="cue"><span class="time">[90:35]</span>are trained in a nutshell.</p>
    <p class="cue"><span class="time">[90:37]</span>And then we&#x27;ll see what supervised fine-tuning</p>
    <p class="cue"><span class="time">[90:41]</span>looks like.</p>
    <p class="cue"><span class="time">[90:41]</span>We&#x27;ll talk about how do we train a critic model, a reward model.</p>
    <p class="cue"><span class="time">[90:45]</span>And then finally, what RLHF looks like</p>
    <p class="cue"><span class="time">[90:49]</span>and why is it so trending in the news.</p>
    <p class="cue"><span class="time">[90:51]</span>So our training objective for language models</p>
    <p class="cue"><span class="time">[90:56]</span>is next token prediction.</p>
    <p class="cue"><span class="time">[90:58]</span>We&#x27;ve already talked about it in a formal lecture.</p>
    <p class="cue"><span class="time">[91:01]</span>The idea is that I will get some inputs.</p>
    <p class="cue"><span class="time">[91:03]</span>I&#x27;m reading Wikipedia, let&#x27;s say, or some of a text online.</p>
    <p class="cue"><span class="time">[91:07]</span>And I read a sentence and I predict the last token.</p>
    <p class="cue"><span class="time">[91:11]</span>And I do that again and again.</p>
    <p class="cue"><span class="time">[91:12]</span>So for example, deep learning, and then deep learning</p>
    <p class="cue"><span class="time">[91:17]</span>is, deep learning is so, deep learning is so cool,</p>
    <p class="cue"><span class="time">[91:25]</span>and that&#x27;s it.</p>
    <p class="cue"><span class="time">[91:27]</span>So you get the idea.</p>
    <p class="cue"><span class="time">[91:28]</span>You always predict the next token.</p>
    <p class="cue"><span class="time">[91:30]</span>And then over time, it forces the model</p>
    <p class="cue"><span class="time">[91:32]</span>to explicit emerging behaviors.</p>
    <p class="cue"><span class="time">[91:36]</span>And it understands the connections</p>
    <p class="cue"><span class="time">[91:37]</span>between those concepts.</p>
    <p class="cue"><span class="time">[91:39]</span>And it&#x27;s really good at generating texts.</p>
    <p class="cue"><span class="time">[91:42]</span>We compute a loss function.</p>
    <p class="cue"><span class="time">[91:44]</span>You&#x27;re actually going to study this loss function in C5.</p>
    <p class="cue"><span class="time">[91:48]</span>So I&#x27;m not going to talk about it right now.</p>
    <p class="cue"><span class="time">[91:50]</span>But you perform a gradient descent loop.</p>
    <p class="cue"><span class="time">[91:55]</span>And this is how you get your first pre-trained language</p>
    <p class="cue"><span class="time">[91:58]</span>model.</p>
    <p class="cue"><span class="time">[91:59]</span>You can pre-trained language model.</p>
    <p class="cue"><span class="time">[92:00]</span>You can call it on a text or a prompt.</p>
    <p class="cue"><span class="time">[92:03]</span>And it will continually generate.</p>
    <p class="cue"><span class="time">[92:04]</span>And you call it again and again and again.</p>
    <p class="cue"><span class="time">[92:06]</span>And it generates, generates, generates.</p>
    <p class="cue"><span class="time">[92:08]</span>Everybody&#x27;s comfortable with that, right?</p>
    <p class="cue"><span class="time">[92:10]</span>OK.</p>
    <p class="cue"><span class="time">[92:11]</span>So that&#x27;s how we train a language model,</p>
    <p class="cue"><span class="time">[92:14]</span>but there is a couple of problems.</p>
    <p class="cue"><span class="time">[92:17]</span>The first problem is that online data does not</p>
    <p class="cue"><span class="time">[92:22]</span>reflect helpfulness.</p>
    <p class="cue"><span class="time">[92:25]</span>So to give you a concrete example, what</p>
    <p class="cue"><span class="time">[92:28]</span>you might find in the training set</p>
    <p class="cue"><span class="time">[92:30]</span>is something like deep learning is so cool.</p>
    <p class="cue"><span class="time">[92:32]</span>When actually what you might find in practice</p>
    <p class="cue"><span class="time">[92:35]</span>is people asking what is deep learning.</p>
    <p class="cue"><span class="time">[92:39]</span>So the data is not really reflective</p>
    <p class="cue"><span class="time">[92:41]</span>of you want an agent to be helpful.</p>
    <p class="cue"><span class="time">[92:43]</span>And that&#x27;s a problem because the model</p>
    <p class="cue"><span class="time">[92:46]</span>was trained to continue text rather than answer questions.</p>
    <p class="cue"><span class="time">[92:50]</span>And in practice, you would see it&#x27;s a big problem.</p>
    <p class="cue"><span class="time">[92:53]</span>Another problem is the model has no concept</p>
    <p class="cue"><span class="time">[92:55]</span>of good, polite or helpful yet.</p>
    <p class="cue"><span class="time">[92:59]</span>And to give you a concrete example,</p>
    <p class="cue"><span class="time">[93:01]</span>you might actually ask a pre-trained language model.</p>
    <p class="cue"><span class="time">[93:04]</span>My laptop won&#x27;t turn on.</p>
    <p class="cue"><span class="time">[93:05]</span>What should I do?</p>
    <p class="cue"><span class="time">[93:07]</span>And then the model responds because it has read it</p>
    <p class="cue"><span class="time">[93:09]</span>on Reddit or on Wikipedia, is laptops</p>
    <p class="cue"><span class="time">[93:12]</span>sometimes don&#x27;t turn on because of power issues,</p>
    <p class="cue"><span class="time">[93:15]</span>which is not what you ask.</p>
    <p class="cue"><span class="time">[93:16]</span>You ask, what should I do.</p>
    <p class="cue"><span class="time">[93:19]</span>And in fact, a better answer would</p>
    <p class="cue"><span class="time">[93:20]</span>have been check your charger if it&#x27;s properly connected</p>
    <p class="cue"><span class="time">[93:24]</span>or the outlet works.</p>
    <p class="cue"><span class="time">[93:25]</span>If that&#x27;s fine, try holding the power button for 10 seconds.</p>
    <p class="cue"><span class="time">[93:28]</span>If it still doesn&#x27;t start, the battery or motherboard</p>
    <p class="cue"><span class="time">[93:30]</span>may blah, blah, blah, blah.</p>
    <p class="cue"><span class="time">[93:31]</span>That&#x27;s a better answer.</p>
    <p class="cue"><span class="time">[93:32]</span>That&#x27;s what you want a language model to do nowadays.</p>
    <p class="cue"><span class="time">[93:35]</span>And the model can give you factual text</p>
    <p class="cue"><span class="time">[93:38]</span>because that&#x27;s what it&#x27;s been trained on.</p>
    <p class="cue"><span class="time">[93:40]</span>But it doesn&#x27;t understand being helpful or having an answer that</p>
    <p class="cue"><span class="time">[93:44]</span>looks like a human-like answer.</p>
    <p class="cue"><span class="time">[93:48]</span>So our solution to it will start with using</p>
    <p class="cue"><span class="time">[93:51]</span>supervised fine-tuning, which is going</p>
    <p class="cue"><span class="time">[93:54]</span>to be learning from human written demonstrations</p>
    <p class="cue"><span class="time">[93:57]</span>of helpful behavior.</p>
    <p class="cue"><span class="time">[93:59]</span>And then we&#x27;ll get to even further and use</p>
    <p class="cue"><span class="time">[94:01]</span>RLHF, which will optimize not only for human written sentences</p>
    <p class="cue"><span class="time">[94:05]</span>or paragraphs, but for preferences.</p>
    <p class="cue"><span class="time">[94:08]</span>And the word preference is the keyword.</p>
    <p class="cue"><span class="time">[94:10]</span>Let&#x27;s talk about how we can improve our pre-trained model</p>
    <p class="cue"><span class="time">[94:13]</span>with supervised fine-tuning.</p>
    <p class="cue"><span class="time">[94:15]</span>I&#x27;ll take that we want to align models</p>
    <p class="cue"><span class="time">[94:18]</span>with human written responses.</p>
    <p class="cue"><span class="time">[94:22]</span>And the step one that we&#x27;re going to use</p>
    <p class="cue"><span class="time">[94:24]</span>is to build a data set.</p>
    <p class="cue"><span class="time">[94:26]</span>Let&#x27;s build the data sets of human prompt response pairs.</p>
    <p class="cue"><span class="time">[94:31]</span>So what actually OpenAI is going to do,</p>
    <p class="cue"><span class="time">[94:33]</span>I&#x27;ll explain it in a second, is it</p>
    <p class="cue"><span class="time">[94:34]</span>might collect some of the prompts</p>
    <p class="cue"><span class="time">[94:37]</span>that we all use, and then ask humans</p>
    <p class="cue"><span class="time">[94:39]</span>to respond to those prompts and put that in a data set.</p>
    <p class="cue"><span class="time">[94:42]</span>It might also ask separately experts</p>
    <p class="cue"><span class="time">[94:45]</span>to write really good prompts and then answer those prompts.</p>
    <p class="cue"><span class="time">[94:48]</span>It&#x27;s a fully human made data set.</p>
    <p class="cue"><span class="time">[94:51]</span>And then we&#x27;ll use that data set to fine-tune</p>
    <p class="cue"><span class="time">[94:54]</span>our pre-trained model.</p>
    <p class="cue"><span class="time">[94:55]</span>And by now, you&#x27;ve learned fine-tuning in the online video.</p>
    <p class="cue"><span class="time">[94:58]</span>So you know what I&#x27;m talking about using supervised learning.</p>
    <p class="cue"><span class="time">[95:01]</span>So what it looks like is I take my pre-trained model</p>
    <p class="cue"><span class="time">[95:04]</span>that I just told you how we train.</p>
    <p class="cue"><span class="time">[95:06]</span>And then I give it a prompt, explain deep learning</p>
    <p class="cue"><span class="time">[95:09]</span>to a beginner.</p>
    <p class="cue"><span class="time">[95:10]</span>And I also will concatenate to it</p>
    <p class="cue"><span class="time">[95:14]</span>a response, a good response written by a human.</p>
    <p class="cue"><span class="time">[95:18]</span>Deep learning is a type of machine learning</p>
    <p class="cue"><span class="time">[95:20]</span>that uses neural.</p>
    <p class="cue"><span class="time">[95:23]</span>And then I expect the model to come up with the word networks.</p>
    <p class="cue"><span class="time">[95:27]</span>So it&#x27;s literally do whatever we need</p>
    <p class="cue"><span class="time">[95:30]</span>to train the pre-trained model, but we do it on human written</p>
    <p class="cue"><span class="time">[95:34]</span>prompt response pairs.</p>
    <p class="cue"><span class="time">[95:38]</span>And if you do that many times, then you</p>
    <p class="cue"><span class="time">[95:40]</span>use the same loss function, how far the model&#x27;s response</p>
    <p class="cue"><span class="time">[95:44]</span>is from a human response.</p>
    <p class="cue"><span class="time">[95:46]</span>You do that many times, and you will get</p>
    <p class="cue"><span class="time">[95:49]</span>SFT, supervised fine-tuning.</p>
    <p class="cue"><span class="time">[95:53]</span>But it has some shortcomings.</p>
    <p class="cue"><span class="time">[95:57]</span>One of the shortcomings it is data that</p>
    <p class="cue"><span class="time">[96:00]</span>is extremely costly to collect.</p>
    <p class="cue"><span class="time">[96:03]</span>In fact, I believe in the first version of that InstructGPT.</p>
    <p class="cue"><span class="time">[96:07]</span>There was only 13,000 prompt response pairs.</p>
    <p class="cue"><span class="time">[96:11]</span>And it turns out it did really well, despite that.</p>
    <p class="cue"><span class="time">[96:18]</span>The second aspect is it&#x27;s unlikely to generalize well</p>
    <p class="cue"><span class="time">[96:21]</span>because you&#x27;re-- again, you&#x27;re not doing reinforcement learning</p>
    <p class="cue"><span class="time">[96:25]</span>here.</p>
    <p class="cue"><span class="time">[96:25]</span>You&#x27;re doing supervised learning.</p>
    <p class="cue"><span class="time">[96:28]</span>And so you&#x27;re just showing a set of examples,</p>
    <p class="cue"><span class="time">[96:31]</span>13,000 examples that you want to learn,</p>
    <p class="cue"><span class="time">[96:33]</span>but it&#x27;s what tells you that it will generalize to an unseen</p>
    <p class="cue"><span class="time">[96:37]</span>prompt that will come up from your user base.</p>
    <p class="cue"><span class="time">[96:41]</span>And so this approach, SFT, really</p>
    <p class="cue"><span class="time">[96:44]</span>teaches the model to imitate good behavior from humans.</p>
    <p class="cue"><span class="time">[96:48]</span>And that&#x27;s the key.</p>
    <p class="cue"><span class="time">[96:49]</span>It&#x27;s imitation.</p>
    <p class="cue"><span class="time">[96:51]</span>It is not preference optimization.</p>
    <p class="cue"><span class="time">[96:55]</span>To do preference optimization, that&#x27;s</p>
    <p class="cue"><span class="time">[96:58]</span>where we&#x27;re going to train a reward model.</p>
    <p class="cue"><span class="time">[97:00]</span>And we&#x27;re going to do proper RLHF.</p>
    <p class="cue"><span class="time">[97:02]</span>So let me talk to you about the RM, reward model.</p>
    <p class="cue"><span class="time">[97:06]</span>And then I&#x27;ll tell you about RLHF in a nutshell.</p>
    <p class="cue"><span class="time">[97:11]</span>The problem of RLHF is to align not with human responses,</p>
    <p class="cue"><span class="time">[97:17]</span>but with human preferences.</p>
    <p class="cue"><span class="time">[97:19]</span>So what&#x27;s going to happen is we&#x27;re</p>
    <p class="cue"><span class="time">[97:21]</span>going to train a separate model to predict</p>
    <p class="cue"><span class="time">[97:26]</span>which responses humans prefer.</p>
    <p class="cue"><span class="time">[97:28]</span>And we&#x27;re going to call that model the reward model.</p>
    <p class="cue"><span class="time">[97:30]</span>It&#x27;s a separate model from whatever we&#x27;ve trained before.</p>
    <p class="cue"><span class="time">[97:33]</span>The model is going to use data from labelers.</p>
    <p class="cue"><span class="time">[97:37]</span>So you&#x27;re going to show labelers two or more responses</p>
    <p class="cue"><span class="time">[97:41]</span>to the same prompts.</p>
    <p class="cue"><span class="time">[97:44]</span>And those responses will be sampled from the SFT.</p>
    <p class="cue"><span class="time">[97:48]</span>So your best model right now is the SFT.</p>
    <p class="cue"><span class="time">[97:51]</span>You will sample three or four responses.</p>
    <p class="cue"><span class="time">[97:53]</span>And you know how we sample, right?</p>
    <p class="cue"><span class="time">[97:55]</span>You can tweak the temperature.</p>
    <p class="cue"><span class="time">[97:57]</span>You can select not only the top priority word, the top--</p>
    <p class="cue"><span class="time">[98:01]</span>the softmax layers, number one word, but you</p>
    <p class="cue"><span class="time">[98:03]</span>can sometimes sample differently and you</p>
    <p class="cue"><span class="time">[98:06]</span>will get a variety of answers.</p>
    <p class="cue"><span class="time">[98:08]</span>And then you will ask a human labeler</p>
    <p class="cue"><span class="time">[98:10]</span>to say answer B is better than answer C, and answer</p>
    <p class="cue"><span class="time">[98:13]</span>C is better than answer A, and answer A is equal to answer D,</p>
    <p class="cue"><span class="time">[98:18]</span>let&#x27;s say.</p>
    <p class="cue"><span class="time">[98:22]</span>They will be asked which response they prefer</p>
    <p class="cue"><span class="time">[98:24]</span>and it can get more complicated.</p>
    <p class="cue"><span class="time">[98:27]</span>It doesn&#x27;t have to be just a simple ranking.</p>
    <p class="cue"><span class="time">[98:28]</span>You have multiple Likert scale methods and so on.</p>
    <p class="cue"><span class="time">[98:32]</span>But the point is that you will collect</p>
    <p class="cue"><span class="time">[98:34]</span>those pairwise comparisons that we&#x27;ll call preference data,</p>
    <p class="cue"><span class="time">[98:37]</span>and you will use it to train a reward model, which</p>
    <p class="cue"><span class="time">[98:40]</span>is initialized from your SFT.</p>
    <p class="cue"><span class="time">[98:42]</span>So your SFT is here, it&#x27;s your best model to date.</p>
    <p class="cue"><span class="time">[98:46]</span>And you&#x27;re going to modify the last layer.</p>
    <p class="cue"><span class="time">[98:48]</span>So the softmax layer at the end of a language model, that</p>
    <p class="cue"><span class="time">[98:52]</span>will tell you this is the token we should output,</p>
    <p class="cue"><span class="time">[98:55]</span>or this is the word we should write.</p>
    <p class="cue"><span class="time">[98:57]</span>Instead of that, you&#x27;ll get rid of that layer.</p>
    <p class="cue"><span class="time">[98:59]</span>You&#x27;ll put a scalar value as output.</p>
    <p class="cue"><span class="time">[99:02]</span>You&#x27;ll put a linear layer with a scalar value that</p>
    <p class="cue"><span class="time">[99:04]</span>will represent the reward head.</p>
    <p class="cue"><span class="time">[99:06]</span>It will predict the reward, which</p>
    <p class="cue"><span class="time">[99:10]</span>is a proxy for the preference of the human.</p>
    <p class="cue"><span class="time">[99:12]</span>The way you train that reward model is you</p>
    <p class="cue"><span class="time">[99:15]</span>give it a batch of two.</p>
    <p class="cue"><span class="time">[99:18]</span>You give it a prompt x with a response</p>
    <p class="cue"><span class="time">[99:21]</span>A and the preference of the user.</p>
    <p class="cue"><span class="time">[99:23]</span>And you give it the same prompt with a response</p>
    <p class="cue"><span class="time">[99:26]</span>B from the SFT with the preference of the user.</p>
    <p class="cue"><span class="time">[99:29]</span>So here the user is saying, response A</p>
    <p class="cue"><span class="time">[99:31]</span>is better than response B. And so if you actually</p>
    <p class="cue"><span class="time">[99:34]</span>were sending that in this model, you will get a predicted reward</p>
    <p class="cue"><span class="time">[99:39]</span>for the preferred answer and a predicted reward for the least</p>
    <p class="cue"><span class="time">[99:42]</span>preferred answer.</p>
    <p class="cue"><span class="time">[99:45]</span>That allows you to train using a loss function</p>
    <p class="cue"><span class="time">[99:52]</span>that I&#x27;m not going to cover given our time sensitivity.</p>
    <p class="cue"><span class="time">[99:55]</span>The loss function will encourage the model</p>
    <p class="cue"><span class="time">[99:57]</span>to assign higher rewards to preferred responses.</p>
    <p class="cue"><span class="time">[100:01]</span>So you&#x27;re trying to dissociate the higher</p>
    <p class="cue"><span class="time">[100:05]</span>reward, better preference from the lower</p>
    <p class="cue"><span class="time">[100:07]</span>reward for lower preference.</p>
    <p class="cue"><span class="time">[100:08]</span>And it turns out that if you do that many times,</p>
    <p class="cue"><span class="time">[100:10]</span>you will have a reward model that,</p>
    <p class="cue"><span class="time">[100:13]</span>given a prompt and a response, will be approximating</p>
    <p class="cue"><span class="time">[100:17]</span>human preference.</p>
    <p class="cue"><span class="time">[100:19]</span>So you&#x27;ve just trained a critique</p>
    <p class="cue"><span class="time">[100:21]</span>that represents your humans.</p>
    <p class="cue"><span class="time">[100:23]</span>It&#x27;s a proxy for what humans prefer.</p>
    <p class="cue"><span class="time">[100:25]</span>It&#x27;s been trained on a lot of human preferences.</p>
    <p class="cue"><span class="time">[100:29]</span>The reason it&#x27;s better to use a model than actual humans</p>
    <p class="cue"><span class="time">[100:31]</span>is because we can use it widely on all sorts of inputs,</p>
    <p class="cue"><span class="time">[100:35]</span>and it can scale from a data standpoint.</p>
    <p class="cue"><span class="time">[100:38]</span>Also note that this method is better than SFT</p>
    <p class="cue"><span class="time">[100:41]</span>because it&#x27;s way easier to ask humans what&#x27;s</p>
    <p class="cue"><span class="time">[100:43]</span>your preference between those two things,</p>
    <p class="cue"><span class="time">[100:45]</span>and to ask them to come up with answers to prompts.</p>
    <p class="cue"><span class="time">[100:47]</span>It takes way less time.</p>
    <p class="cue"><span class="time">[100:49]</span>And if you&#x27;ve used ChatGPT, you&#x27;ve</p>
    <p class="cue"><span class="time">[100:51]</span>probably been asked before to tell them</p>
    <p class="cue"><span class="time">[100:53]</span>which response you prefer.</p>
    <p class="cue"><span class="time">[100:57]</span>Yeah.</p>
    <p class="cue"><span class="time">[100:58]</span>So once trained, the reward model</p>
    <p class="cue"><span class="time">[100:59]</span>replaces the human as the evaluator</p>
    <p class="cue"><span class="time">[101:02]</span>during reinforcement learning from human feedback.</p>
    <p class="cue"><span class="time">[101:05]</span>And reinforcement learning from human feedback</p>
    <p class="cue"><span class="time">[101:08]</span>is very comfortable for you now.</p>
    <p class="cue"><span class="time">[101:10]</span>I will show you what it looks like given</p>
    <p class="cue"><span class="time">[101:12]</span>the Q-learning algorithm we learned.</p>
    <p class="cue"><span class="time">[101:14]</span>But essentially, we have first start</p>
    <p class="cue"><span class="time">[101:16]</span>the model, what good behavior looks like with SFT.</p>
    <p class="cue"><span class="time">[101:19]</span>And then we built a reward model that</p>
    <p class="cue"><span class="time">[101:21]</span>can tell us how good an answer is</p>
    <p class="cue"><span class="time">[101:23]</span>according to human preferences.</p>
    <p class="cue"><span class="time">[101:25]</span>And the RLHF approach is where we will let this model practice,</p>
    <p class="cue"><span class="time">[101:30]</span>get scored by the reward model or the critic,</p>
    <p class="cue"><span class="time">[101:33]</span>and update itself to produce higher scoring answers.</p>
    <p class="cue"><span class="time">[101:37]</span>So more preferred answers.</p>
    <p class="cue"><span class="time">[101:39]</span>And it&#x27;s the same as the games we&#x27;ve seen together,</p>
    <p class="cue"><span class="time">[101:43]</span>but some things differ.</p>
    <p class="cue"><span class="time">[101:45]</span>So I just pasted here the exact setup</p>
    <p class="cue"><span class="time">[101:48]</span>that we&#x27;ve learned together for reinforcement learning.</p>
    <p class="cue"><span class="time">[101:52]</span>The differences are the following.</p>
    <p class="cue"><span class="time">[101:54]</span>Our objective is still to maximize the expected reward</p>
    <p class="cue"><span class="time">[101:58]</span>that is produced by the reward model aligned</p>
    <p class="cue"><span class="time">[102:01]</span>with human preferences.</p>
    <p class="cue"><span class="time">[102:05]</span>The agent is the language model being fine-tuned.</p>
    <p class="cue"><span class="time">[102:10]</span>The environment is the space of possible prompts</p>
    <p class="cue"><span class="time">[102:13]</span>and continuations.</p>
    <p class="cue"><span class="time">[102:15]</span>It&#x27;s any text that you can encounter.</p>
    <p class="cue"><span class="time">[102:19]</span>The state is the specific prompt plus the tokens</p>
    <p class="cue"><span class="time">[102:23]</span>that were generated so far.</p>
    <p class="cue"><span class="time">[102:25]</span>The next state is one more token added.</p>
    <p class="cue"><span class="time">[102:29]</span>And the action is the next token that</p>
    <p class="cue"><span class="time">[102:31]</span>is chosen by the agent or the model, which is,</p>
    <p class="cue"><span class="time">[102:34]</span>of course, determined by the policy.</p>
    <p class="cue"><span class="time">[102:38]</span>And then the reward is estimated by the reward model</p>
    <p class="cue"><span class="time">[102:43]</span>that we trained to represent human preferences.</p>
    <p class="cue"><span class="time">[102:50]</span>In this case, one episode is one full prompt.</p>
    <p class="cue"><span class="time">[102:55]</span>So imagine that you get a prompt and you start generating.</p>
    <p class="cue"><span class="time">[102:58]</span>And you go through this reinforcement learning loop.</p>
    <p class="cue"><span class="time">[103:00]</span>And you observe the rewards.</p>
    <p class="cue"><span class="time">[103:01]</span>And then you try to maximize the future rewards.</p>
    <p class="cue"><span class="time">[103:04]</span>And then at the end of training, you end up</p>
    <p class="cue"><span class="time">[103:06]</span>with having your pre-trained model turn into an SFT</p>
    <p class="cue"><span class="time">[103:09]</span>and your SFT turn into a way better model using RLHF.</p>
    <p class="cue"><span class="time">[103:18]</span>OK.</p>
    <p class="cue"><span class="time">[103:19]</span>So a few things to note to end on this.</p>
    <p class="cue"><span class="time">[103:22]</span>The model does not get a reward at every single token.</p>
    <p class="cue"><span class="time">[103:27]</span>It gets a reward at the end of a sequence</p>
    <p class="cue"><span class="time">[103:29]</span>when the completion is finished.</p>
    <p class="cue"><span class="time">[103:31]</span>Because reward model was asked to rate</p>
    <p class="cue"><span class="time">[103:35]</span>prompts and responses together.</p>
    <p class="cue"><span class="time">[103:37]</span>So you need to finish the generation in order</p>
    <p class="cue"><span class="time">[103:39]</span>to see what&#x27;s the reward.</p>
    <p class="cue"><span class="time">[103:41]</span>And so again, going back to making</p>
    <p class="cue"><span class="time">[103:43]</span>good sequences of decisions, that&#x27;s exactly it.</p>
    <p class="cue"><span class="time">[103:45]</span>You want the model to make enough good sequences</p>
    <p class="cue"><span class="time">[103:47]</span>of decisions so that the response is preferred</p>
    <p class="cue"><span class="time">[103:51]</span>by the critic, which represents a proxy</p>
    <p class="cue"><span class="time">[103:53]</span>to the human preferences.</p>
    <p class="cue"><span class="time">[103:56]</span>So all intermediary rewards are typically 0.</p>
    <p class="cue"><span class="time">[103:59]</span>And that makes it a very sparse reward episodic tasks,</p>
    <p class="cue"><span class="time">[104:05]</span>just like a game of chess where you only</p>
    <p class="cue"><span class="time">[104:07]</span>get a reward when you finish, assuming you&#x27;re not</p>
    <p class="cue"><span class="time">[104:09]</span>defining intermediary reward.</p>
    <p class="cue"><span class="time">[104:11]</span>So you only know if you did well at the end.</p>
    <p class="cue"><span class="time">[104:14]</span>And you have to then use that information</p>
    <p class="cue"><span class="time">[104:16]</span>to update your network and get a better proxy for it.</p>
    <p class="cue"><span class="time">[104:20]</span>Super.</p>
    <p class="cue"><span class="time">[104:21]</span>There&#x27;s a very nice video.</p>
    <p class="cue"><span class="time">[104:22]</span>We&#x27;re not going to play it for the sake of time,</p>
    <p class="cue"><span class="time">[104:24]</span>but I will send it online.</p>
    <p class="cue"><span class="time">[104:25]</span>It&#x27;s from four days ago.</p>
    <p class="cue"><span class="time">[104:29]</span>A former Stanford student, [INAUDIBLE],</p>
    <p class="cue"><span class="time">[104:33]</span>who is very thoughtful and articulate</p>
    <p class="cue"><span class="time">[104:35]</span>and was explaining four days ago, why reinforcement learning</p>
    <p class="cue"><span class="time">[104:39]</span>can be terrible at times, and that human minds work way more</p>
    <p class="cue"><span class="time">[104:44]</span>efficiently.</p>
    <p class="cue"><span class="time">[104:45]</span>And so I would encourage you to watch this four-minute video</p>
    <p class="cue"><span class="time">[104:48]</span>because he&#x27;s very clearly outlining why reinforcement</p>
    <p class="cue"><span class="time">[104:50]</span>learning is still not great, even if it&#x27;s the best</p>
    <p class="cue"><span class="time">[104:54]</span>thing we can use in many ways.</p>
  </section>
</article>
</body>
</html>
