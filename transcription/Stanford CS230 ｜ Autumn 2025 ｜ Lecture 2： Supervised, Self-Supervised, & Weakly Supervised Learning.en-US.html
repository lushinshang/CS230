<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Stanford CS230 ｜ Autumn 2025 ｜ Lecture 2： Supervised, Self-Supervised, &amp; Weakly Supervised Learning.en-US.html</title>
  <style>
    body{font-family:Inter, system-ui, -apple-system, Arial, sans-serif;line-height:1.6;padding:1rem;max-width:900px;margin:0 auto;color:#111}
    .meta{color:#666;font-size:0.95rem;margin-bottom:0.5rem}
    article{background:#fff;border-radius:8px;padding:1rem 1.2rem;box-shadow:0 6px 18px rgba(10,20,30,0.05)}
    p.cue{margin:0 0 0.6rem}
    span.time{color:#666;margin-right:8px;font-size:0.95rem}
  </style>
</head>
<body>
<article lang="en">
  <header>
    <h1>Stanford CS230 ｜ Autumn 2025 ｜ Lecture 2： Supervised, Self-Supervised, &amp; Weakly Supervised Learning</h1>
    <p class="meta">Source: Stanford CS230 ｜ Autumn 2025 ｜ Lecture 2： Supervised, Self-Supervised, & Weakly Supervised Learning.en-US.srt</p>
  </header>
  <section class="transcript">
    <p class="cue"><span class="time">[00:05]</span>I&#x27;m Kian Katanforoosh and I am the co-creator and collector</p>
    <p class="cue"><span class="time">[00:10]</span>with Andrew for this class, CS230.</p>
    <p class="cue"><span class="time">[00:15]</span>And I will teach about half of the in-person lectures</p>
    <p class="cue"><span class="time">[00:19]</span>this quarter.</p>
    <p class="cue"><span class="time">[00:21]</span>Outside of Stanford, I work in industry.</p>
    <p class="cue"><span class="time">[00:24]</span>I lead a company called Workera which uses AI to measure skills.</p>
    <p class="cue"><span class="time">[00:28]</span>And with the history of CS230 students that</p>
    <p class="cue"><span class="time">[00:32]</span>have started AI startups and companies, what I try to do,</p>
    <p class="cue"><span class="time">[00:35]</span>usually, is to bring a lot of examples from industry.</p>
    <p class="cue"><span class="time">[00:38]</span>So what you should expect from these in-class lectures</p>
    <p class="cue"><span class="time">[00:42]</span>is not as much of the academic side of things, which</p>
    <p class="cue"><span class="time">[00:45]</span>we learn anyway in the online videos,</p>
    <p class="cue"><span class="time">[00:48]</span>but also the industry specific input.</p>
    <p class="cue"><span class="time">[00:53]</span>And some of the topics that we&#x27;ll cover this year together</p>
    <p class="cue"><span class="time">[00:58]</span>includes decision-making in AI projects,</p>
    <p class="cue"><span class="time">[01:02]</span>which we&#x27;re going to see today.</p>
    <p class="cue"><span class="time">[01:04]</span>I want you to come out of today&#x27;s lecture feeling</p>
    <p class="cue"><span class="time">[01:07]</span>like you had some fun, it was interactive.</p>
    <p class="cue"><span class="time">[01:10]</span>And also you have a better way to make decisions in AI projects</p>
    <p class="cue"><span class="time">[01:14]</span>because you&#x27;ve seen how deep-learning researchers,</p>
    <p class="cue"><span class="time">[01:18]</span>engineers, and scientists make their decisions solving problems</p>
    <p class="cue"><span class="time">[01:22]</span>in industry.</p>
    <p class="cue"><span class="time">[01:23]</span>Other topics later in the quarter for in classroom time</p>
    <p class="cue"><span class="time">[01:27]</span>include things like adversarial attacks and defenses.</p>
    <p class="cue"><span class="time">[01:32]</span>We might have some time to cover it today.</p>
    <p class="cue"><span class="time">[01:34]</span>Deep reinforcement learning, which</p>
    <p class="cue"><span class="time">[01:36]</span>is really hot in the market right now,</p>
    <p class="cue"><span class="time">[01:38]</span>and I think it&#x27;s very important to about it.</p>
    <p class="cue"><span class="time">[01:40]</span>And then all the stuff that is very practical,</p>
    <p class="cue"><span class="time">[01:43]</span>like retrieval augmented generation,</p>
    <p class="cue"><span class="time">[01:46]</span>AI agents, multi-agent system.</p>
    <p class="cue"><span class="time">[01:49]</span>As we go deeper into the class, and you</p>
    <p class="cue"><span class="time">[01:52]</span>get the baggage of neural networks,</p>
    <p class="cue"><span class="time">[01:54]</span>we&#x27;ll be able to cover even more fun topics.</p>
    <p class="cue"><span class="time">[01:58]</span>So today&#x27;s lecture is going to be structured</p>
    <p class="cue"><span class="time">[02:02]</span>in three parts, maybe four depending</p>
    <p class="cue"><span class="time">[02:06]</span>on whether we have time.</p>
    <p class="cue"><span class="time">[02:07]</span>We&#x27;ll start with a little recap of the week.</p>
    <p class="cue"><span class="time">[02:11]</span>What you&#x27;ve learned online about neurons and layers</p>
    <p class="cue"><span class="time">[02:14]</span>and deep neural networks.</p>
    <p class="cue"><span class="time">[02:15]</span>Then we&#x27;ll get into a set of supervised learning projects,</p>
    <p class="cue"><span class="time">[02:20]</span>including a day and night simple vanilla classification,</p>
    <p class="cue"><span class="time">[02:25]</span>the trigger word detection, which is actually</p>
    <p class="cue"><span class="time">[02:27]</span>a project you&#x27;re going to build at the end of the class</p>
    <p class="cue"><span class="time">[02:30]</span>yourself.</p>
    <p class="cue"><span class="time">[02:31]</span>And then face verification, which</p>
    <p class="cue"><span class="time">[02:33]</span>we&#x27;ll see also variation of how face verification</p>
    <p class="cue"><span class="time">[02:36]</span>algorithms work.</p>
    <p class="cue"><span class="time">[02:38]</span>In the third section, we&#x27;ll focus</p>
    <p class="cue"><span class="time">[02:40]</span>on self-supervised learning and weakly supervised learning.</p>
    <p class="cue"><span class="time">[02:43]</span>Don&#x27;t worry if you don&#x27;t these terms.</p>
    <p class="cue"><span class="time">[02:45]</span>We&#x27;re going to learn them together.</p>
    <p class="cue"><span class="time">[02:46]</span>And we&#x27;ll talk a lot about embeddings</p>
    <p class="cue"><span class="time">[02:49]</span>because embeddings are the connective tissue of many AI</p>
    <p class="cue"><span class="time">[02:54]</span>systems online today, and it&#x27;s important to know about them.</p>
    <p class="cue"><span class="time">[02:58]</span>And finally, if we have time, we&#x27;ll</p>
    <p class="cue"><span class="time">[02:59]</span>also talk about adversarial attacks and defenses.</p>
    <p class="cue"><span class="time">[03:03]</span>With more and more AI systems in the wild,</p>
    <p class="cue"><span class="time">[03:06]</span>knowing how to defend them is very important,</p>
    <p class="cue"><span class="time">[03:09]</span>and knowing how to attack them can also</p>
    <p class="cue"><span class="time">[03:11]</span>teach you how to defend them.</p>
    <p class="cue"><span class="time">[03:12]</span>So we&#x27;ll cover that as well.</p>
    <p class="cue"><span class="time">[03:14]</span>Sounds good?</p>
    <p class="cue"><span class="time">[03:16]</span>Please interrupt me as we go through the lecture.</p>
    <p class="cue"><span class="time">[03:19]</span>We want this to be very conversational as much as</p>
    <p class="cue"><span class="time">[03:23]</span>possible.</p>
    <p class="cue"><span class="time">[03:24]</span>So recap of the week is the core way</p>
    <p class="cue"><span class="time">[03:30]</span>that AI learns from data in a traditional supervised learning</p>
    <p class="cue"><span class="time">[03:35]</span>setup.</p>
    <p class="cue"><span class="time">[03:36]</span>You can think of it as an input, such as this little image</p>
    <p class="cue"><span class="time">[03:40]</span>of the confused cat and an output,</p>
    <p class="cue"><span class="time">[03:44]</span>in this case, a number between 0 and 1</p>
    <p class="cue"><span class="time">[03:46]</span>that represents the chance that there</p>
    <p class="cue"><span class="time">[03:48]</span>might be a cat on the picture 1 or there</p>
    <p class="cue"><span class="time">[03:51]</span>is no cat on the picture 0.</p>
    <p class="cue"><span class="time">[03:54]</span>What the model is, and oftentimes you&#x27;ll see</p>
    <p class="cue"><span class="time">[03:57]</span>me refer to the model as two things.</p>
    <p class="cue"><span class="time">[03:59]</span>There&#x27;s an architecture which is essentially</p>
    <p class="cue"><span class="time">[04:01]</span>the blueprint of the model, the skeleton and parameters.</p>
    <p class="cue"><span class="time">[04:05]</span>It might be a few parameters, it might</p>
    <p class="cue"><span class="time">[04:07]</span>be billions of parameters like the models</p>
    <p class="cue"><span class="time">[04:10]</span>that OpenAI, DeepMind and others work on.</p>
    <p class="cue"><span class="time">[04:16]</span>Outside of that--</p>
    <p class="cue"><span class="time">[04:18]</span>And so when you think about AI models being deployed</p>
    <p class="cue"><span class="time">[04:21]</span>in the wild, when you think about what&#x27;s</p>
    <p class="cue"><span class="time">[04:23]</span>happening with ChatGPT, you can really</p>
    <p class="cue"><span class="time">[04:27]</span>come down to there&#x27;s two files somewhere on the cloud, one that</p>
    <p class="cue"><span class="time">[04:31]</span>describes the architecture of the model, one</p>
    <p class="cue"><span class="time">[04:33]</span>that describes the parameters that</p>
    <p class="cue"><span class="time">[04:34]</span>are part of this architecture.</p>
    <p class="cue"><span class="time">[04:36]</span>And you keep calling to those two files,</p>
    <p class="cue"><span class="time">[04:38]</span>and you get your inference or your output.</p>
    <p class="cue"><span class="time">[04:41]</span>That&#x27;s really what&#x27;s happening behind the scenes.</p>
    <p class="cue"><span class="time">[04:43]</span>Much more complicated than that, obviously.</p>
    <p class="cue"><span class="time">[04:45]</span>But those are the two critical components of a neural network</p>
    <p class="cue"><span class="time">[04:48]</span>architecture, and its parameters that are trained.</p>
    <p class="cue"><span class="time">[04:54]</span>How does the model learn?</p>
    <p class="cue"><span class="time">[04:55]</span>Is through a gradient descent optimization.</p>
    <p class="cue"><span class="time">[04:59]</span>Meaning I send the picture of the cat through the model,</p>
    <p class="cue"><span class="time">[05:03]</span>and the model, at the beginning, is not trained,</p>
    <p class="cue"><span class="time">[05:06]</span>so it&#x27;s probably wrong.</p>
    <p class="cue"><span class="time">[05:08]</span>It tells me I think there is no cat, I think it&#x27;s 0.</p>
    <p class="cue"><span class="time">[05:11]</span>And then I use something called the loss function</p>
    <p class="cue"><span class="time">[05:13]</span>to compare the ground truth.</p>
    <p class="cue"><span class="time">[05:15]</span>There is a cat on the picture with the prediction</p>
    <p class="cue"><span class="time">[05:18]</span>from the model at this point in time.</p>
    <p class="cue"><span class="time">[05:21]</span>Those two numbers are far from each other.</p>
    <p class="cue"><span class="time">[05:24]</span>That should be a penalty, which the loss function describes.</p>
    <p class="cue"><span class="time">[05:28]</span>And then, in order to give feedback to the parameters,</p>
    <p class="cue"><span class="time">[05:31]</span>we use this gradient descent update.</p>
    <p class="cue"><span class="time">[05:34]</span>We do that many, many times.</p>
    <p class="cue"><span class="time">[05:36]</span>What it means is that we take our parameters and we tell them,</p>
    <p class="cue"><span class="time">[05:39]</span>hey, you should go a little bit more</p>
    <p class="cue"><span class="time">[05:41]</span>to the right or a little bit more</p>
    <p class="cue"><span class="time">[05:42]</span>to the left until that number, that</p>
    <p class="cue"><span class="time">[05:46]</span>is the prediction for the cat, is closer to the ground truth.</p>
    <p class="cue"><span class="time">[05:49]</span>We do that with batches of data, millions</p>
    <p class="cue"><span class="time">[05:52]</span>of images of cats and images of anything</p>
    <p class="cue"><span class="time">[05:55]</span>else, and we give that feedback repetitively to the model</p>
    <p class="cue"><span class="time">[06:00]</span>until the parameters are calibrated</p>
    <p class="cue"><span class="time">[06:03]</span>and the model is, in fact, finding the cat on this picture.</p>
    <p class="cue"><span class="time">[06:06]</span>Nothing new here.</p>
    <p class="cue"><span class="time">[06:08]</span>You&#x27;ve seen it in the videos.</p>
    <p class="cue"><span class="time">[06:09]</span>Any question on that learning setup?</p>
    <p class="cue"><span class="time">[06:12]</span>No.</p>
    <p class="cue"><span class="time">[06:13]</span>OK, easy so far.</p>
    <p class="cue"><span class="time">[06:15]</span>There&#x27;s many things that can change in this setup,</p>
    <p class="cue"><span class="time">[06:17]</span>and you&#x27;ll see in the class.</p>
    <p class="cue"><span class="time">[06:18]</span>First thing is the input.</p>
    <p class="cue"><span class="time">[06:20]</span>The input does not have to be an image.</p>
    <p class="cue"><span class="time">[06:22]</span>It can be text like when you chat with ChatGPT.</p>
    <p class="cue"><span class="time">[06:26]</span>It can be audio, it can be video,</p>
    <p class="cue"><span class="time">[06:28]</span>it can be structured data, it can be spreadsheets and numbers.</p>
    <p class="cue"><span class="time">[06:32]</span>Those we&#x27;ll see a variety of examples in the class</p>
    <p class="cue"><span class="time">[06:34]</span>and how it influences the architecture.</p>
    <p class="cue"><span class="time">[06:38]</span>The output again doesn&#x27;t have to be 0 and 1.</p>
    <p class="cue"><span class="time">[06:41]</span>This is an example of a classification.</p>
    <p class="cue"><span class="time">[06:43]</span>You could turn this problem into a regression.</p>
    <p class="cue"><span class="time">[06:46]</span>For example, if I was asking you, what&#x27;s the age of the cat,</p>
    <p class="cue"><span class="time">[06:50]</span>estimate the age of the cat.</p>
    <p class="cue"><span class="time">[06:51]</span>That would be a regression task, not a classification task</p>
    <p class="cue"><span class="time">[06:54]</span>anymore.</p>
    <p class="cue"><span class="time">[06:55]</span>Later in the class, we&#x27;ll also see generative tasks.</p>
    <p class="cue"><span class="time">[06:58]</span>In fact, lecture 4 is going to focus on diffusion models</p>
    <p class="cue"><span class="time">[07:01]</span>generative adversarial networks where the output, actually,</p>
    <p class="cue"><span class="time">[07:05]</span>is much bigger than the input, typically.</p>
    <p class="cue"><span class="time">[07:08]</span>So you can have a low resolution of a cat as input and the output</p>
    <p class="cue"><span class="time">[07:12]</span>is a high-resolution of the same cat.</p>
    <p class="cue"><span class="time">[07:14]</span>The output is bigger than the input, which can</p>
    <p class="cue"><span class="time">[07:16]</span>be counterintuitive to people.</p>
    <p class="cue"><span class="time">[07:19]</span>Other things that can change include the architecture.</p>
    <p class="cue"><span class="time">[07:22]</span>You&#x27;ve learned about the vanilla multilayer perceptron</p>
    <p class="cue"><span class="time">[07:27]</span>or the fully connected neural network.</p>
    <p class="cue"><span class="time">[07:28]</span>That&#x27;s what we&#x27;re learning right now online together.</p>
    <p class="cue"><span class="time">[07:31]</span>By the end of the class, you&#x27;ll have many architectures</p>
    <p class="cue"><span class="time">[07:35]</span>that you&#x27;ll be familiar with from RNNs</p>
    <p class="cue"><span class="time">[07:37]</span>and convolutional neural networks, transformer models.</p>
    <p class="cue"><span class="time">[07:42]</span>All of these, at the, end of the day,</p>
    <p class="cue"><span class="time">[07:44]</span>use the basis neural network that you&#x27;re learning right now.</p>
    <p class="cue"><span class="time">[07:48]</span>They&#x27;re just stacked on top of each other differently.</p>
    <p class="cue"><span class="time">[07:52]</span>The loss function is actually a big focus of today&#x27;s class</p>
    <p class="cue"><span class="time">[07:55]</span>and of the class in general.</p>
    <p class="cue"><span class="time">[07:58]</span>The loss function, which is what gives the feedback to the model,</p>
    <p class="cue"><span class="time">[08:02]</span>you were right or you were wrong, and what to do about it</p>
    <p class="cue"><span class="time">[08:06]</span>is an art.</p>
    <p class="cue"><span class="time">[08:07]</span>Designing good loss functions.</p>
    <p class="cue"><span class="time">[08:09]</span>Great deep learning researchers are</p>
    <p class="cue"><span class="time">[08:12]</span>very creative when it comes to designing loss functions.</p>
    <p class="cue"><span class="time">[08:15]</span>And in fact, when we built the algorithm called YOLO,</p>
    <p class="cue"><span class="time">[08:19]</span>it is called YOLO for you only look once,</p>
    <p class="cue"><span class="time">[08:23]</span>not you only live once.</p>
    <p class="cue"><span class="time">[08:24]</span>But YOLO has a very, difficult to understand at first loss</p>
    <p class="cue"><span class="time">[08:29]</span>function.</p>
    <p class="cue"><span class="time">[08:30]</span>And there&#x27;s a reason why the loss function</p>
    <p class="cue"><span class="time">[08:32]</span>was designed like that.</p>
    <p class="cue"><span class="time">[08:33]</span>So by the end of this class, you&#x27;ll</p>
    <p class="cue"><span class="time">[08:34]</span>also have a better intuition on how do</p>
    <p class="cue"><span class="time">[08:36]</span>we design great loss functions.</p>
    <p class="cue"><span class="time">[08:40]</span>Other things I&#x27;m not going to cover right now,</p>
    <p class="cue"><span class="time">[08:42]</span>the activation functions in your neural network, the optimizer</p>
    <p class="cue"><span class="time">[08:45]</span>that you use for your gradient descent loop, and then the</p>
    <p class="cue"><span class="time">[08:48]</span>hyperparameters that might come in when</p>
    <p class="cue"><span class="time">[08:50]</span>you train your algorithms.</p>
    <p class="cue"><span class="time">[08:52]</span>OK nothing new here.</p>
    <p class="cue"><span class="time">[08:54]</span>This is the basic setup.</p>
    <p class="cue"><span class="time">[08:56]</span>You&#x27;ve also learned this week about neurons.</p>
    <p class="cue"><span class="time">[08:59]</span>The easiest way to think about a neuron</p>
    <p class="cue"><span class="time">[09:01]</span>is the classic logistic regression algorithm.</p>
    <p class="cue"><span class="time">[09:05]</span>Where I&#x27;m taking the image of the cat.</p>
    <p class="cue"><span class="time">[09:07]</span>So an image in computer science with the way the machine reads</p>
    <p class="cue"><span class="time">[09:11]</span>it is three channels.</p>
    <p class="cue"><span class="time">[09:13]</span>RGB for the three colors red, green, blue.</p>
    <p class="cue"><span class="time">[09:16]</span>And we take all these numbers.</p>
    <p class="cue"><span class="time">[09:18]</span>We put them in a vector.</p>
    <p class="cue"><span class="time">[09:20]</span>The vector is then fed into a neuron.</p>
    <p class="cue"><span class="time">[09:22]</span>And the neuron has two components the linear part</p>
    <p class="cue"><span class="time">[09:25]</span>W transpose X plus B. W being the weights</p>
    <p class="cue"><span class="time">[09:28]</span>and B being the bias.</p>
    <p class="cue"><span class="time">[09:29]</span>And then an activation function, in this case,</p>
    <p class="cue"><span class="time">[09:32]</span>the sigmoid function, which is very handy because it takes</p>
    <p class="cue"><span class="time">[09:36]</span>any number, and it puts it between 0 and 1</p>
    <p class="cue"><span class="time">[09:38]</span>so that the output can look like a probability.</p>
    <p class="cue"><span class="time">[09:42]</span>Classic setup.</p>
    <p class="cue"><span class="time">[09:43]</span>And here the probability is 0.73 which is above 0.5.</p>
    <p class="cue"><span class="time">[09:49]</span>Which tells me the model thinks there&#x27;s a cat on the picture</p>
    <p class="cue"><span class="time">[09:51]</span>because 1 is a cat 0 is no cat.</p>
    <p class="cue"><span class="time">[09:55]</span>So question for you to get started.</p>
    <p class="cue"><span class="time">[09:58]</span>How would you modify this binary classification</p>
    <p class="cue"><span class="time">[10:02]</span>that detects cats in an algorithm that</p>
    <p class="cue"><span class="time">[10:05]</span>would be able to detect multiple animals,</p>
    <p class="cue"><span class="time">[10:07]</span>such as a cat, a dog, and a giraffe?</p>
    <p class="cue"><span class="time">[10:11]</span>What do you need to change about this neural network?</p>
    <p class="cue"><span class="time">[10:14]</span>Yeah.</p>
    <p class="cue"><span class="time">[10:22]</span>OK, so you would change the output layer</p>
    <p class="cue"><span class="time">[10:24]</span>to match to the number of animals you want to detect.</p>
    <p class="cue"><span class="time">[10:27]</span>Yeah, correct?</p>
    <p class="cue"><span class="time">[10:29]</span>Anyone wants to add anything else?</p>
    <p class="cue"><span class="time">[10:30]</span>Yeah.</p>
    <p class="cue"><span class="time">[10:31]</span>The data that goes in there.</p>
    <p class="cue"><span class="time">[10:32]</span>The data that goes in there.</p>
    <p class="cue"><span class="time">[10:33]</span>How would you change the data?</p>
    <p class="cue"><span class="time">[10:35]</span>It has to be for the animal data.</p>
    <p class="cue"><span class="time">[10:37]</span>OK very good.</p>
    <p class="cue"><span class="time">[10:38]</span>Yeah, you need data from dogs and giraffes</p>
    <p class="cue"><span class="time">[10:41]</span>and also maybe nature in general.</p>
    <p class="cue"><span class="time">[10:44]</span>What else do we need not to forget?</p>
    <p class="cue"><span class="time">[10:47]</span>Yeah.</p>
    <p class="cue"><span class="time">[10:48]</span>Maybe you could add a neuron for each node,</p>
    <p class="cue"><span class="time">[10:51]</span>and then your prediction would be whichever</p>
    <p class="cue"><span class="time">[10:53]</span>output is the highest.</p>
    <p class="cue"><span class="time">[10:54]</span>Yeah, OK.</p>
    <p class="cue"><span class="time">[10:55]</span>Add 1 neuron per animal.</p>
    <p class="cue"><span class="time">[10:56]</span>Those neurons will be independent from each other.</p>
    <p class="cue"><span class="time">[10:58]</span>And each neuron would focus on one animal.</p>
    <p class="cue"><span class="time">[11:01]</span>Yeah good point.</p>
    <p class="cue"><span class="time">[11:02]</span>It&#x27;s actually what we&#x27;re going to do.</p>
    <p class="cue"><span class="time">[11:04]</span>So yes, I think your suggestions were the right one.</p>
    <p class="cue"><span class="time">[11:07]</span>We could multiply this output layer</p>
    <p class="cue"><span class="time">[11:09]</span>to have three neurons instead of one.</p>
    <p class="cue"><span class="time">[11:12]</span>All of them, because it&#x27;s a fully connected neural net,</p>
    <p class="cue"><span class="time">[11:14]</span>see the entire pixels flattened in the vector.</p>
    <p class="cue"><span class="time">[11:18]</span>And then each of them will be focused on an animal.</p>
    <p class="cue"><span class="time">[11:21]</span>The number one mistake that we see in projects</p>
    <p class="cue"><span class="time">[11:24]</span>is that people add more data, but forget to adjust the labels.</p>
    <p class="cue"><span class="time">[11:29]</span>So how do the labels need to be adjusted here?</p>
    <p class="cue"><span class="time">[11:31]</span>It&#x27;s not any more 0 and 1.</p>
    <p class="cue"><span class="time">[11:34]</span>What type of labels do we need to train this?</p>
    <p class="cue"><span class="time">[11:47]</span>Yeah.</p>
    <p class="cue"><span class="time">[11:49]</span>The number of the number of range.</p>
    <p class="cue"><span class="time">[11:52]</span>You know how we call that?</p>
    <p class="cue"><span class="time">[11:53]</span>Or no.</p>
    <p class="cue"><span class="time">[11:55]</span>OK.</p>
    <p class="cue"><span class="time">[11:56]</span>So yeah.</p>
    <p class="cue"><span class="time">[11:57]</span>Yeah.</p>
    <p class="cue"><span class="time">[11:58]</span>Use vectors.</p>
    <p class="cue"><span class="time">[11:59]</span>Vectors, yeah.</p>
    <p class="cue"><span class="time">[12:00]</span>I think you&#x27;re seeing the same thing, but.</p>
    <p class="cue"><span class="time">[12:02]</span>Yeah So here you would use a one hot vector or a multi-hot.</p>
    <p class="cue"><span class="time">[12:07]</span>One hot means you&#x27;ll have a vector of size 3.</p>
    <p class="cue"><span class="time">[12:13]</span>And if there is a cat on the picture</p>
    <p class="cue"><span class="time">[12:15]</span>the label is going to be 0, 1 0 because the second neuron is</p>
    <p class="cue"><span class="time">[12:18]</span>going to be responsible to detect cats.</p>
    <p class="cue"><span class="time">[12:21]</span>In fact, that would be called a one hot vector.</p>
    <p class="cue"><span class="time">[12:24]</span>Oftentimes you&#x27;ll have multiple animals on the picture</p>
    <p class="cue"><span class="time">[12:27]</span>because cats and dogs can appear together.</p>
    <p class="cue"><span class="time">[12:29]</span>Cats and giraffes less so.</p>
    <p class="cue"><span class="time">[12:30]</span>And dogs and giraffes, I&#x27;ve never</p>
    <p class="cue"><span class="time">[12:32]</span>seen one in the same picture, but anyway.</p>
    <p class="cue"><span class="time">[12:34]</span>You&#x27;ll have a multi-hot vector if you</p>
    <p class="cue"><span class="time">[12:36]</span>have a cat and a dog on the picture,</p>
    <p class="cue"><span class="time">[12:38]</span>you&#x27;ll probably label it as 110.</p>
    <p class="cue"><span class="time">[12:40]</span>And the reason I&#x27;m mentioning that it</p>
    <p class="cue"><span class="time">[12:42]</span>may sound silly, but in a lot of projects people</p>
    <p class="cue"><span class="time">[12:44]</span>change their data.</p>
    <p class="cue"><span class="time">[12:45]</span>They forget to change their labels,</p>
    <p class="cue"><span class="time">[12:46]</span>and then they wonder why it doesn&#x27;t work.</p>
    <p class="cue"><span class="time">[12:50]</span>OK, cool.</p>
    <p class="cue"><span class="time">[12:51]</span>Now in the class, we use a specific notation</p>
    <p class="cue"><span class="time">[12:54]</span>with a superscript and subscript.</p>
    <p class="cue"><span class="time">[12:56]</span>So when you see me refer to something like a 1 1,</p>
    <p class="cue"><span class="time">[13:02]</span>the superscript in square brackets</p>
    <p class="cue"><span class="time">[13:06]</span>indicates the layer that you&#x27;re in.</p>
    <p class="cue"><span class="time">[13:08]</span>So you&#x27;re in the first layer.</p>
    <p class="cue"><span class="time">[13:09]</span>The subscript refers to the index of the neuron, OK.</p>
    <p class="cue"><span class="time">[13:15]</span>And a is for activation.</p>
    <p class="cue"><span class="time">[13:18]</span>So a subscript three square bracket 1</p>
    <p class="cue"><span class="time">[13:23]</span>is the output of the third neuron of the first layer, OK.</p>
    <p class="cue"><span class="time">[13:29]</span>Again if I continue second layer would be written like that.</p>
    <p class="cue"><span class="time">[13:32]</span>And then you&#x27;ll get your probability.</p>
    <p class="cue"><span class="time">[13:34]</span>The deeper the network is, the more capacity it has.</p>
    <p class="cue"><span class="time">[13:38]</span>This is the word we use, capacity.</p>
    <p class="cue"><span class="time">[13:41]</span>What it means is that if you send a million pictures of cats</p>
    <p class="cue"><span class="time">[13:45]</span>and a million pictures of non cats to a shallow network,</p>
    <p class="cue"><span class="time">[13:49]</span>it might not have the capacity to learn what&#x27;s in the data set.</p>
    <p class="cue"><span class="time">[13:52]</span>It&#x27;s just not flexible enough.</p>
    <p class="cue"><span class="time">[13:53]</span>The deeper the network, the more capacity it has.</p>
    <p class="cue"><span class="time">[13:56]</span>So in fact, the network that is super deep.</p>
    <p class="cue"><span class="time">[13:58]</span>Imagine a billion parameters transformer model</p>
    <p class="cue"><span class="time">[14:01]</span>with one million pictures of cat and non-cat,</p>
    <p class="cue"><span class="time">[14:04]</span>it will just overfit to those pictures.</p>
    <p class="cue"><span class="time">[14:06]</span>Meaning it&#x27;s not going to learn what a cat is,</p>
    <p class="cue"><span class="time">[14:08]</span>it&#x27;s just going to learn by heart those million</p>
    <p class="cue"><span class="time">[14:10]</span>pictures because its capacity is way bigger than the data set</p>
    <p class="cue"><span class="time">[14:14]</span>it&#x27;s fed.</p>
    <p class="cue"><span class="time">[14:14]</span>So it&#x27;s very important to understand the amount</p>
    <p class="cue"><span class="time">[14:17]</span>of data you&#x27;re going to feed.</p>
    <p class="cue"><span class="time">[14:18]</span>And the complexity, diversity of that data</p>
    <p class="cue"><span class="time">[14:20]</span>will probably dictate the capacity</p>
    <p class="cue"><span class="time">[14:23]</span>of the models you need to use.</p>
    <p class="cue"><span class="time">[14:27]</span>Now, just to give you a little bit more intuition on what</p>
    <p class="cue"><span class="time">[14:30]</span>happens inside those neural networks.</p>
    <p class="cue"><span class="time">[14:32]</span>We take these relatively shallow network,</p>
    <p class="cue"><span class="time">[14:34]</span>but call it three layers.</p>
    <p class="cue"><span class="time">[14:36]</span>And we train it on a data set of facial images.</p>
    <p class="cue"><span class="time">[14:42]</span>Ignore the task.</p>
    <p class="cue"><span class="time">[14:43]</span>In face data sets, there&#x27;s a lot of tasks you could do.</p>
    <p class="cue"><span class="time">[14:47]</span>You could do a face verification,</p>
    <p class="cue"><span class="time">[14:48]</span>you could do a face recognition, you could do face clustering,</p>
    <p class="cue"><span class="time">[14:54]</span>things like that.</p>
    <p class="cue"><span class="time">[14:54]</span>We&#x27;ll talk about that later.</p>
    <p class="cue"><span class="time">[14:56]</span>But let&#x27;s say it&#x27;s been trained really</p>
    <p class="cue"><span class="time">[14:58]</span>well on understanding faces.</p>
    <p class="cue"><span class="time">[15:00]</span>If you now unpack this network and you query each neuron</p>
    <p class="cue"><span class="time">[15:04]</span>and look what&#x27;s going on inside, what you&#x27;ll notice</p>
    <p class="cue"><span class="time">[15:08]</span>is that the first layers are going</p>
    <p class="cue"><span class="time">[15:11]</span>to be better at encoding low-complexity features,</p>
    <p class="cue"><span class="time">[15:15]</span>while the deeper networks are going to be better at encoding</p>
    <p class="cue"><span class="time">[15:19]</span>higher complexity features.</p>
    <p class="cue"><span class="time">[15:21]</span>So here&#x27;s how it goes.</p>
    <p class="cue"><span class="time">[15:22]</span>Nothing too complicated for now.</p>
    <p class="cue"><span class="time">[15:25]</span>The neuron in the first layers, they&#x27;re</p>
    <p class="cue"><span class="time">[15:27]</span>looking at pixels because you&#x27;re giving them him</p>
    <p class="cue"><span class="time">[15:30]</span>directly the pixels.</p>
    <p class="cue"><span class="time">[15:31]</span>So they&#x27;re going to be good at stitching those pixels together.</p>
    <p class="cue"><span class="time">[15:34]</span>And maybe the first neuron will be</p>
    <p class="cue"><span class="time">[15:36]</span>good at detecting diagonal edge, the second neuron</p>
    <p class="cue"><span class="time">[15:40]</span>will be good at vertical edges, and the third one</p>
    <p class="cue"><span class="time">[15:43]</span>at horizontal edges, because they&#x27;re just looking at pixels</p>
    <p class="cue"><span class="time">[15:46]</span>and trying to make sense out of them.</p>
    <p class="cue"><span class="time">[15:48]</span>Now you go one layer deeper in the middle of the network.</p>
    <p class="cue"><span class="time">[15:51]</span>Those are not seeing pixels, they&#x27;re</p>
    <p class="cue"><span class="time">[15:53]</span>seeing the output of the first layer, which is already</p>
    <p class="cue"><span class="time">[15:55]</span>slightly more complex.</p>
    <p class="cue"><span class="time">[15:57]</span>So what you can expect the layers</p>
    <p class="cue"><span class="time">[15:59]</span>in the middle of the network to give you</p>
    <p class="cue"><span class="time">[16:02]</span>or to activate for is higher level features,</p>
    <p class="cue"><span class="time">[16:04]</span>like an eye or a nose or an ear.</p>
    <p class="cue"><span class="time">[16:07]</span>Because it turns out if you have a few edges,</p>
    <p class="cue"><span class="time">[16:09]</span>you can start detecting circles.</p>
    <p class="cue"><span class="time">[16:11]</span>And so you would see a neuron that</p>
    <p class="cue"><span class="time">[16:13]</span>is really good at detecting circles, eyes.</p>
    <p class="cue"><span class="time">[16:17]</span>The deeper you go in the network,</p>
    <p class="cue"><span class="time">[16:18]</span>the more you get closer to the task itself, which in this case,</p>
    <p class="cue"><span class="time">[16:22]</span>facial analysis.</p>
    <p class="cue"><span class="time">[16:24]</span>Let&#x27;s say you would see the last few neurons,</p>
    <p class="cue"><span class="time">[16:27]</span>detect larger features of the face.</p>
    <p class="cue"><span class="time">[16:29]</span>Because again, they&#x27;re seeing higher complexity information.</p>
    <p class="cue"><span class="time">[16:32]</span>Does that make sense?</p>
    <p class="cue"><span class="time">[16:34]</span>This concept we call it encoding,</p>
    <p class="cue"><span class="time">[16:37]</span>we&#x27;ll also talk about embeddings.</p>
    <p class="cue"><span class="time">[16:39]</span>It&#x27;s very important because when you train neural networks,</p>
    <p class="cue"><span class="time">[16:43]</span>you want to make sure first that they&#x27;re</p>
    <p class="cue"><span class="time">[16:45]</span>understanding what they&#x27;re doing and that&#x27;s one way to see it.</p>
    <p class="cue"><span class="time">[16:49]</span>We&#x27;ll have an entire lecture on interpreting and visualizing</p>
    <p class="cue"><span class="time">[16:51]</span>neural networks later this quarter.</p>
    <p class="cue"><span class="time">[16:54]</span>And on top of that you probably can</p>
    <p class="cue"><span class="time">[16:58]</span>make use of some of those encodings and embeddings.</p>
    <p class="cue"><span class="time">[17:02]</span>We&#x27;ll see that later.</p>
    <p class="cue"><span class="time">[17:03]</span>But why in the vector space the distances between those concepts</p>
    <p class="cue"><span class="time">[17:08]</span>are important.</p>
    <p class="cue"><span class="time">[17:09]</span>You can already imagine that for tasks like search searching</p>
    <p class="cue"><span class="time">[17:12]</span>in a database, having a neural network able to encode</p>
    <p class="cue"><span class="time">[17:16]</span>information in a very meaningful way,</p>
    <p class="cue"><span class="time">[17:18]</span>can allow you to find concepts that</p>
    <p class="cue"><span class="time">[17:20]</span>are close to each other and associate them with each other,</p>
    <p class="cue"><span class="time">[17:23]</span>and concepts that are far from each other</p>
    <p class="cue"><span class="time">[17:25]</span>and dissociate them from each other.</p>
    <p class="cue"><span class="time">[17:29]</span>OK.</p>
    <p class="cue"><span class="time">[17:29]</span>So this was the warm up for today.</p>
    <p class="cue"><span class="time">[17:32]</span>How much time we spent.</p>
    <p class="cue"><span class="time">[17:33]</span>OK, 15 minutes in the warm up.</p>
    <p class="cue"><span class="time">[17:34]</span>That&#x27;s good.</p>
    <p class="cue"><span class="time">[17:35]</span>So we learned a few new words.</p>
    <p class="cue"><span class="time">[17:37]</span>Model, architecture, parameters.</p>
    <p class="cue"><span class="time">[17:40]</span>I didn&#x27;t talk about it, but feature engineering</p>
    <p class="cue"><span class="time">[17:42]</span>versus feature learning, that&#x27;s the core concept</p>
    <p class="cue"><span class="time">[17:46]</span>in deep learning is feature engineering</p>
    <p class="cue"><span class="time">[17:48]</span>is what we used to do before deep learning.</p>
    <p class="cue"><span class="time">[17:49]</span>Which is you might actually build an algorithm that</p>
    <p class="cue"><span class="time">[17:52]</span>is good at detecting eyes.</p>
    <p class="cue"><span class="time">[17:54]</span>It&#x27;s just a good at scanning eyes.</p>
    <p class="cue"><span class="time">[17:55]</span>You manually build it.</p>
    <p class="cue"><span class="time">[17:57]</span>And then you build another one that</p>
    <p class="cue"><span class="time">[17:59]</span>is good at detecting a mouth.</p>
    <p class="cue"><span class="time">[18:02]</span>And then you put them together to detect faces.</p>
    <p class="cue"><span class="time">[18:05]</span>We don&#x27;t do that anymore.</p>
    <p class="cue"><span class="time">[18:06]</span>We do end-to-end learning.</p>
    <p class="cue"><span class="time">[18:08]</span>Meaning we let the data speak for itself and train the model.</p>
    <p class="cue"><span class="time">[18:10]</span>This is called feature learning.</p>
    <p class="cue"><span class="time">[18:12]</span>It&#x27;s automatic, and that&#x27;s how the neural network actually</p>
    <p class="cue"><span class="time">[18:15]</span>learns those features without you needing to tell it eyes</p>
    <p class="cue"><span class="time">[18:19]</span>are important to detect faces.</p>
    <p class="cue"><span class="time">[18:20]</span>You don&#x27;t need to do that.</p>
    <p class="cue"><span class="time">[18:23]</span>Encoding and embedding.</p>
    <p class="cue"><span class="time">[18:25]</span>The real difference is encoding is any vector representation</p>
    <p class="cue"><span class="time">[18:29]</span>and an embedding, though-- sorry,</p>
    <p class="cue"><span class="time">[18:31]</span>it should have been-- an embedding is</p>
    <p class="cue"><span class="time">[18:33]</span>when an encoding has meaning.</p>
    <p class="cue"><span class="time">[18:35]</span>meaning The distance between two encodings has meaning.</p>
    <p class="cue"><span class="time">[18:40]</span>They might be close or far from each other,</p>
    <p class="cue"><span class="time">[18:42]</span>and it tells you something.</p>
    <p class="cue"><span class="time">[18:43]</span>There is a logic to it.</p>
    <p class="cue"><span class="time">[18:46]</span>And then we talked about one hot and multi-hot vectors.</p>
    <p class="cue"><span class="time">[18:50]</span>So end of the recap.</p>
    <p class="cue"><span class="time">[18:52]</span>Now let&#x27;s go into supervised learning projects</p>
    <p class="cue"><span class="time">[18:54]</span>and we&#x27;re going to make decisions together and walk</p>
    <p class="cue"><span class="time">[18:57]</span>through it.</p>
    <p class="cue"><span class="time">[18:57]</span>The first case study is day and night classification.</p>
    <p class="cue"><span class="time">[19:01]</span>So here&#x27;s my problem for you.</p>
    <p class="cue"><span class="time">[19:04]</span>Given an image that I give you, classify it</p>
    <p class="cue"><span class="time">[19:08]</span>as whether it&#x27;s the day or whether it&#x27;s the night.</p>
    <p class="cue"><span class="time">[19:12]</span>OK, open ended problem.</p>
    <p class="cue"><span class="time">[19:14]</span>Ignore that foundation models exist.</p>
    <p class="cue"><span class="time">[19:18]</span>You don&#x27;t have access to ChatGPT or Claude or whatever.</p>
    <p class="cue"><span class="time">[19:21]</span>The point is to get under the hood</p>
    <p class="cue"><span class="time">[19:22]</span>and have those discussions because obviously it&#x27;s</p>
    <p class="cue"><span class="time">[19:24]</span>a toy example.</p>
    <p class="cue"><span class="time">[19:25]</span>So what do you do?</p>
    <p class="cue"><span class="time">[19:27]</span>What data do you want to collect to solve this problem?</p>
    <p class="cue"><span class="time">[19:35]</span>Start yes.</p>
    <p class="cue"><span class="time">[19:37]</span>Check on the grid pixels in the same rows, column.</p>
    <p class="cue"><span class="time">[19:42]</span>So tell me more.</p>
    <p class="cue"><span class="time">[19:43]</span>Check how pixels in the same row are?</p>
    <p class="cue"><span class="time">[19:46]</span>Different rows.</p>
    <p class="cue"><span class="time">[19:47]</span>Just see how they are different.</p>
    <p class="cue"><span class="time">[19:52]</span>And if they are very different, it&#x27;s</p>
    <p class="cue"><span class="time">[19:54]</span>a possibility that [INAUDIBLE].</p>
    <p class="cue"><span class="time">[19:57]</span>What does it tell you that if you</p>
    <p class="cue"><span class="time">[19:59]</span>look at a row of pixels and the row next to it,</p>
    <p class="cue"><span class="time">[20:01]</span>if they&#x27;re very different.</p>
    <p class="cue"><span class="time">[20:02]</span>What does it tell you?</p>
    <p class="cue"><span class="time">[20:04]</span>It has more colors than just [INAUDIBLE].</p>
    <p class="cue"><span class="time">[20:07]</span>OK.</p>
    <p class="cue"><span class="time">[20:08]</span>So you say if the delta between pixels</p>
    <p class="cue"><span class="time">[20:10]</span>that are close geographically to each other is high,</p>
    <p class="cue"><span class="time">[20:15]</span>then there&#x27;s probably colors--</p>
    <p class="cue"><span class="time">[20:17]</span>color changes so it&#x27;s day, most likely.</p>
    <p class="cue"><span class="time">[20:19]</span>Is that it?</p>
    <p class="cue"><span class="time">[20:20]</span>So that&#x27;s an example of feature engineering.</p>
    <p class="cue"><span class="time">[20:24]</span>It&#x27;s like you&#x27;re going for it and great.</p>
    <p class="cue"><span class="time">[20:25]</span>You&#x27;re going for it and you&#x27;re trying</p>
    <p class="cue"><span class="time">[20:26]</span>to understand what&#x27;s a pattern that tells me</p>
    <p class="cue"><span class="time">[20:29]</span>that a picture is day or night.</p>
    <p class="cue"><span class="time">[20:31]</span>What else can you do in the world of neural networks.?</p>
    <p class="cue"><span class="time">[20:41]</span>Any other ideas?</p>
    <p class="cue"><span class="time">[20:43]</span>Yeah.</p>
    <p class="cue"><span class="time">[20:45]</span>Just feed in a bunch of pictures.</p>
    <p class="cue"><span class="time">[20:46]</span>Half that are during the day half du--</p>
    <p class="cue"><span class="time">[20:48]</span>OK, good.</p>
    <p class="cue"><span class="time">[20:49]</span>Yeah.</p>
    <p class="cue"><span class="time">[20:49]</span>So yeah, I agree.</p>
    <p class="cue"><span class="time">[20:51]</span>I said 10,000 images, but how do you even determine how many</p>
    <p class="cue"><span class="time">[20:54]</span>pictures you need to get started with this project?</p>
    <p class="cue"><span class="time">[21:02]</span>Feed some data.</p>
    <p class="cue"><span class="time">[21:03]</span>Feed some data.</p>
    <p class="cue"><span class="time">[21:04]</span>So you start with 10 pictures and then you continue to go.</p>
    <p class="cue"><span class="time">[21:07]</span>Yeah.</p>
    <p class="cue"><span class="time">[21:07]</span>You could do that.</p>
    <p class="cue"><span class="time">[21:08]</span>Might take some time.</p>
    <p class="cue"><span class="time">[21:10]</span>I think the question is how easy is it to collect that data.</p>
    <p class="cue"><span class="time">[21:13]</span>How would you collect that data?</p>
    <p class="cue"><span class="time">[21:16]</span>Probably from the same location you</p>
    <p class="cue"><span class="time">[21:18]</span>could get pictures from day and pictures.</p>
    <p class="cue"><span class="time">[21:20]</span>OK yeah.</p>
    <p class="cue"><span class="time">[21:21]</span>We could put our phone out there and record the day and the night</p>
    <p class="cue"><span class="time">[21:25]</span>and have a stream of pictures and add it to the data set.</p>
    <p class="cue"><span class="time">[21:28]</span>Same location, but different lightings.</p>
    <p class="cue"><span class="time">[21:31]</span>Yea.</p>
    <p class="cue"><span class="time">[21:32]</span>And you mentioned that this was a very hard problem,</p>
    <p class="cue"><span class="time">[21:36]</span>because in my opinion, also it depends on what kind of model</p>
    <p class="cue"><span class="time">[21:42]</span>you want to operate.</p>
    <p class="cue"><span class="time">[21:43]</span>For example, if you were building something</p>
    <p class="cue"><span class="time">[21:46]</span>for one location to set, whether it&#x27;s day or night,</p>
    <p class="cue"><span class="time">[21:49]</span>then that one location would be really nice.</p>
    <p class="cue"><span class="time">[21:52]</span>But then if you were trying to look at anywhere in the world,</p>
    <p class="cue"><span class="time">[21:58]</span>like in any kind of climate, whether it&#x27;s day or night,</p>
    <p class="cue"><span class="time">[22:01]</span>then you would have to have an extremely diverse set of images.</p>
    <p class="cue"><span class="time">[22:05]</span>And because the problem is so broad</p>
    <p class="cue"><span class="time">[22:07]</span>we would also need, so many-- a huge amount of data.</p>
    <p class="cue"><span class="time">[22:13]</span>That&#x27;s a great point.</p>
    <p class="cue"><span class="time">[22:14]</span>So just to repeat for the people online,</p>
    <p class="cue"><span class="time">[22:17]</span>you have to define the task first.</p>
    <p class="cue"><span class="time">[22:19]</span>Because the task can be easy.</p>
    <p class="cue"><span class="time">[22:21]</span>You can be in a park in a very specific location</p>
    <p class="cue"><span class="time">[22:23]</span>and say, just detect if it&#x27;s day or night,</p>
    <p class="cue"><span class="time">[22:26]</span>or you can have a problem, which is your camera can be anywhere.</p>
    <p class="cue"><span class="time">[22:29]</span>And that makes it more complicated.</p>
    <p class="cue"><span class="time">[22:31]</span>And also the amount of data you need</p>
    <p class="cue"><span class="time">[22:33]</span>is probably much more for that second problem.</p>
    <p class="cue"><span class="time">[22:36]</span>That&#x27;s what you said right.</p>
    <p class="cue"><span class="time">[22:37]</span>Now-- actually, that&#x27;s a great thread.</p>
    <p class="cue"><span class="time">[22:39]</span>Tell me about cases where this problem would</p>
    <p class="cue"><span class="time">[22:42]</span>be really hard to solve?</p>
    <p class="cue"><span class="time">[22:45]</span>Yeah.</p>
    <p class="cue"><span class="time">[22:46]</span>Pictures of places like inside.</p>
    <p class="cue"><span class="time">[22:49]</span>Yeah, indoor.</p>
    <p class="cue"><span class="time">[22:50]</span>Indoor pictures.</p>
    <p class="cue"><span class="time">[22:51]</span>Actually, how could you tell if you took a picture of me</p>
    <p class="cue"><span class="time">[22:55]</span>with the screen here what time it was?</p>
    <p class="cue"><span class="time">[22:58]</span>You couldn&#x27;t.</p>
    <p class="cue"><span class="time">[22:59]</span>You actually can.</p>
    <p class="cue"><span class="time">[23:00]</span>Yeah.</p>
    <p class="cue"><span class="time">[23:01]</span>There&#x27;s a clock here.</p>
    <p class="cue"><span class="time">[23:03]</span>So that&#x27;s very interesting because if that</p>
    <p class="cue"><span class="time">[23:07]</span>was part of the task then our problem would be hard.</p>
    <p class="cue"><span class="time">[23:10]</span>And 10,000 images is not going to cut it to understand time.</p>
    <p class="cue"><span class="time">[23:14]</span>And so it&#x27;s very important to define the task very well.</p>
    <p class="cue"><span class="time">[23:17]</span>What else can be hard other than indoor pictures?</p>
    <p class="cue"><span class="time">[23:20]</span>The clock could be AM/ PM.</p>
    <p class="cue"><span class="time">[23:22]</span>Also.</p>
    <p class="cue"><span class="time">[23:23]</span>Also the clock can be, AM or PM.</p>
    <p class="cue"><span class="time">[23:25]</span>But you can probably take additional information, which</p>
    <p class="cue"><span class="time">[23:29]</span>is how people are dressed and think that it might</p>
    <p class="cue"><span class="time">[23:33]</span>be warmer outside than colder.</p>
    <p class="cue"><span class="time">[23:35]</span>You could-- again, it can be very complicated</p>
    <p class="cue"><span class="time">[23:38]</span>at the end of the day, but a human</p>
    <p class="cue"><span class="time">[23:39]</span>would say someone is teaching, students are in class.</p>
    <p class="cue"><span class="time">[23:43]</span>It&#x27;s probably not 12:00 AM.</p>
    <p class="cue"><span class="time">[23:46]</span>Or so I mean, it gets complicated.</p>
    <p class="cue"><span class="time">[23:48]</span>What else can be hard?</p>
    <p class="cue"><span class="time">[23:50]</span>Sunny versus cloudy.</p>
    <p class="cue"><span class="time">[23:52]</span>OK.</p>
    <p class="cue"><span class="time">[23:52]</span>Sunny, cloudy.</p>
    <p class="cue"><span class="time">[23:53]</span>Yeah.</p>
    <p class="cue"><span class="time">[23:54]</span>If you&#x27;re in the part of the world</p>
    <p class="cue"><span class="time">[23:55]</span>where the sun doesn&#x27;t set during the day.</p>
    <p class="cue"><span class="time">[23:58]</span>Great point.</p>
    <p class="cue"><span class="time">[23:59]</span>If you&#x27;re in the North of Norway right now or Sweden,</p>
    <p class="cue"><span class="time">[24:02]</span>even the clock can&#x27;t tell you probably if it&#x27;s day or night.</p>
    <p class="cue"><span class="time">[24:05]</span>Yeah.</p>
    <p class="cue"><span class="time">[24:06]</span>Dawn and dusk.</p>
    <p class="cue"><span class="time">[24:07]</span>Dawn and dusk.</p>
    <p class="cue"><span class="time">[24:08]</span>Yeah, exactly.</p>
    <p class="cue"><span class="time">[24:09]</span>Those are great examples.</p>
    <p class="cue"><span class="time">[24:10]</span>Actually, this is a good semantic one</p>
    <p class="cue"><span class="time">[24:11]</span>because you need also to define exactly what&#x27;s the definition</p>
    <p class="cue"><span class="time">[24:14]</span>of day and night.</p>
    <p class="cue"><span class="time">[24:15]</span>So long story short, the problem can seem easy at first,</p>
    <p class="cue"><span class="time">[24:20]</span>it can be very complicated.</p>
    <p class="cue"><span class="time">[24:22]</span>And trust me, if you wanted to do this really well,</p>
    <p class="cue"><span class="time">[24:24]</span>even the foundation models today couldn&#x27;t do it in certain cases.</p>
    <p class="cue"><span class="time">[24:29]</span>OK, let&#x27;s say we have 10,000 images,</p>
    <p class="cue"><span class="time">[24:31]</span>and you talked about the split of images earlier.</p>
    <p class="cue"><span class="time">[24:33]</span>And I agree with you, you want a mix of different situations</p>
    <p class="cue"><span class="time">[24:36]</span>in order to be able to cover all of them.</p>
    <p class="cue"><span class="time">[24:39]</span>And going back to our discussion on model capacity,</p>
    <p class="cue"><span class="time">[24:41]</span>if it&#x27;s just a simple problem, you probably</p>
    <p class="cue"><span class="time">[24:44]</span>need just a small capacity model.</p>
    <p class="cue"><span class="time">[24:46]</span>If you want to add all these edge cases,</p>
    <p class="cue"><span class="time">[24:48]</span>you probably are looking for bigger-capacity models and more</p>
    <p class="cue"><span class="time">[24:51]</span>data.</p>
    <p class="cue"><span class="time">[24:53]</span>What&#x27;s the input to our model?</p>
    <p class="cue"><span class="time">[24:56]</span>I think someone said it already.</p>
    <p class="cue"><span class="time">[24:57]</span>So let&#x27;s say a picture of--</p>
    <p class="cue"><span class="time">[25:01]</span>a picture of day or night or whatever.</p>
    <p class="cue"><span class="time">[25:05]</span>What&#x27;s the resolution we&#x27;re going to work with?</p>
    <p class="cue"><span class="time">[25:07]</span>How do you determine resolution when you build a data set,</p>
    <p class="cue"><span class="time">[25:10]</span>and why does it matter?</p>
    <p class="cue"><span class="time">[25:17]</span>Yes.</p>
    <p class="cue"><span class="time">[25:17]</span>Give a number of pixels that would [INAUDIBLE] values</p>
    <p class="cue"><span class="time">[25:23]</span>and round them of to be the same vector.</p>
    <p class="cue"><span class="time">[25:25]</span>So you want to choose a resolution that&#x27;s</p>
    <p class="cue"><span class="time">[25:27]</span>going to be the same across the board.</p>
    <p class="cue"><span class="time">[25:29]</span>You&#x27;re going to vectorize everything that&#x27;s going</p>
    <p class="cue"><span class="time">[25:31]</span>to fit in the same size matrix.</p>
    <p class="cue"><span class="time">[25:32]</span>Yeah.</p>
    <p class="cue"><span class="time">[25:33]</span>Why is it important to have the right resolution let&#x27;s say?</p>
    <p class="cue"><span class="time">[25:40]</span>It&#x27;s going to have the super resolution so</p>
    <p class="cue"><span class="time">[25:43]</span>that it can recognize patterns.</p>
    <p class="cue"><span class="time">[25:45]</span>If the picture is the same to us.</p>
    <p class="cue"><span class="time">[25:48]</span>But it&#x27;s in a limited resolution.</p>
    <p class="cue"><span class="time">[25:50]</span>So you&#x27;re saying we want homogeneity</p>
    <p class="cue"><span class="time">[25:52]</span>in the data set in terms of resolution.</p>
    <p class="cue"><span class="time">[25:54]</span>I would say that could be a thing</p>
    <p class="cue"><span class="time">[25:55]</span>but, today, you can actually write</p>
    <p class="cue"><span class="time">[25:57]</span>scripts that downsize high resolution images before it</p>
    <p class="cue"><span class="time">[26:00]</span>gets in the model.</p>
    <p class="cue"><span class="time">[26:01]</span>And so it would probably solve-- upsizing is slightly harder.</p>
    <p class="cue"><span class="time">[26:05]</span>You will need another algorithm.</p>
    <p class="cue"><span class="time">[26:06]</span>But, it&#x27;s OK to have different resolutions,</p>
    <p class="cue"><span class="time">[26:09]</span>but you still want to what&#x27;s the target resolution that you&#x27;re</p>
    <p class="cue"><span class="time">[26:12]</span>looking for.</p>
    <p class="cue"><span class="time">[26:12]</span>If you have too low resolution, you lose some of your features.</p>
    <p class="cue"><span class="time">[26:15]</span>If you have too high resolution, you have too much data</p>
    <p class="cue"><span class="time">[26:17]</span>and you have a lot of weights to optimize.</p>
    <p class="cue"><span class="time">[26:19]</span>Exactly.</p>
    <p class="cue"><span class="time">[26:20]</span>Exactly.</p>
    <p class="cue"><span class="time">[26:20]</span>Low resolution, we lack information,</p>
    <p class="cue"><span class="time">[26:23]</span>so we might get things wrong.</p>
    <p class="cue"><span class="time">[26:24]</span>For example, the clock might not appear in the picture</p>
    <p class="cue"><span class="time">[26:27]</span>if it&#x27;s too low resolution.</p>
    <p class="cue"><span class="time">[26:29]</span>High resolution means more compute needed.</p>
    <p class="cue"><span class="time">[26:31]</span>If you have big pictures, it&#x27;s going</p>
    <p class="cue"><span class="time">[26:33]</span>to be heavier to train your model</p>
    <p class="cue"><span class="time">[26:36]</span>and you&#x27;re going to pay more.</p>
    <p class="cue"><span class="time">[26:37]</span>And also your cycles of iterations</p>
    <p class="cue"><span class="time">[26:39]</span>are going to be longer.</p>
    <p class="cue"><span class="time">[26:40]</span>In an AI project, that&#x27;s why your resolution matters a lot.</p>
    <p class="cue"><span class="time">[26:44]</span>So my question is, what resolution do we go for?</p>
    <p class="cue"><span class="time">[26:50]</span>Is there a way to just get to a number</p>
    <p class="cue"><span class="time">[26:53]</span>really quickly in the next 10 minutes, let&#x27;s say,</p>
    <p class="cue"><span class="time">[26:56]</span>if we were doing this project?</p>
    <p class="cue"><span class="time">[27:08]</span>Yeah.</p>
    <p class="cue"><span class="time">[27:08]</span>Who&#x27;s--</p>
    <p class="cue"><span class="time">[27:09]</span>[INAUDIBLE] you stick the image size</p>
    <p class="cue"><span class="time">[27:10]</span>and see whether you can detect.</p>
    <p class="cue"><span class="time">[27:13]</span>Yeah, exactly.</p>
    <p class="cue"><span class="time">[27:14]</span>I think that&#x27;s a great idea.</p>
    <p class="cue"><span class="time">[27:16]</span>You&#x27;re saying use the human as a proxy.</p>
    <p class="cue"><span class="time">[27:18]</span>Yeah so actually, this is how we did it.</p>
    <p class="cue"><span class="time">[27:21]</span>Back in the days we would print pictures</p>
    <p class="cue"><span class="time">[27:26]</span>in different resolutions and run it through our friends and say,</p>
    <p class="cue"><span class="time">[27:29]</span>can you tell if it&#x27;s the day or the night.</p>
    <p class="cue"><span class="time">[27:32]</span>And it turns out that below a certain resolution,</p>
    <p class="cue"><span class="time">[27:34]</span>they can&#x27;t anymore.</p>
    <p class="cue"><span class="time">[27:35]</span>It&#x27;s just like, I don&#x27;t have the information I need.</p>
    <p class="cue"><span class="time">[27:37]</span>And where we ended is somewhere around 64 by 64 by 3.</p>
    <p class="cue"><span class="time">[27:44]</span>And I stress the 3 because later this quarter we&#x27;ll</p>
    <p class="cue"><span class="time">[27:48]</span>see that OpenAI and DeepMind--</p>
    <p class="cue"><span class="time">[27:50]</span>DeepMind is [? well ?] in reinforcement learning.</p>
    <p class="cue"><span class="time">[27:53]</span>It turns out for one of the famous algorithms we learned</p>
    <p class="cue"><span class="time">[27:56]</span>together for reinforcement learning discovered that you can</p>
    <p class="cue"><span class="time">[28:01]</span>actually remove colors, and the model is not impacted,</p>
    <p class="cue"><span class="time">[28:04]</span>but your training is way simpler.</p>
    <p class="cue"><span class="time">[28:06]</span>In this case, I think colors matter because of the blue sky,</p>
    <p class="cue"><span class="time">[28:11]</span>because it does have an inherent information about whether it&#x27;s</p>
    <p class="cue"><span class="time">[28:14]</span>day or night.</p>
    <p class="cue"><span class="time">[28:15]</span>And it turns out the task, if you give it to humans,</p>
    <p class="cue"><span class="time">[28:18]</span>is much harder without colors than it is with colors.</p>
    <p class="cue"><span class="time">[28:21]</span>So those type of insights we could</p>
    <p class="cue"><span class="time">[28:23]</span>get over the next 10 minutes, literally</p>
    <p class="cue"><span class="time">[28:25]</span>by using humans as a proxy.</p>
    <p class="cue"><span class="time">[28:27]</span>This is a toy example, but I want</p>
    <p class="cue"><span class="time">[28:29]</span>you to think about that in your AI project.</p>
    <p class="cue"><span class="time">[28:31]</span>You&#x27;re going to be at points where</p>
    <p class="cue"><span class="time">[28:33]</span>you want to validate a hypothesis,</p>
    <p class="cue"><span class="time">[28:34]</span>and the best proxy you&#x27;ll have is the human.</p>
    <p class="cue"><span class="time">[28:37]</span>Yeah.</p>
    <p class="cue"><span class="time">[28:39]</span>OK, what&#x27;s the output for this model?</p>
    <p class="cue"><span class="time">[28:44]</span>Day or night.</p>
    <p class="cue"><span class="time">[28:45]</span>Yeah day or night.</p>
    <p class="cue"><span class="time">[28:46]</span>0 or 1.</p>
    <p class="cue"><span class="time">[28:47]</span>Yeah.</p>
    <p class="cue"><span class="time">[28:48]</span>The other question.</p>
    <p class="cue"><span class="time">[28:49]</span>Is there a relationship on how many input you have</p>
    <p class="cue"><span class="time">[28:53]</span>and how many neurons you have in the proxy in there.</p>
    <p class="cue"><span class="time">[28:56]</span>Do you just start from and then you</p>
    <p class="cue"><span class="time">[28:59]</span>look at the number of their relationship in seperate.</p>
    <p class="cue"><span class="time">[29:01]</span>Yeah good question.</p>
    <p class="cue"><span class="time">[29:02]</span>So does the size of the image impact the input layer?</p>
    <p class="cue"><span class="time">[29:06]</span>The size of the input layer, is that it?</p>
    <p class="cue"><span class="time">[29:08]</span>Yeah it doesn&#x27;t.</p>
    <p class="cue"><span class="time">[29:10]</span>You can make your decisions.</p>
    <p class="cue"><span class="time">[29:11]</span>You can have three neurons and you send</p>
    <p class="cue"><span class="time">[29:14]</span>a massive picture inside it.</p>
    <p class="cue"><span class="time">[29:16]</span>Oftentime, and that&#x27;s why I often</p>
    <p class="cue"><span class="time">[29:19]</span>say deep learning is an engineering field.</p>
    <p class="cue"><span class="time">[29:21]</span>You have to try it or you have to know the hacks.</p>
    <p class="cue"><span class="time">[29:25]</span>Typically the network-- in a binary classification,</p>
    <p class="cue"><span class="time">[29:31]</span>you&#x27;re going from a high-dimensional input</p>
    <p class="cue"><span class="time">[29:34]</span>to a very low dimensional output.</p>
    <p class="cue"><span class="time">[29:36]</span>So by nature, you&#x27;d imagine your network is going</p>
    <p class="cue"><span class="time">[29:38]</span>to be like this, probably.</p>
    <p class="cue"><span class="time">[29:41]</span>Meaning you need more edge detection</p>
    <p class="cue"><span class="time">[29:44]</span>at the beginning and then higher feature and higher feature,</p>
    <p class="cue"><span class="time">[29:46]</span>until at the end it detects the face,</p>
    <p class="cue"><span class="time">[29:47]</span>so it&#x27;s going to be like that.</p>
    <p class="cue"><span class="time">[29:49]</span>We&#x27;re going to see examples in week four</p>
    <p class="cue"><span class="time">[29:50]</span>where the output is bigger than the input and your network</p>
    <p class="cue"><span class="time">[29:54]</span>is probably like this.</p>
    <p class="cue"><span class="time">[29:56]</span>So you&#x27;ll build intuition during the class</p>
    <p class="cue"><span class="time">[29:59]</span>to when you want to downsize your input layer</p>
    <p class="cue"><span class="time">[30:01]</span>or upsize your input layer.</p>
    <p class="cue"><span class="time">[30:03]</span>Yeah.</p>
    <p class="cue"><span class="time">[30:04]</span>And by the way, these hacks are valuable.</p>
    <p class="cue"><span class="time">[30:07]</span>Why does meta go out and give crazy offers</p>
    <p class="cue"><span class="time">[30:10]</span>to a few researchers?</p>
    <p class="cue"><span class="time">[30:11]</span>Because they know stuff.</p>
    <p class="cue"><span class="time">[30:12]</span>They know those hacks, literally.</p>
    <p class="cue"><span class="time">[30:15]</span>OK.</p>
    <p class="cue"><span class="time">[30:18]</span>OK.</p>
    <p class="cue"><span class="time">[30:19]</span>So again, someone said it.</p>
    <p class="cue"><span class="time">[30:22]</span>The output is 0 or 1.</p>
    <p class="cue"><span class="time">[30:24]</span>The last activation is going to be a sigmoid.</p>
    <p class="cue"><span class="time">[30:26]</span>Yes question.</p>
    <p class="cue"><span class="time">[30:27]</span>Yeah.</p>
    <p class="cue"><span class="time">[30:28]</span>So forcing yourself to pick a specific resolution,</p>
    <p class="cue"><span class="time">[30:31]</span>you come up with a more general model by taking any image</p>
    <p class="cue"><span class="time">[30:34]</span>and then sampling.</p>
    <p class="cue"><span class="time">[30:35]</span>Yeah.</p>
    <p class="cue"><span class="time">[30:35]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[30:38]</span>Yeah you can do that.</p>
    <p class="cue"><span class="time">[30:39]</span>You can probably say in the data set, I don&#x27;t care of the sizes</p>
    <p class="cue"><span class="time">[30:42]</span>because I&#x27;m scraping data from everywhere.</p>
    <p class="cue"><span class="time">[30:44]</span>I&#x27;m collecting data on my phone.</p>
    <p class="cue"><span class="time">[30:46]</span>I&#x27;m putting it all in a database.</p>
    <p class="cue"><span class="time">[30:48]</span>I write a script that downsamples or upsamples</p>
    <p class="cue"><span class="time">[30:50]</span>everything to the same resolution,</p>
    <p class="cue"><span class="time">[30:52]</span>and then I send those lower res images in the network.</p>
    <p class="cue"><span class="time">[30:55]</span>Because what I care about is the training of my network.</p>
    <p class="cue"><span class="time">[30:58]</span>I want it to be fast and inefficient.</p>
    <p class="cue"><span class="time">[31:00]</span>You could do that.</p>
    <p class="cue"><span class="time">[31:01]</span>Yeah, for sure.</p>
    <p class="cue"><span class="time">[31:02]</span>Yeah.</p>
    <p class="cue"><span class="time">[31:03]</span>And there are networks we&#x27;re going to see in the--</p>
    <p class="cue"><span class="time">[31:05]</span>YOLO is an example of a network where</p>
    <p class="cue"><span class="time">[31:07]</span>in the later versions of YOLO it does that automatically.</p>
    <p class="cue"><span class="time">[31:11]</span>It can take any resolution and it just samples it accordingly.</p>
    <p class="cue"><span class="time">[31:16]</span>OK last activation is going to be sigmoid.</p>
    <p class="cue"><span class="time">[31:18]</span>We wanted the output to be between 0 and 1.</p>
    <p class="cue"><span class="time">[31:20]</span>And the architecture most likely a shallow network.</p>
    <p class="cue"><span class="time">[31:24]</span>We don&#x27;t need too much capacity unless the task</p>
    <p class="cue"><span class="time">[31:26]</span>is highly complex.</p>
    <p class="cue"><span class="time">[31:27]</span>And a convolution should do the work really well.</p>
    <p class="cue"><span class="time">[31:30]</span>You don&#x27;t what a convolution is yet,</p>
    <p class="cue"><span class="time">[31:32]</span>you&#x27;re going to know in a few weeks,</p>
    <p class="cue"><span class="time">[31:33]</span>but those are known to be good for sequences and for images.</p>
    <p class="cue"><span class="time">[31:38]</span>And finally, the loss function.</p>
    <p class="cue"><span class="time">[31:39]</span>What loss function would you use?</p>
    <p class="cue"><span class="time">[31:44]</span>Uh.</p>
    <p class="cue"><span class="time">[31:46]</span>Which one?</p>
    <p class="cue"><span class="time">[31:47]</span>Sigmoid.</p>
    <p class="cue"><span class="time">[31:47]</span>Sigmoid.</p>
    <p class="cue"><span class="time">[31:48]</span>No, sigmoid is the activation.</p>
    <p class="cue"><span class="time">[31:50]</span>What&#x27;s the loss function?</p>
    <p class="cue"><span class="time">[31:54]</span>Logistic loss.</p>
    <p class="cue"><span class="time">[31:55]</span>Logistic loss.</p>
    <p class="cue"><span class="time">[31:56]</span>Yeah, logistic loss also called binary cross-entropy loss,</p>
    <p class="cue"><span class="time">[32:00]</span>which is the one you&#x27;ve seen in the videos this week.</p>
    <p class="cue"><span class="time">[32:03]</span>Yeah.</p>
    <p class="cue"><span class="time">[32:05]</span>When you&#x27;re like selecting your images and your resolution,</p>
    <p class="cue"><span class="time">[32:08]</span>how much is designing that around the hardware limitations</p>
    <p class="cue"><span class="time">[32:12]</span>matter?</p>
    <p class="cue"><span class="time">[32:12]</span>If you have certain memory bandwidth,</p>
    <p class="cue"><span class="time">[32:16]</span>adder sizes, caching, is that an issue or do you just let that</p>
    <p class="cue"><span class="time">[32:20]</span>come out raw.</p>
    <p class="cue"><span class="time">[32:21]</span>So repeating the question how much the amount of hardware</p>
    <p class="cue"><span class="time">[32:25]</span>and the quality of the hardware you have at your disposal</p>
    <p class="cue"><span class="time">[32:27]</span>influences those decisions?</p>
    <p class="cue"><span class="time">[32:29]</span>The answer is a lot.</p>
    <p class="cue"><span class="time">[32:32]</span>But, in fact, I&#x27;m assuming you&#x27;re</p>
    <p class="cue"><span class="time">[32:34]</span>training it on your laptop right now, which will work.</p>
    <p class="cue"><span class="time">[32:38]</span>But you need to make those trade offs.</p>
    <p class="cue"><span class="time">[32:40]</span>And I think that&#x27;s why those skills matter.</p>
    <p class="cue"><span class="time">[32:42]</span>Now, if the task is more complicated,</p>
    <p class="cue"><span class="time">[32:45]</span>you look at the hardware you have available,</p>
    <p class="cue"><span class="time">[32:47]</span>you&#x27;ll make a quick back of the envelope calculation.</p>
    <p class="cue"><span class="time">[32:50]</span>The calculation is really about how fast</p>
    <p class="cue"><span class="time">[32:52]</span>our iteration cycles can be.</p>
    <p class="cue"><span class="time">[32:54]</span>It&#x27;s not about necessarily the performance of the model.</p>
    <p class="cue"><span class="time">[32:56]</span>In the end, you want to be able to iterate super quickly.</p>
    <p class="cue"><span class="time">[32:59]</span>And if your model takes one year to train,</p>
    <p class="cue"><span class="time">[33:02]</span>you&#x27;re not going to be able to iterate.</p>
    <p class="cue"><span class="time">[33:03]</span>You need to reduce it somehow.</p>
    <p class="cue"><span class="time">[33:05]</span>So there are situations that we&#x27;re going from 64 by 64 to 65</p>
    <p class="cue"><span class="time">[33:10]</span>by 65, will double your compute time depending on--</p>
    <p class="cue"><span class="time">[33:13]</span>Yeah.</p>
    <p class="cue"><span class="time">[33:14]</span>Yeah, for sure.</p>
    <p class="cue"><span class="time">[33:16]</span>Yeah.</p>
    <p class="cue"><span class="time">[33:17]</span>Are the features of the model, every scaler in the 64</p>
    <p class="cue"><span class="time">[33:21]</span>by 64 by 3 array?</p>
    <p class="cue"><span class="time">[33:24]</span>What do you mean?</p>
    <p class="cue"><span class="time">[33:25]</span>The--</p>
    <p class="cue"><span class="time">[33:26]</span>Like the features is every single scalar value in this--</p>
    <p class="cue"><span class="time">[33:32]</span>So the 64 by 64 by 3 is like literally, you look at a picture</p>
    <p class="cue"><span class="time">[33:36]</span>and you look at one pixel.</p>
    <p class="cue"><span class="time">[33:38]</span>These have three values that describe the color.</p>
    <p class="cue"><span class="time">[33:41]</span>And then you just flatten it.</p>
    <p class="cue"><span class="time">[33:43]</span>And then those numbers are given to the input</p>
    <p class="cue"><span class="time">[33:45]</span>layer of your neural network.</p>
    <p class="cue"><span class="time">[33:46]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[33:48]</span>Yeah right.</p>
    <p class="cue"><span class="time">[33:50]</span>OK let&#x27;s move on just for the sake of time.</p>
    <p class="cue"><span class="time">[33:53]</span>This was just the easy warm up.</p>
    <p class="cue"><span class="time">[33:55]</span>The two things I just want you to remember from that project</p>
    <p class="cue"><span class="time">[33:57]</span>is we&#x27;re going to build a lot of proxy projects,</p>
    <p class="cue"><span class="time">[34:00]</span>and it&#x27;s important to remember them for your own projects.</p>
    <p class="cue"><span class="time">[34:03]</span>So you remember, oh, I remember this experiment</p>
    <p class="cue"><span class="time">[34:05]</span>we did on humans.</p>
    <p class="cue"><span class="time">[34:06]</span>Or I remember how many images we needed for that project.</p>
    <p class="cue"><span class="time">[34:09]</span>And then that can help you make decisions faster.</p>
    <p class="cue"><span class="time">[34:12]</span>And then the other thing that is worth highlighting</p>
    <p class="cue"><span class="time">[34:14]</span>is the human experiments.</p>
    <p class="cue"><span class="time">[34:16]</span>We&#x27;re going to do a lot of human experiments</p>
    <p class="cue"><span class="time">[34:18]</span>starting with the next example.</p>
    <p class="cue"><span class="time">[34:19]</span>And those are usually helpful to make quick decisions</p>
    <p class="cue"><span class="time">[34:22]</span>in your project when you&#x27;re in the industry.</p>
    <p class="cue"><span class="time">[34:25]</span>So second project, trigger word detection.</p>
    <p class="cue"><span class="time">[34:27]</span>Let me give some context on this.</p>
    <p class="cue"><span class="time">[34:29]</span>The general problem--</p>
    <p class="cue"><span class="time">[34:32]</span>OK, you&#x27;re familiar with Alexa and all these--</p>
    <p class="cue"><span class="time">[34:37]</span>let&#x27;s say Siri and things like that that you</p>
    <p class="cue"><span class="time">[34:40]</span>might have in your kitchen listening to you.</p>
    <p class="cue"><span class="time">[34:42]</span>Everybody knows.</p>
    <p class="cue"><span class="time">[34:43]</span>So the way these networks typically</p>
    <p class="cue"><span class="time">[34:46]</span>work these models is it&#x27;s not a single model,</p>
    <p class="cue"><span class="time">[34:49]</span>it&#x27;s a cascade of models for energy and efficiency purposes.</p>
    <p class="cue"><span class="time">[34:54]</span>So for example, if you have a virtual assistant</p>
    <p class="cue"><span class="time">[34:58]</span>in your kitchen, the first model is activity detection.</p>
    <p class="cue"><span class="time">[35:02]</span>It just detects if there is any volume.</p>
    <p class="cue"><span class="time">[35:04]</span>Because you don&#x27;t want to listen with the heavy model</p>
    <p class="cue"><span class="time">[35:07]</span>at all times, it just uses a lot of energy.</p>
    <p class="cue"><span class="time">[35:09]</span>You want a very lightweight model</p>
    <p class="cue"><span class="time">[35:11]</span>that understands when volume is playing.</p>
    <p class="cue"><span class="time">[35:13]</span>And so let&#x27;s say this network detects volume,</p>
    <p class="cue"><span class="time">[35:16]</span>it calls another network that is focused on the activation</p>
    <p class="cue"><span class="time">[35:20]</span>word the trigger word.</p>
    <p class="cue"><span class="time">[35:22]</span>Alexa, hey Siri, OK Google.</p>
    <p class="cue"><span class="time">[35:26]</span>That&#x27;s usually the second layer.</p>
    <p class="cue"><span class="time">[35:28]</span>And that one is only listening for a specific keyword.</p>
    <p class="cue"><span class="time">[35:31]</span>If the keyword comes in, it would typically</p>
    <p class="cue"><span class="time">[35:33]</span>call a better model that&#x27;s slightly slower,</p>
    <p class="cue"><span class="time">[35:37]</span>that might be heavier and more energy consumption,</p>
    <p class="cue"><span class="time">[35:41]</span>and that might understand what you&#x27;re trying to do.</p>
    <p class="cue"><span class="time">[35:45]</span>And then I&#x27;m not going to go into the details,</p>
    <p class="cue"><span class="time">[35:48]</span>but you have architectures that get very complicated.</p>
    <p class="cue"><span class="time">[35:51]</span>Back in the day, some of these companies</p>
    <p class="cue"><span class="time">[35:53]</span>were doing one model to set up a timer, one model</p>
    <p class="cue"><span class="time">[35:57]</span>to buy something online.</p>
    <p class="cue"><span class="time">[35:59]</span>One model was very complicated.</p>
    <p class="cue"><span class="time">[36:01]</span>Today it&#x27;s slightly simpler and more end to end</p>
    <p class="cue"><span class="time">[36:03]</span>but I just want you to know the cascade of models</p>
    <p class="cue"><span class="time">[36:05]</span>that are being called because this case</p>
    <p class="cue"><span class="time">[36:07]</span>study is about the second model is about the trigger word.</p>
    <p class="cue"><span class="time">[36:10]</span>So here&#x27;s my problem for you.</p>
    <p class="cue"><span class="time">[36:12]</span>Given a 10-second audio speech, detect the word activate.</p>
    <p class="cue"><span class="time">[36:17]</span>How would you build that off the shelf like that starting from 0.</p>
    <p class="cue"><span class="time">[36:23]</span>What data would you collect?</p>
    <p class="cue"><span class="time">[36:30]</span>Yep.</p>
    <p class="cue"><span class="time">[36:31]</span>So we are looking at frequencies that we might receive.</p>
    <p class="cue"><span class="time">[36:35]</span>And from those frequency we want to figure out what was the word.</p>
    <p class="cue"><span class="time">[36:40]</span>And we also want to look at [INAUDIBLE].</p>
    <p class="cue"><span class="time">[36:44]</span>OK.</p>
    <p class="cue"><span class="time">[36:45]</span>How long does this usually take to say the word, and then</p>
    <p class="cue"><span class="time">[36:49]</span>we will run some kind of algorithm</p>
    <p class="cue"><span class="time">[36:51]</span>to translate our frequency to [INAUDIBLE].</p>
    <p class="cue"><span class="time">[36:53]</span>Like a Fourier transform, or--</p>
    <p class="cue"><span class="time">[36:57]</span>what you described is a Fourier transform</p>
    <p class="cue"><span class="time">[36:59]</span>or plus the preprocessing.</p>
    <p class="cue"><span class="time">[37:01]</span>You&#x27;re right.</p>
    <p class="cue"><span class="time">[37:01]</span>So you&#x27;re saying audio is a bunch of frequencies</p>
    <p class="cue"><span class="time">[37:05]</span>with values.</p>
    <p class="cue"><span class="time">[37:06]</span>And we want to first pre-process that, then</p>
    <p class="cue"><span class="time">[37:09]</span>to give it to an algorithm.</p>
    <p class="cue"><span class="time">[37:11]</span>And the length of the sequence matters as well.</p>
    <p class="cue"><span class="time">[37:13]</span>Because if you want to detect the word</p>
    <p class="cue"><span class="time">[37:15]</span>activate you know that the length needs to be--</p>
    <p class="cue"><span class="time">[37:18]</span>there&#x27;s a minimum length.</p>
    <p class="cue"><span class="time">[37:19]</span>You can&#x27;t say activate in less than 10 milliseconds.</p>
    <p class="cue"><span class="time">[37:21]</span>So the length matters as well.</p>
    <p class="cue"><span class="time">[37:23]</span>That&#x27;s good insight.</p>
    <p class="cue"><span class="time">[37:25]</span>What else?</p>
    <p class="cue"><span class="time">[37:25]</span>What data?</p>
    <p class="cue"><span class="time">[37:26]</span>How would you collect that data?</p>
    <p class="cue"><span class="time">[37:29]</span>Yes.</p>
    <p class="cue"><span class="time">[37:31]</span>Microphone like with your phone.</p>
    <p class="cue"><span class="time">[37:33]</span>OK.</p>
    <p class="cue"><span class="time">[37:33]</span>You would go around campus and record people.</p>
    <p class="cue"><span class="time">[37:36]</span>How would you-- what would you ask them to say?</p>
    <p class="cue"><span class="time">[37:40]</span>The word activate.</p>
    <p class="cue"><span class="time">[37:41]</span>You would ask-- you would record a bunch of people</p>
    <p class="cue"><span class="time">[37:43]</span>saying the word activate.</p>
    <p class="cue"><span class="time">[37:44]</span>You would ask them anything else?</p>
    <p class="cue"><span class="time">[37:48]</span>Other words.</p>
    <p class="cue"><span class="time">[37:48]</span>State--</p>
    <p class="cue"><span class="time">[37:49]</span>Other words.</p>
    <p class="cue"><span class="time">[37:50]</span>Yeah.</p>
    <p class="cue"><span class="time">[37:51]</span>Say a sentence.</p>
    <p class="cue"><span class="time">[37:51]</span>Say a sentence.</p>
    <p class="cue"><span class="time">[37:52]</span>Yeah.</p>
    <p class="cue"><span class="time">[37:53]</span>It turns out you have websites that are random generators,</p>
    <p class="cue"><span class="time">[37:56]</span>and you just say, say this and say this,</p>
    <p class="cue"><span class="time">[37:58]</span>and you record everything.</p>
    <p class="cue"><span class="time">[37:59]</span>Yeah.</p>
    <p class="cue"><span class="time">[38:00]</span>Say the word deactivate.</p>
    <p class="cue"><span class="time">[38:01]</span>Oh good one.</p>
    <p class="cue"><span class="time">[38:02]</span>So you&#x27;re saying you want to find negative words that</p>
    <p class="cue"><span class="time">[38:05]</span>are close to the positive word, just to make sure</p>
    <p class="cue"><span class="time">[38:08]</span>that the model learns that.</p>
    <p class="cue"><span class="time">[38:09]</span>That&#x27;s a great one.</p>
    <p class="cue"><span class="time">[38:09]</span>Yeah.</p>
    <p class="cue"><span class="time">[38:10]</span>Actually, it turns out activate is a really bad word to choose.</p>
    <p class="cue"><span class="time">[38:14]</span>The reason Alexa, and actually, there</p>
    <p class="cue"><span class="time">[38:16]</span>was a lot of discussions at Amazon</p>
    <p class="cue"><span class="time">[38:19]</span>back in the days around what would the word be.</p>
    <p class="cue"><span class="time">[38:21]</span>And it turns out it&#x27;s very important, what you choose,</p>
    <p class="cue"><span class="time">[38:23]</span>because you want to choose a word that is not</p>
    <p class="cue"><span class="time">[38:25]</span>used in common language, otherwise your assistant</p>
    <p class="cue"><span class="time">[38:28]</span>is always turning on.</p>
    <p class="cue"><span class="time">[38:30]</span>And Alexa is not ideal either.</p>
    <p class="cue"><span class="time">[38:32]</span>It&#x27;s not bad, but it&#x27;s not ideal either.</p>
    <p class="cue"><span class="time">[38:34]</span>OK, so let me just narrow down the problem.</p>
    <p class="cue"><span class="time">[38:37]</span>Let&#x27;s say we&#x27;ve gone around campus,</p>
    <p class="cue"><span class="time">[38:39]</span>and we&#x27;ve collected a bunch of 10-second audio clips.</p>
    <p class="cue"><span class="time">[38:48]</span>Do we need to think about the distribution of the data?</p>
    <p class="cue"><span class="time">[38:51]</span>Why does it matter?</p>
    <p class="cue"><span class="time">[38:52]</span>Why campus only might be limited, let&#x27;s say?</p>
    <p class="cue"><span class="time">[38:56]</span>Yeah.</p>
    <p class="cue"><span class="time">[38:57]</span>Maybe [? I&#x27;d ?] get a lot of accents.</p>
    <p class="cue"><span class="time">[38:59]</span>OK, accents.</p>
    <p class="cue"><span class="time">[39:00]</span>Turns out the first version of this model</p>
    <p class="cue"><span class="time">[39:04]</span>that I built with Andrew, my German friends,</p>
    <p class="cue"><span class="time">[39:08]</span>could not make it work.</p>
    <p class="cue"><span class="time">[39:09]</span>None of my German friends would make it work.</p>
    <p class="cue"><span class="time">[39:11]</span>And we had to collect more data from--</p>
    <p class="cue"><span class="time">[39:13]</span>and I had two German roommates at the time,</p>
    <p class="cue"><span class="time">[39:15]</span>so we had to collect more data from them,</p>
    <p class="cue"><span class="time">[39:17]</span>because there&#x27;s just a certain way that people would say words.</p>
    <p class="cue"><span class="time">[39:22]</span>OK good insight.</p>
    <p class="cue"><span class="time">[39:23]</span>What else other than accents?</p>
    <p class="cue"><span class="time">[39:26]</span>Yes.</p>
    <p class="cue"><span class="time">[39:27]</span>Average age.</p>
    <p class="cue"><span class="time">[39:29]</span>Good point.</p>
    <p class="cue"><span class="time">[39:30]</span>Average age on campus is probably</p>
    <p class="cue"><span class="time">[39:32]</span>younger than if you actually cross campus and go</p>
    <p class="cue"><span class="time">[39:35]</span>to Palo Alto.</p>
    <p class="cue"><span class="time">[39:37]</span>And in fact, the frequencies are going to be different</p>
    <p class="cue"><span class="time">[39:39]</span>that younger people use.</p>
    <p class="cue"><span class="time">[39:40]</span>That&#x27;s correct.</p>
    <p class="cue"><span class="time">[39:43]</span>Yeah.</p>
    <p class="cue"><span class="time">[39:43]</span>There is also the cadence.</p>
    <p class="cue"><span class="time">[39:45]</span>How fast you say words.</p>
    <p class="cue"><span class="time">[39:46]</span>How fast-- some people speak fast,</p>
    <p class="cue"><span class="time">[39:48]</span>some people-- and it has to do with the language of origin.</p>
    <p class="cue"><span class="time">[39:50]</span>Some people just speak faster.</p>
    <p class="cue"><span class="time">[39:53]</span>Correct.</p>
    <p class="cue"><span class="time">[39:53]</span>And look at this.</p>
    <p class="cue"><span class="time">[39:56]</span>When you hear someone who speaks fast</p>
    <p class="cue"><span class="time">[39:58]</span>versus someone who speaks slow, it doesn&#x27;t make a big difference</p>
    <p class="cue"><span class="time">[40:02]</span>to you as a human.</p>
    <p class="cue"><span class="time">[40:04]</span>But if you actually just had access</p>
    <p class="cue"><span class="time">[40:06]</span>to the numbers and the frequencies,</p>
    <p class="cue"><span class="time">[40:08]</span>it would look completely different.</p>
    <p class="cue"><span class="time">[40:09]</span>So the model actually struggles a lot with that problem.</p>
    <p class="cue"><span class="time">[40:13]</span>Yeah.</p>
    <p class="cue"><span class="time">[40:14]</span>I don&#x27;t think this is very important for the ratio of male</p>
    <p class="cue"><span class="time">[40:18]</span>to female voice.</p>
    <p class="cue"><span class="time">[40:18]</span>Yeah for sure.</p>
    <p class="cue"><span class="time">[40:19]</span>Ratio of male to female.</p>
    <p class="cue"><span class="time">[40:21]</span>Anything that would modify the frequencies.</p>
    <p class="cue"><span class="time">[40:23]</span>And on average, yes, there&#x27;s different frequencies</p>
    <p class="cue"><span class="time">[40:25]</span>or distribution male to female.</p>
    <p class="cue"><span class="time">[40:27]</span>Yeah.</p>
    <p class="cue"><span class="time">[40:27]</span>[INAUDIBLE] noise or other people like talking to them.</p>
    <p class="cue"><span class="time">[40:29]</span>Background noise.</p>
    <p class="cue"><span class="time">[40:30]</span>Very important.</p>
    <p class="cue"><span class="time">[40:31]</span>Turns out on Stanford campus, you don&#x27;t hear the metro.</p>
    <p class="cue"><span class="time">[40:36]</span>So it&#x27;s very likely that your algorithm will not</p>
    <p class="cue"><span class="time">[40:39]</span>work for people in New York that are taking the subway all</p>
    <p class="cue"><span class="time">[40:42]</span>the time because of the background noise behind it.</p>
    <p class="cue"><span class="time">[40:44]</span>Yeah OK.</p>
    <p class="cue"><span class="time">[40:45]</span>I think we get a sense of the again, the complexity</p>
    <p class="cue"><span class="time">[40:48]</span>of the task ahead of us.</p>
    <p class="cue"><span class="time">[40:50]</span>Let&#x27;s say the input is a 10-second audio CLIP.</p>
    <p class="cue"><span class="time">[40:54]</span>I&#x27;m going to call X. And this audio CLIP</p>
    <p class="cue"><span class="time">[40:56]</span>has a few things that are special to it.</p>
    <p class="cue"><span class="time">[41:00]</span>So one of the things is a negative words</p>
    <p class="cue"><span class="time">[41:03]</span>which are in purple, positive words which are in green,</p>
    <p class="cue"><span class="time">[41:08]</span>and then the background is in orange.</p>
    <p class="cue"><span class="time">[41:11]</span>So this is for example, someone saying hi,</p>
    <p class="cue"><span class="time">[41:13]</span>activate yourself, whatever.</p>
    <p class="cue"><span class="time">[41:16]</span>You see what I mean?</p>
    <p class="cue"><span class="time">[41:17]</span>Activate is the positive word.</p>
    <p class="cue"><span class="time">[41:19]</span>What&#x27;s the resolution we want?</p>
    <p class="cue"><span class="time">[41:22]</span>OK I&#x27;m not going to ask you this question</p>
    <p class="cue"><span class="time">[41:24]</span>because we don&#x27;t have speech expert-- a speech expert would</p>
    <p class="cue"><span class="time">[41:26]</span>know it.</p>
    <p class="cue"><span class="time">[41:27]</span>What you can do, though, without being a speech expert,</p>
    <p class="cue"><span class="time">[41:30]</span>is to go on GitHub and find another speech project,</p>
    <p class="cue"><span class="time">[41:34]</span>and you actually search for the hyperparameters they&#x27;re using.</p>
    <p class="cue"><span class="time">[41:37]</span>And if you&#x27;re using human audio, you&#x27;ll</p>
    <p class="cue"><span class="time">[41:39]</span>find that the same numbers will work for your project.</p>
    <p class="cue"><span class="time">[41:42]</span>OK.</p>
    <p class="cue"><span class="time">[41:44]</span>So you do that little search, and you&#x27;ll</p>
    <p class="cue"><span class="time">[41:46]</span>find that there is just a certain sample rate that</p>
    <p class="cue"><span class="time">[41:50]</span>works well with human voice.</p>
    <p class="cue"><span class="time">[41:53]</span>What&#x27;s the output?</p>
    <p class="cue"><span class="time">[41:59]</span>0 or 1.</p>
    <p class="cue"><span class="time">[42:00]</span>OK, let&#x27;s try something.</p>
    <p class="cue"><span class="time">[42:01]</span>So let&#x27;s say the output is 0 or 1.</p>
    <p class="cue"><span class="time">[42:03]</span>0 meaning there is no positive word.</p>
    <p class="cue"><span class="time">[42:05]</span>The word activate is not there.</p>
    <p class="cue"><span class="time">[42:06]</span>One meaning there is a positive word in that 10-second audio</p>
    <p class="cue"><span class="time">[42:10]</span>clip.</p>
    <p class="cue"><span class="time">[42:10]</span>So we&#x27;re going to do a little human experiment.</p>
    <p class="cue"><span class="time">[42:13]</span>I&#x27;ve selected three-- let me turn the volume on.</p>
    <p class="cue"><span class="time">[42:18]</span>I&#x27;ve selected three audio samples of around 10 seconds.</p>
    <p class="cue"><span class="time">[42:24]</span>OK.</p>
    <p class="cue"><span class="time">[42:26]</span>I&#x27;m not going to tell you what the language is,</p>
    <p class="cue"><span class="time">[42:29]</span>because the model doesn&#x27;t language</p>
    <p class="cue"><span class="time">[42:31]</span>when we start training it.</p>
    <p class="cue"><span class="time">[42:32]</span>So you&#x27;re acting like the model.</p>
    <p class="cue"><span class="time">[42:34]</span>That&#x27;s the experiment.</p>
    <p class="cue"><span class="time">[42:35]</span>I&#x27;m just telling you that the first.</p>
    <p class="cue"><span class="time">[42:38]</span>And the third sample have the word that we&#x27;re looking for.</p>
    <p class="cue"><span class="time">[42:42]</span>And I&#x27;m not telling you what the word is again,</p>
    <p class="cue"><span class="time">[42:44]</span>because the model doesn&#x27;t what the word is</p>
    <p class="cue"><span class="time">[42:45]</span>at the beginning of training.</p>
    <p class="cue"><span class="time">[42:47]</span>So now up to you to guess what the word is.</p>
    <p class="cue"><span class="time">[42:50]</span>And I hope one of you will save us and find the word.</p>
    <p class="cue"><span class="time">[42:53]</span>OK let&#x27;s try it.</p>
    <p class="cue"><span class="time">[42:56]</span>[NON-ENGLISH SPEECH]</p>
    <p class="cue"><span class="time">[43:02]</span>To loud.</p>
    <p class="cue"><span class="time">[43:03]</span>Second sample.</p>
    <p class="cue"><span class="time">[43:06]</span>Wait let me see if I can put the microphone here.</p>
    <p class="cue"><span class="time">[43:11]</span>[NON-ENGLISH SPEECH]</p>
    <p class="cue"><span class="time">[43:18]</span>Anybody has it or.</p>
    <p class="cue"><span class="time">[43:19]</span>No, no.</p>
    <p class="cue"><span class="time">[43:20]</span>No way.</p>
    <p class="cue"><span class="time">[43:21]</span>Impossible.</p>
    <p class="cue"><span class="time">[43:23]</span>Third one.</p>
    <p class="cue"><span class="time">[43:25]</span>[NON-ENGLISH SPEECH]</p>
    <p class="cue"><span class="time">[43:31]</span>OK.</p>
    <p class="cue"><span class="time">[43:32]</span>Who has the word?</p>
    <p class="cue"><span class="time">[43:35]</span>[NON-ENGLISH SPEECH]</p>
    <p class="cue"><span class="time">[43:37]</span>OK, maybe.</p>
    <p class="cue"><span class="time">[43:38]</span>That&#x27;s not it, but maybe.</p>
    <p class="cue"><span class="time">[43:41]</span>Yeah in the back?</p>
    <p class="cue"><span class="time">[43:42]</span>Sounds like [NON-ENGLISH SPEECH]</p>
    <p class="cue"><span class="time">[43:43]</span>You&#x27;re Italian?</p>
    <p class="cue"><span class="time">[43:46]</span>You speak-- OK.</p>
    <p class="cue"><span class="time">[43:47]</span>[LAUGHTER] Yeah it&#x27;s funny.</p>
    <p class="cue"><span class="time">[43:50]</span>Nobody finds it usually in the first try, but you did find it.</p>
    <p class="cue"><span class="time">[43:54]</span>Yeah that&#x27;s correct.</p>
    <p class="cue"><span class="time">[43:56]</span>OK, let&#x27;s try again and do it with a different labeling scheme</p>
    <p class="cue"><span class="time">[44:01]</span>this time.</p>
    <p class="cue"><span class="time">[44:02]</span>OK, let&#x27;s try again, I&#x27;m going to play it again,</p>
    <p class="cue"><span class="time">[44:04]</span>but the labeling scheme has changed.</p>
    <p class="cue"><span class="time">[44:09]</span>[NON-ENGLISH SPEECH]</p>
    <p class="cue"><span class="time">[44:14]</span>It&#x27;s the first one.</p>
    <p class="cue"><span class="time">[44:17]</span>[NON-ENGLISH SPEECH]</p>
    <p class="cue"><span class="time">[44:23]</span>Third one.</p>
    <p class="cue"><span class="time">[44:24]</span>[NON-ENGLISH SPEECH]</p>
    <p class="cue"><span class="time">[44:31]</span>What&#x27;s the word.</p>
    <p class="cue"><span class="time">[44:33]</span>Someone who&#x27;s not Italian speaker.</p>
    <p class="cue"><span class="time">[44:42]</span>I&#x27;m not sure people heard so I want to try someone else.</p>
    <p class="cue"><span class="time">[44:45]</span>Yeah, you&#x27;ve heard it.</p>
    <p class="cue"><span class="time">[44:46]</span>Something like promojo.</p>
    <p class="cue"><span class="time">[44:47]</span>OK not far promojo, it&#x27;s pomeriggio But you&#x27;re close.</p>
    <p class="cue"><span class="time">[44:52]</span>Was it easier the second time or the first time?</p>
    <p class="cue"><span class="time">[44:54]</span>The second.</p>
    <p class="cue"><span class="time">[44:55]</span>Way more.</p>
    <p class="cue"><span class="time">[44:56]</span>Way easier.</p>
    <p class="cue"><span class="time">[44:57]</span>So if it&#x27;s easier for you, it&#x27;s easier for the model, basically.</p>
    <p class="cue"><span class="time">[45:01]</span>And so what is the question I&#x27;m posing here</p>
    <p class="cue"><span class="time">[45:04]</span>is we could go with the first labeling scheme which is</p>
    <p class="cue"><span class="time">[45:08]</span>easier for us to label frankly.</p>
    <p class="cue"><span class="time">[45:10]</span>You don&#x27;t need to indicate the location of the word.</p>
    <p class="cue"><span class="time">[45:14]</span>But how much more data do you think</p>
    <p class="cue"><span class="time">[45:16]</span>we&#x27;ll need in order for the model to figure it out?</p>
    <p class="cue"><span class="time">[45:20]</span>It&#x27;s probably 1,000x data.</p>
    <p class="cue"><span class="time">[45:24]</span>And so the question is is the second labeling scheme 1,000</p>
    <p class="cue"><span class="time">[45:27]</span>times harder for us to label than the first one</p>
    <p class="cue"><span class="time">[45:31]</span>and the answer is no.</p>
    <p class="cue"><span class="time">[45:32]</span>So the answer is very clear.</p>
    <p class="cue"><span class="time">[45:34]</span>You would rather have the second labeling scheme,</p>
    <p class="cue"><span class="time">[45:36]</span>and your model is going to learn way</p>
    <p class="cue"><span class="time">[45:37]</span>faster than in the first case.</p>
    <p class="cue"><span class="time">[45:41]</span>So that&#x27;s the type of human experiment you can do.</p>
    <p class="cue"><span class="time">[45:43]</span>Now we&#x27;re going to use that labeling scheme.</p>
    <p class="cue"><span class="time">[45:46]</span>Yeah question.</p>
    <p class="cue"><span class="time">[45:47]</span>For the label.</p>
    <p class="cue"><span class="time">[45:48]</span>This is Marvin, right?</p>
    <p class="cue"><span class="time">[45:49]</span>Yeah we&#x27;ll talk about it.</p>
    <p class="cue"><span class="time">[45:51]</span>Question is it manual labeling or not?</p>
    <p class="cue"><span class="time">[45:53]</span>Yes right now it&#x27;s manual, but I&#x27;ll</p>
    <p class="cue"><span class="time">[45:54]</span>explain some tricks we can use.</p>
    <p class="cue"><span class="time">[45:56]</span>Yeah.</p>
    <p class="cue"><span class="time">[45:57]</span>Is there a tradeoff where you spend a lot more time</p>
    <p class="cue"><span class="time">[46:02]</span>to label something.</p>
    <p class="cue"><span class="time">[46:04]</span>But then it does do better on the model.</p>
    <p class="cue"><span class="time">[46:06]</span>How do you think that would [? trade off? ?]</p>
    <p class="cue"><span class="time">[46:08]</span>Yeah how do you pick the trade-off</p>
    <p class="cue"><span class="time">[46:10]</span>between the different labeling strategies and not.</p>
    <p class="cue"><span class="time">[46:12]</span>In fact, today you could have a pre-training</p>
    <p class="cue"><span class="time">[46:17]</span>and a post-training that have different labeling schemes.</p>
    <p class="cue"><span class="time">[46:20]</span>The question is for pre-training.</p>
    <p class="cue"><span class="time">[46:24]</span>You want the model to get really good</p>
    <p class="cue"><span class="time">[46:26]</span>and you don&#x27;t want to-- you want to avoid a cold start problem.</p>
    <p class="cue"><span class="time">[46:29]</span>The problem of the cold start is with the first labeling scheme,</p>
    <p class="cue"><span class="time">[46:33]</span>maybe even with 1,000 data points that you collect</p>
    <p class="cue"><span class="time">[46:36]</span>and label manually, it&#x27;s not even going to understand</p>
    <p class="cue"><span class="time">[46:38]</span>anything.</p>
    <p class="cue"><span class="time">[46:39]</span>So you need-- the second labeling</p>
    <p class="cue"><span class="time">[46:42]</span>scheme can be a great way to work around the cold starts.</p>
    <p class="cue"><span class="time">[46:46]</span>You might need less data, but it will start understanding</p>
    <p class="cue"><span class="time">[46:48]</span>what you mean and then the rest of the data</p>
    <p class="cue"><span class="time">[46:50]</span>might be labeled differently, essentially.</p>
    <p class="cue"><span class="time">[46:53]</span>OK, let me talk a little bit about the labeling scheme.</p>
    <p class="cue"><span class="time">[46:55]</span>We&#x27;re actually going to use a slightly different</p>
    <p class="cue"><span class="time">[46:57]</span>labeling scheme.</p>
    <p class="cue"><span class="time">[46:58]</span>And the reason is the one with 1, 1 and only 0</p>
    <p class="cue"><span class="time">[47:04]</span>the risk is you can, actually, have a model that</p>
    <p class="cue"><span class="time">[47:07]</span>performs 99.9% accurate that is, all zeros just predict 0</p>
    <p class="cue"><span class="time">[47:12]</span>all the time.</p>
    <p class="cue"><span class="time">[47:13]</span>It&#x27;s very accurate.</p>
    <p class="cue"><span class="time">[47:14]</span>But the thing is there&#x27;s just one one to find,</p>
    <p class="cue"><span class="time">[47:17]</span>and it&#x27;s very hard to find it.</p>
    <p class="cue"><span class="time">[47:18]</span>And so the network is going to lean toward zeros.</p>
    <p class="cue"><span class="time">[47:23]</span>Meaning the data is-- the labels are so skewed towards 0</p>
    <p class="cue"><span class="time">[47:26]</span>that it&#x27;s going to be hard to find any signal in it.</p>
    <p class="cue"><span class="time">[47:29]</span>And so the trick that deep learning researchers</p>
    <p class="cue"><span class="time">[47:31]</span>use generally is can we actually do</p>
    <p class="cue"><span class="time">[47:34]</span>a little more balanced between the positive</p>
    <p class="cue"><span class="time">[47:37]</span>and the negative labels.</p>
    <p class="cue"><span class="time">[47:39]</span>Just a pure engineering hack.</p>
    <p class="cue"><span class="time">[47:41]</span>Not much science behind it.</p>
    <p class="cue"><span class="time">[47:43]</span>The last activation we&#x27;re going to use a sigmoid.</p>
    <p class="cue"><span class="time">[47:45]</span>But because it&#x27;s a sequential problem,</p>
    <p class="cue"><span class="time">[47:46]</span>we&#x27;re going to use sigmoid in sequence.</p>
    <p class="cue"><span class="time">[47:48]</span>At every time step, there&#x27;s going</p>
    <p class="cue"><span class="time">[47:50]</span>to be a sigmoid activation.</p>
    <p class="cue"><span class="time">[47:52]</span>And then the architecture, we&#x27;re going</p>
    <p class="cue"><span class="time">[47:53]</span>to learn it later in the class, you don&#x27;t need to worry.</p>
    <p class="cue"><span class="time">[47:56]</span>This would be likely an RNN.</p>
    <p class="cue"><span class="time">[47:58]</span>You&#x27;ll learn later in the class what that means.</p>
    <p class="cue"><span class="time">[48:00]</span>And then the loss function-- can someone</p>
    <p class="cue"><span class="time">[48:03]</span>guess what loss function, we would be using?</p>
    <p class="cue"><span class="time">[48:06]</span>Yes yeah.</p>
    <p class="cue"><span class="time">[48:08]</span>Binary cross-entropy, but we&#x27;re going to use it sequentially.</p>
    <p class="cue"><span class="time">[48:12]</span>Meaning at every step, we&#x27;re going</p>
    <p class="cue"><span class="time">[48:14]</span>to compute that with the sigmoid output.</p>
    <p class="cue"><span class="time">[48:17]</span>Yes.</p>
    <p class="cue"><span class="time">[48:17]</span>Sorry.</p>
    <p class="cue"><span class="time">[48:18]</span>On the output.</p>
    <p class="cue"><span class="time">[48:19]</span>So you just label the activate word as multiple ones.</p>
    <p class="cue"><span class="time">[48:23]</span>Correct.</p>
    <p class="cue"><span class="time">[48:24]</span>You take your 10 second inputs.</p>
    <p class="cue"><span class="time">[48:27]</span>You look at where the positive word activate is</p>
    <p class="cue"><span class="time">[48:30]</span>and you put ones in there when it plays.</p>
    <p class="cue"><span class="time">[48:34]</span>And you force the model to predict, hey, activate</p>
    <p class="cue"><span class="time">[48:37]</span>has been played.</p>
    <p class="cue"><span class="time">[48:38]</span>There is a lot of nuances you see,</p>
    <p class="cue"><span class="time">[48:40]</span>because you actually are going to build this project</p>
    <p class="cue"><span class="time">[48:42]</span>with do we want the ones to start</p>
    <p class="cue"><span class="time">[48:44]</span>exactly when the words start.</p>
    <p class="cue"><span class="time">[48:45]</span>Do we want to have a delay?</p>
    <p class="cue"><span class="time">[48:47]</span>There&#x27;s a lot of technical questions around that.</p>
    <p class="cue"><span class="time">[48:49]</span>Yeah.</p>
    <p class="cue"><span class="time">[48:49]</span>The other question.</p>
    <p class="cue"><span class="time">[48:50]</span>I was wondering what do you mean by steps.</p>
    <p class="cue"><span class="time">[48:51]</span>Are they like [INAUDIBLE]?</p>
    <p class="cue"><span class="time">[48:54]</span>What I mean by steps is audio is sequence data.</p>
    <p class="cue"><span class="time">[48:59]</span>And so it&#x27;s time step by time step.</p>
    <p class="cue"><span class="time">[49:02]</span>And so what I mean is that at every time step, whatever</p>
    <p class="cue"><span class="time">[49:05]</span>your sample rate is, you&#x27;re going</p>
    <p class="cue"><span class="time">[49:07]</span>to have a prediction at every step.</p>
    <p class="cue"><span class="time">[49:12]</span>So what is critical to the success of this project</p>
    <p class="cue"><span class="time">[49:15]</span>is really the labeling strategy.</p>
    <p class="cue"><span class="time">[49:18]</span>Here is how we did it.</p>
    <p class="cue"><span class="time">[49:21]</span>And you just have to it or not know it.</p>
    <p class="cue"><span class="time">[49:25]</span>It actually takes a long time to figure something like that out.</p>
    <p class="cue"><span class="time">[49:28]</span>And so thankfully, when I was a grad student,</p>
    <p class="cue"><span class="time">[49:31]</span>one of my senior PhDs helped me with these methods,</p>
    <p class="cue"><span class="time">[49:35]</span>and he was able to guide me and saved me probably a month</p>
    <p class="cue"><span class="time">[49:38]</span>because I would not have figured out myself this probably.</p>
    <p class="cue"><span class="time">[49:41]</span>So here&#x27;s what we did.</p>
    <p class="cue"><span class="time">[49:43]</span>We took three databases.</p>
    <p class="cue"><span class="time">[49:45]</span>We created three databases, one that would have positive words.</p>
    <p class="cue"><span class="time">[49:49]</span>So as you were saying earlier, we</p>
    <p class="cue"><span class="time">[49:51]</span>record people for the word activate.</p>
    <p class="cue"><span class="time">[49:53]</span>One that would have negative words, other words including</p>
    <p class="cue"><span class="time">[49:57]</span>deactivate, but also kitchen and lion and dog and whatever.</p>
    <p class="cue"><span class="time">[50:02]</span>And then we have background noise.</p>
    <p class="cue"><span class="time">[50:05]</span>Turns out background noise in audio data is almost free.</p>
    <p class="cue"><span class="time">[50:08]</span>There&#x27;s just a ton of background noise online.</p>
    <p class="cue"><span class="time">[50:11]</span>You can go on many platforms, online video platforms.</p>
    <p class="cue"><span class="time">[50:14]</span>You can just take the audio.</p>
    <p class="cue"><span class="time">[50:15]</span>It will be background noise.</p>
    <p class="cue"><span class="time">[50:17]</span>So background noise is free.</p>
    <p class="cue"><span class="time">[50:18]</span>You don&#x27;t need to go and collect it, most likely.</p>
    <p class="cue"><span class="time">[50:21]</span>The other two, though, are harder.</p>
    <p class="cue"><span class="time">[50:23]</span>Yeah question.</p>
    <p class="cue"><span class="time">[50:24]</span>Did you record those first two yourself.</p>
    <p class="cue"><span class="time">[50:26]</span>Yeah.</p>
    <p class="cue"><span class="time">[50:27]</span>How many?</p>
    <p class="cue"><span class="time">[50:28]</span>So I&#x27;ll tell you--</p>
    <p class="cue"><span class="time">[50:29]</span>I&#x27;ll tell you how we do it in a second.</p>
    <p class="cue"><span class="time">[50:31]</span>But yes, we recorded everything manually.</p>
    <p class="cue"><span class="time">[50:34]</span>What we did is we went online.</p>
    <p class="cue"><span class="time">[50:37]</span>We scraped free license data.</p>
    <p class="cue"><span class="time">[50:40]</span>Actually, it&#x27;s a skill to know the licensing models.</p>
    <p class="cue"><span class="time">[50:42]</span>You&#x27;re going to learn that in the project mentorship</p>
    <p class="cue"><span class="time">[50:45]</span>with the TAs.</p>
    <p class="cue"><span class="time">[50:46]</span>What licensing allows you to do what with data</p>
    <p class="cue"><span class="time">[50:49]</span>it&#x27;s good to know forever.</p>
    <p class="cue"><span class="time">[50:50]</span>You learn it once and then you know.</p>
    <p class="cue"><span class="time">[50:52]</span>What is CC BY, what is CC BY NA, what is the MIT license, what is</p>
    <p class="cue"><span class="time">[50:56]</span>the Apache license, et cetera.</p>
    <p class="cue"><span class="time">[50:58]</span>And so we take 10-second audio clips.</p>
    <p class="cue"><span class="time">[51:00]</span>We CLIP the background noise.</p>
    <p class="cue"><span class="time">[51:02]</span>And we went around campus and we literally</p>
    <p class="cue"><span class="time">[51:04]</span>recorded people saying activate and other words.</p>
    <p class="cue"><span class="time">[51:07]</span>And we cut it when they said it.</p>
    <p class="cue"><span class="time">[51:10]</span>So the word was contained exactly to the amount</p>
    <p class="cue"><span class="time">[51:13]</span>that it was needed.</p>
    <p class="cue"><span class="time">[51:14]</span>Then we created a Python script that randomly inserts words,</p>
    <p class="cue"><span class="time">[51:17]</span>randomly.</p>
    <p class="cue"><span class="time">[51:18]</span>Non-overlapping words.</p>
    <p class="cue"><span class="time">[51:20]</span>So for example, this would be created synthetically.</p>
    <p class="cue"><span class="time">[51:23]</span>I would have tens of background noise.</p>
    <p class="cue"><span class="time">[51:25]</span>I would insert two negative words,</p>
    <p class="cue"><span class="time">[51:27]</span>and I would insert one positive word, OK.</p>
    <p class="cue"><span class="time">[51:30]</span>The trick is that because the Python script</p>
    <p class="cue"><span class="time">[51:33]</span>did that, the Python script knows where activate was put</p>
    <p class="cue"><span class="time">[51:38]</span>so it can label automatically.</p>
    <p class="cue"><span class="time">[51:40]</span>And so it turns out that we went around campus,</p>
    <p class="cue"><span class="time">[51:43]</span>we used an app even to get help.</p>
    <p class="cue"><span class="time">[51:45]</span>So we hired a couple of people to come with us.</p>
    <p class="cue"><span class="time">[51:48]</span>Each brought their phone, and we went all</p>
    <p class="cue"><span class="time">[51:49]</span>around campus recording people to say</p>
    <p class="cue"><span class="time">[51:51]</span>activate, and other words.</p>
    <p class="cue"><span class="time">[51:52]</span>And we created those data sets.</p>
    <p class="cue"><span class="time">[51:54]</span>Within three hours, we had millions of data points.</p>
    <p class="cue"><span class="time">[51:59]</span>Because think about it.</p>
    <p class="cue"><span class="time">[52:00]</span>Let&#x27;s say you have 1,000 activates across campus,</p>
    <p class="cue"><span class="time">[52:06]</span>10,000 other words, infinite background noise.</p>
    <p class="cue"><span class="time">[52:09]</span>Imagine how much data you can create with that.</p>
    <p class="cue"><span class="time">[52:11]</span>When you actually write the Python script,</p>
    <p class="cue"><span class="time">[52:13]</span>you can also add some data augmentation.</p>
    <p class="cue"><span class="time">[52:15]</span>You can reduce some frequencies.</p>
    <p class="cue"><span class="time">[52:16]</span>You can augment some frequencies,</p>
    <p class="cue"><span class="time">[52:18]</span>you can accelerate it, you can decelerate it.</p>
    <p class="cue"><span class="time">[52:20]</span>So you can actually create a pretty meaningful data sets</p>
    <p class="cue"><span class="time">[52:23]</span>for this problem in three hours.</p>
    <p class="cue"><span class="time">[52:28]</span>Now I&#x27;m talking about training sets</p>
    <p class="cue"><span class="time">[52:31]</span>because you don&#x27;t want to use that data for test sets.</p>
    <p class="cue"><span class="time">[52:34]</span>For test sets, you want data to be as real as possible.</p>
    <p class="cue"><span class="time">[52:37]</span>You&#x27;re familiar with the concept of train and test set.</p>
    <p class="cue"><span class="time">[52:39]</span>So for the test set, we had to manually label data,</p>
    <p class="cue"><span class="time">[52:42]</span>but it was a much smaller set than the training set.</p>
    <p class="cue"><span class="time">[52:45]</span>So it&#x27;s much more convenient.</p>
    <p class="cue"><span class="time">[52:47]</span>The second important part was architecture search.</p>
    <p class="cue"><span class="time">[52:51]</span>I&#x27;m not going to talk about it too much here,</p>
    <p class="cue"><span class="time">[52:53]</span>but there are architectures that just work better</p>
    <p class="cue"><span class="time">[52:56]</span>for these types of problems.</p>
    <p class="cue"><span class="time">[52:58]</span>And this is an example of an architecture,</p>
    <p class="cue"><span class="time">[53:01]</span>on the right, that works way better.</p>
    <p class="cue"><span class="time">[53:04]</span>And my learning from that project</p>
    <p class="cue"><span class="time">[53:07]</span>was just go to the expert and ask them what they&#x27;ve tried</p>
    <p class="cue"><span class="time">[53:10]</span>and try to learn from their mistakes.</p>
    <p class="cue"><span class="time">[53:12]</span>And in fact, I remember Ani Hainan,</p>
    <p class="cue"><span class="time">[53:14]</span>which was in the first level at the gates computer science</p>
    <p class="cue"><span class="time">[53:17]</span>building, and he knew that this architecture was going to fail.</p>
    <p class="cue"><span class="time">[53:21]</span>And he knew why, because he&#x27;s done so many speech projects,</p>
    <p class="cue"><span class="time">[53:24]</span>and he just knows what works and what doesn&#x27;t work.</p>
    <p class="cue"><span class="time">[53:27]</span>So that&#x27;s why your TAs are here for actually in your projects.</p>
    <p class="cue"><span class="time">[53:31]</span>So you should give them a call, and say,</p>
    <p class="cue"><span class="time">[53:33]</span>hey, is what I&#x27;m doing good or give me</p>
    <p class="cue"><span class="time">[53:36]</span>a pointer for what I might spend my next week doing.</p>
    <p class="cue"><span class="time">[53:41]</span>OK.</p>
    <p class="cue"><span class="time">[53:41]</span>So learnings from this section, this case study.</p>
    <p class="cue"><span class="time">[53:44]</span>Data collection strategy is extremely important,</p>
    <p class="cue"><span class="time">[53:47]</span>including the data labeling strategy</p>
    <p class="cue"><span class="time">[53:49]</span>using human experiments matters as well,</p>
    <p class="cue"><span class="time">[53:53]</span>and then referring to expert advice.</p>
    <p class="cue"><span class="time">[53:55]</span>That&#x27;s the type of thing you want to do in a project.</p>
    <p class="cue"><span class="time">[54:01]</span>Do you understand conceptually how such project</p>
    <p class="cue"><span class="time">[54:03]</span>is built now at a high level?</p>
    <p class="cue"><span class="time">[54:04]</span>OK, good.</p>
    <p class="cue"><span class="time">[54:06]</span>Yeah question.</p>
    <p class="cue"><span class="time">[54:08]</span>At the application layer, how often</p>
    <p class="cue"><span class="time">[54:10]</span>do you have to create your own architecture?</p>
    <p class="cue"><span class="time">[54:13]</span>How often it&#x27;s just like top the shelf [INAUDIBLE]?</p>
    <p class="cue"><span class="time">[54:17]</span>So how often do you need to do an architecture search nowadays?</p>
    <p class="cue"><span class="time">[54:23]</span>Less often than before.</p>
    <p class="cue"><span class="time">[54:24]</span>But this class is all about understanding</p>
    <p class="cue"><span class="time">[54:26]</span>what&#x27;s going on under the hood.</p>
    <p class="cue"><span class="time">[54:27]</span>So we&#x27;re going to walk you through that.</p>
    <p class="cue"><span class="time">[54:29]</span>In practice, it depends on your problem.</p>
    <p class="cue"><span class="time">[54:33]</span>I&#x27;ll give you in the industry you</p>
    <p class="cue"><span class="time">[54:35]</span>might be building a company that requires the model</p>
    <p class="cue"><span class="time">[54:38]</span>to be running on the browser.</p>
    <p class="cue"><span class="time">[54:39]</span>And so you have additional constraints that</p>
    <p class="cue"><span class="time">[54:41]</span>push you to create your own architecture,</p>
    <p class="cue"><span class="time">[54:43]</span>collect your own data fine tune the model the way you want.</p>
    <p class="cue"><span class="time">[54:46]</span>For many startups out there and companies out there,</p>
    <p class="cue"><span class="time">[54:50]</span>you&#x27;re going to start from a foundation model.</p>
    <p class="cue"><span class="time">[54:52]</span>You&#x27;re going to start from a foundation model,</p>
    <p class="cue"><span class="time">[54:54]</span>and then you might, actually, quantize it</p>
    <p class="cue"><span class="time">[54:56]</span>or prune it or modify it to meet your needs in terms of latency,</p>
    <p class="cue"><span class="time">[55:00]</span>in terms of memory, in terms of hardware capacity.</p>
    <p class="cue"><span class="time">[55:03]</span>And knowing what&#x27;s going on we did is important in those cases.</p>
    <p class="cue"><span class="time">[55:06]</span>Yeah.</p>
    <p class="cue"><span class="time">[55:08]</span>OK.</p>
    <p class="cue"><span class="time">[55:09]</span>Super.</p>
    <p class="cue"><span class="time">[55:10]</span>So we&#x27;re one hour in.</p>
    <p class="cue"><span class="time">[55:12]</span>We are about halfway through the class, a little bit ahead,</p>
    <p class="cue"><span class="time">[55:15]</span>and I have a few more case studies to cover with you.</p>
    <p class="cue"><span class="time">[55:19]</span>By the end, we will have a full set of proxy projects</p>
    <p class="cue"><span class="time">[55:24]</span>to work with.</p>
    <p class="cue"><span class="time">[55:25]</span>So this one is cool.</p>
    <p class="cue"><span class="time">[55:27]</span>It&#x27;s face verification.</p>
    <p class="cue"><span class="time">[55:30]</span>A school wants to use face verification</p>
    <p class="cue"><span class="time">[55:32]</span>to validate student IDs in facilities like dining</p>
    <p class="cue"><span class="time">[55:35]</span>halls, gym, and pool.</p>
    <p class="cue"><span class="time">[55:37]</span>So let me explain what it is.</p>
    <p class="cue"><span class="time">[55:39]</span>It&#x27;s like you arrive at the Arriaga gym</p>
    <p class="cue"><span class="time">[55:42]</span>and instead of just normally you would swipe your student ID.</p>
    <p class="cue"><span class="time">[55:47]</span>And your picture will show up on the screen,</p>
    <p class="cue"><span class="time">[55:49]</span>and there&#x27;s someone sitting there.</p>
    <p class="cue"><span class="time">[55:51]</span>It&#x27;s going to compare the picture they&#x27;re</p>
    <p class="cue"><span class="time">[55:53]</span>seeing on the screen to the picture they&#x27;re</p>
    <p class="cue"><span class="time">[55:55]</span>seeing with their eyes.</p>
    <p class="cue"><span class="time">[55:57]</span>And if it&#x27;s the same, they&#x27;re going to say can go ahead.</p>
    <p class="cue"><span class="time">[56:00]</span>This is slightly different.</p>
    <p class="cue"><span class="time">[56:01]</span>Here you&#x27;re going to be verified by a camera.</p>
    <p class="cue"><span class="time">[56:07]</span>So there&#x27;s actually a camera ahead.</p>
    <p class="cue"><span class="time">[56:09]</span>And you&#x27;re walking in and you&#x27;re swiping your card.</p>
    <p class="cue"><span class="time">[56:13]</span>And we&#x27;re going to compare the picture in the database</p>
    <p class="cue"><span class="time">[56:15]</span>to the picture that is being taken by the camera</p>
    <p class="cue"><span class="time">[56:18]</span>to make sure that it&#x27;s the same person.</p>
    <p class="cue"><span class="time">[56:23]</span>How do you get started?</p>
    <p class="cue"><span class="time">[56:33]</span>[INAUDIBLE] students [INAUDIBLE].</p>
    <p class="cue"><span class="time">[56:36]</span>So everybody who got admitted uploaded pictures at least one.</p>
    <p class="cue"><span class="time">[56:41]</span>So we have that already in the database, correct?</p>
    <p class="cue"><span class="time">[56:44]</span>Yeah.</p>
    <p class="cue"><span class="time">[56:44]</span>What else?</p>
    <p class="cue"><span class="time">[56:45]</span>You would probably use cameras that</p>
    <p class="cue"><span class="time">[56:47]</span>are going to be used for detecting fakes later.</p>
    <p class="cue"><span class="time">[56:49]</span>You&#x27;re saying the camera matters?</p>
    <p class="cue"><span class="time">[56:51]</span>Yeah.</p>
    <p class="cue"><span class="time">[56:51]</span>Yeah for sure the camera matters.</p>
    <p class="cue"><span class="time">[56:53]</span>In fact, we talked about resolution.</p>
    <p class="cue"><span class="time">[56:55]</span>You think the resolution is lower or higher than the day</p>
    <p class="cue"><span class="time">[56:58]</span>and night project?</p>
    <p class="cue"><span class="time">[57:00]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[57:01]</span>Probably higher.</p>
    <p class="cue"><span class="time">[57:02]</span>In fact, it&#x27;s going to be higher.</p>
    <p class="cue"><span class="time">[57:03]</span>And again, I would go back to literally doing</p>
    <p class="cue"><span class="time">[57:06]</span>a human experiment and showing pictures of twins</p>
    <p class="cue"><span class="time">[57:09]</span>and asking people if they can differentiate the twins.</p>
    <p class="cue"><span class="time">[57:13]</span>And you&#x27;ll see that the resolution matters, actually.</p>
    <p class="cue"><span class="time">[57:16]</span>OK yeah.</p>
    <p class="cue"><span class="time">[57:16]</span>You wanted to add something.</p>
    <p class="cue"><span class="time">[57:18]</span>Oh, yeah.</p>
    <p class="cue"><span class="time">[57:19]</span>You also need noise.</p>
    <p class="cue"><span class="time">[57:22]</span>Other people that are not used to.</p>
    <p class="cue"><span class="time">[57:24]</span>Also, data from outside the University.</p>
    <p class="cue"><span class="time">[57:26]</span>We could look at the features.</p>
    <p class="cue"><span class="time">[57:29]</span>Which features.</p>
    <p class="cue"><span class="time">[57:30]</span>The person&#x27;s human features that we could compare.</p>
    <p class="cue"><span class="time">[57:33]</span>Understand the person&#x27;s human feature,</p>
    <p class="cue"><span class="time">[57:35]</span>but they&#x27;re already in the picture.</p>
    <p class="cue"><span class="time">[57:36]</span>So the picture would have the feature</p>
    <p class="cue"><span class="time">[57:38]</span>or you would add anything on top of that?</p>
    <p class="cue"><span class="time">[57:42]</span>So you typically would not actually</p>
    <p class="cue"><span class="time">[57:44]</span>want to get the feature level, you</p>
    <p class="cue"><span class="time">[57:46]</span>would just want to say we need to make sure it&#x27;s in the data.</p>
    <p class="cue"><span class="time">[57:49]</span>If it&#x27;s in the data, the neural network will learn it.</p>
    <p class="cue"><span class="time">[57:51]</span>We likely use like the same person multiple</p>
    <p class="cue"><span class="time">[57:53]</span>times [INAUDIBLE].</p>
    <p class="cue"><span class="time">[57:55]</span>Absolutely.</p>
    <p class="cue"><span class="time">[57:56]</span>Same person multiple times because the angle</p>
    <p class="cue"><span class="time">[57:58]</span>matters, the time matters, the-- et cetera.</p>
    <p class="cue"><span class="time">[58:01]</span>Yeah.</p>
    <p class="cue"><span class="time">[58:02]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[58:02]</span>OK.</p>
    <p class="cue"><span class="time">[58:03]</span>Same thing.</p>
    <p class="cue"><span class="time">[58:05]</span>Try to crop the image so that the [INAUDIBLE].</p>
    <p class="cue"><span class="time">[58:08]</span>OK, so you&#x27;re saying we might do some pre-processing</p>
    <p class="cue"><span class="time">[58:10]</span>to crop all the images so that it&#x27;s centered, or at least</p>
    <p class="cue"><span class="time">[58:13]</span>that the image that we&#x27;re training the model with</p>
    <p class="cue"><span class="time">[58:15]</span>looks like the image that the camera is</p>
    <p class="cue"><span class="time">[58:17]</span>going to take because the model will run on the camera.</p>
    <p class="cue"><span class="time">[58:20]</span>All of these are good.</p>
    <p class="cue"><span class="time">[58:22]</span>So let&#x27;s say our data set is picture of every student labeled</p>
    <p class="cue"><span class="time">[58:25]</span>with their name.</p>
    <p class="cue"><span class="time">[58:26]</span>So this is one of my friends Bertrand.</p>
    <p class="cue"><span class="time">[58:28]</span>And he has his picture, which is his picture from the student ID.</p>
    <p class="cue"><span class="time">[58:32]</span>And then the input is--</p>
    <p class="cue"><span class="time">[58:33]</span>OK.</p>
    <p class="cue"><span class="time">[58:33]</span>He shows up in front of the building.</p>
    <p class="cue"><span class="time">[58:36]</span>He&#x27;s a little bit confused.</p>
    <p class="cue"><span class="time">[58:37]</span>But he showed up and a picture was taken.</p>
    <p class="cue"><span class="time">[58:41]</span>The resolution, we talked about it.</p>
    <p class="cue"><span class="time">[58:43]</span>What we use here is 412, 412, by 3.</p>
    <p class="cue"><span class="time">[58:45]</span>It&#x27;s much higher than before, OK,</p>
    <p class="cue"><span class="time">[58:48]</span>as we were expecting because we need small details.</p>
    <p class="cue"><span class="time">[58:51]</span>Even eye color is identifiable.</p>
    <p class="cue"><span class="time">[58:53]</span>So these things we cannot find without a higher resolution.</p>
    <p class="cue"><span class="time">[58:57]</span>In fact, if you actually go through airport security and you</p>
    <p class="cue"><span class="time">[59:02]</span>use some of these FasTracks which take a picture of you.</p>
    <p class="cue"><span class="time">[59:06]</span>Trust me, the resolution is going</p>
    <p class="cue"><span class="time">[59:08]</span>to be even higher than that.</p>
    <p class="cue"><span class="time">[59:09]</span>Much higher than that because they&#x27;re actually getting</p>
    <p class="cue"><span class="time">[59:11]</span>into the iris at that level.</p>
    <p class="cue"><span class="time">[59:14]</span>So what&#x27;s the output?</p>
    <p class="cue"><span class="time">[59:15]</span>The output is 0 or 1.</p>
    <p class="cue"><span class="time">[59:17]</span>Yeah it&#x27;s Bertrand or it&#x27;s not Bertrand.</p>
    <p class="cue"><span class="time">[59:20]</span>We&#x27;re good so far?</p>
    <p class="cue"><span class="time">[59:23]</span>The architecture.</p>
    <p class="cue"><span class="time">[59:27]</span>Let me actually ask how would you do this comparison</p>
    <p class="cue"><span class="time">[59:33]</span>without neural networks?</p>
    <p class="cue"><span class="time">[59:35]</span>Let&#x27;s say a very basic way.</p>
    <p class="cue"><span class="time">[59:37]</span>If you have to start with the first method.</p>
    <p class="cue"><span class="time">[59:40]</span>Yeah.</p>
    <p class="cue"><span class="time">[59:42]</span>We have like average list of characteristics</p>
    <p class="cue"><span class="time">[59:45]</span>that [INAUDIBLE] applies.</p>
    <p class="cue"><span class="time">[59:48]</span>So you would feature engineer.</p>
    <p class="cue"><span class="time">[59:49]</span>You would say, for example--</p>
    <p class="cue"><span class="time">[59:51]</span>you would define 10 features that are</p>
    <p class="cue"><span class="time">[59:53]</span>good for identifying people.</p>
    <p class="cue"><span class="time">[59:54]</span>And you would have a filter for each of them</p>
    <p class="cue"><span class="time">[59:56]</span>and you would run it on the picture and say,</p>
    <p class="cue"><span class="time">[59:58]</span>yes do we have this feature or not, essentially.</p>
    <p class="cue"><span class="time">[60:01]</span>OK.</p>
    <p class="cue"><span class="time">[60:02]</span>Yeah, it&#x27;s a good one.</p>
    <p class="cue"><span class="time">[60:03]</span>Even more basic than that would be a pixel comparison.</p>
    <p class="cue"><span class="time">[60:06]</span>Just compare the two pixels.</p>
    <p class="cue"><span class="time">[60:08]</span>What&#x27;s the problem with doing a pixel comparison?</p>
    <p class="cue"><span class="time">[60:12]</span>So the idea is I take the two pictures, I compare them,</p>
    <p class="cue"><span class="time">[60:16]</span>and if they&#x27;re close enough in pixel-wise comparison,</p>
    <p class="cue"><span class="time">[60:19]</span>then it&#x27;s the same person, if they&#x27;re far,</p>
    <p class="cue"><span class="time">[60:22]</span>it&#x27;s not the same person.</p>
    <p class="cue"><span class="time">[60:24]</span>What can go wrong?</p>
    <p class="cue"><span class="time">[60:26]</span>Difference in lighting.</p>
    <p class="cue"><span class="time">[60:27]</span>Difference in what?</p>
    <p class="cue"><span class="time">[60:28]</span>Lighting.</p>
    <p class="cue"><span class="time">[60:29]</span>Lighting.</p>
    <p class="cue"><span class="time">[60:29]</span>Yeah actually, what&#x27;s interesting</p>
    <p class="cue"><span class="time">[60:31]</span>with the lighting is if you look here.</p>
    <p class="cue"><span class="time">[60:33]</span>So in this one you take the top left pixel right here, OK.</p>
    <p class="cue"><span class="time">[60:38]</span>It&#x27;s bright.</p>
    <p class="cue"><span class="time">[60:40]</span>You take the top left pixel on this one,</p>
    <p class="cue"><span class="time">[60:43]</span>it&#x27;s dark or at least dark green.</p>
    <p class="cue"><span class="time">[60:46]</span>The difference between these two pixels is massive.</p>
    <p class="cue"><span class="time">[60:48]</span>It&#x27;s close to 255 yet the pixel doesn&#x27;t even matter.</p>
    <p class="cue"><span class="time">[60:52]</span>So why would you use that?</p>
    <p class="cue"><span class="time">[60:54]</span>It would penalize the comparison without actually</p>
    <p class="cue"><span class="time">[60:56]</span>mattering at all.</p>
    <p class="cue"><span class="time">[60:57]</span>So that&#x27;s a good point.</p>
    <p class="cue"><span class="time">[60:58]</span>Yeah.</p>
    <p class="cue"><span class="time">[61:02]</span>Background noise can be [INAUDIBLE].</p>
    <p class="cue"><span class="time">[61:06]</span>Yeah, absolutely background difference.</p>
    <p class="cue"><span class="time">[61:08]</span>Translation invariance.</p>
    <p class="cue"><span class="time">[61:10]</span>Imagine the same picture, but the person</p>
    <p class="cue"><span class="time">[61:13]</span>is like three pixels to the right.</p>
    <p class="cue"><span class="time">[61:16]</span>The comparison will be completely different</p>
    <p class="cue"><span class="time">[61:18]</span>because it&#x27;s a pixel comparison rather than a semantically</p>
    <p class="cue"><span class="time">[61:21]</span>meaningful comparison.</p>
    <p class="cue"><span class="time">[61:22]</span>What are other things that can go wrong?</p>
    <p class="cue"><span class="time">[61:26]</span>The distance from the camera.</p>
    <p class="cue"><span class="time">[61:27]</span>Distance from the camera.</p>
    <p class="cue"><span class="time">[61:28]</span>Again rotation invariance, translation invariance,</p>
    <p class="cue"><span class="time">[61:32]</span>scale invariance, all of these matter.</p>
    <p class="cue"><span class="time">[61:34]</span>What are other things that are not geometric modifications?</p>
    <p class="cue"><span class="time">[61:39]</span>Yeah.</p>
    <p class="cue"><span class="time">[61:39]</span>Wearing glasses or a hat.</p>
    <p class="cue"><span class="time">[61:40]</span>Wearing glasses or hats.</p>
    <p class="cue"><span class="time">[61:42]</span>What else?</p>
    <p class="cue"><span class="time">[61:45]</span>Hairstyle.</p>
    <p class="cue"><span class="time">[61:46]</span>Yeah.</p>
    <p class="cue"><span class="time">[61:47]</span>The big beard.</p>
    <p class="cue"><span class="time">[61:49]</span>And in fact, you look here he has much more</p>
    <p class="cue"><span class="time">[61:51]</span>beard than on the picture.</p>
    <p class="cue"><span class="time">[61:53]</span>And he was much younger, by the way, on the first picture.</p>
    <p class="cue"><span class="time">[61:56]</span>And often time we look-- were younger on our student IDs</p>
    <p class="cue"><span class="time">[62:01]</span>than we actually are in person, and that may make a difference.</p>
    <p class="cue"><span class="time">[62:04]</span>OK, so these issues we all talked about.</p>
    <p class="cue"><span class="time">[62:07]</span>I think people get it.</p>
    <p class="cue"><span class="time">[62:09]</span>Good.</p>
    <p class="cue"><span class="time">[62:10]</span>So our solution is to use the concept of encoding.</p>
    <p class="cue"><span class="time">[62:14]</span>Remember what we talked about earlier with the face example?</p>
    <p class="cue"><span class="time">[62:17]</span>It turns out a network that has been trained</p>
    <p class="cue"><span class="time">[62:19]</span>to understand faces should have meaningful information</p>
    <p class="cue"><span class="time">[62:22]</span>in those layers that you can use as comparison</p>
    <p class="cue"><span class="time">[62:25]</span>that is more meaningful than a pixel comparison.</p>
    <p class="cue"><span class="time">[62:27]</span>So this is how it goes.</p>
    <p class="cue"><span class="time">[62:29]</span>We have Bertrand picture from the student ID</p>
    <p class="cue"><span class="time">[62:32]</span>we run it through a deep neural network and we get a vector.</p>
    <p class="cue"><span class="time">[62:38]</span>What is this vector?</p>
    <p class="cue"><span class="time">[62:39]</span>It&#x27;s a vector that we grab in the middle of the network</p>
    <p class="cue"><span class="time">[62:41]</span>somewhere.</p>
    <p class="cue"><span class="time">[62:42]</span>Remember what I said if the vector is</p>
    <p class="cue"><span class="time">[62:45]</span>taken earlier in the network we&#x27;re going</p>
    <p class="cue"><span class="time">[62:47]</span>to get lower level features.</p>
    <p class="cue"><span class="time">[62:49]</span>If you go deeper, it&#x27;s going to get more facial features.</p>
    <p class="cue"><span class="time">[62:51]</span>So you probably go slightly deeper.</p>
    <p class="cue"><span class="time">[62:53]</span>You go much deeper actually for this example.</p>
    <p class="cue"><span class="time">[62:55]</span>And then same thing.</p>
    <p class="cue"><span class="time">[62:56]</span>You run the exact same network on the picture from the camera,</p>
    <p class="cue"><span class="time">[63:01]</span>and normally if the network was trained well,</p>
    <p class="cue"><span class="time">[63:04]</span>those two vectors should be close to each other.</p>
    <p class="cue"><span class="time">[63:08]</span>Distance is 0.4.</p>
    <p class="cue"><span class="time">[63:09]</span>You set the threshold.</p>
    <p class="cue"><span class="time">[63:11]</span>You might do a little study to see what&#x27;s the right threshold.</p>
    <p class="cue"><span class="time">[63:14]</span>And of course, this threshold is going</p>
    <p class="cue"><span class="time">[63:15]</span>to determine the number of true positives</p>
    <p class="cue"><span class="time">[63:17]</span>that you&#x27;re going to get versus false positives</p>
    <p class="cue"><span class="time">[63:19]</span>versus false negatives.</p>
    <p class="cue"><span class="time">[63:21]</span>The higher the threshold, the more likely you make a mistake.</p>
    <p class="cue"><span class="time">[63:24]</span>The more relaxed you are.</p>
    <p class="cue"><span class="time">[63:25]</span>So here let&#x27;s say I set a threshold of a 0.5.</p>
    <p class="cue"><span class="time">[63:30]</span>Hey, I&#x27;m confident enough that this is Bertrand.</p>
    <p class="cue"><span class="time">[63:33]</span>And again, airport security might have a much higher lower</p>
    <p class="cue"><span class="time">[63:36]</span>threshold than the dining Hall at Stanford, obviously.</p>
    <p class="cue"><span class="time">[63:42]</span>OK, so this is the general idea behind what we want to do,</p>
    <p class="cue"><span class="time">[63:46]</span>but I still haven&#x27;t talked to you about how</p>
    <p class="cue"><span class="time">[63:48]</span>the network is trained.</p>
    <p class="cue"><span class="time">[63:49]</span>The network right now is not trained.</p>
    <p class="cue"><span class="time">[63:51]</span>We haven&#x27;t learned how to train it.</p>
    <p class="cue"><span class="time">[63:52]</span>Yeah.</p>
    <p class="cue"><span class="time">[63:52]</span>Question.</p>
    <p class="cue"><span class="time">[63:53]</span>Question about the [INAUDIBLE] networking use.</p>
    <p class="cue"><span class="time">[63:55]</span>Is it like just a regular picture classification or as</p>
    <p class="cue"><span class="time">[64:01]</span>a binary output or is it--</p>
    <p class="cue"><span class="time">[64:03]</span>What&#x27;s the network?</p>
    <p class="cue"><span class="time">[64:04]</span>We&#x27;ll learn it, actually.</p>
    <p class="cue"><span class="time">[64:05]</span>We&#x27;re going to see it together.</p>
    <p class="cue"><span class="time">[64:06]</span>Yeah, I&#x27;ll describe it.</p>
    <p class="cue"><span class="time">[64:07]</span>But I don&#x27;t want to get into the architectural nitty gritty,</p>
    <p class="cue"><span class="time">[64:10]</span>I want to focus on the general training scheme</p>
    <p class="cue"><span class="time">[64:13]</span>that we&#x27;re going to use, and then</p>
    <p class="cue"><span class="time">[64:14]</span>you&#x27;re actually going to build that</p>
    <p class="cue"><span class="time">[64:16]</span>at some point in the quarter.</p>
    <p class="cue"><span class="time">[64:17]</span>Yeah other question.</p>
    <p class="cue"><span class="time">[64:18]</span>Yes.</p>
    <p class="cue"><span class="time">[64:19]</span>This 128 dimensional vector, what exactly hidden features</p>
    <p class="cue"><span class="time">[64:23]</span>are there?</p>
    <p class="cue"><span class="time">[64:24]</span>What are the hidden features in the 128 dimensional vector?</p>
    <p class="cue"><span class="time">[64:29]</span>We don&#x27;t know.</p>
    <p class="cue"><span class="time">[64:30]</span>That&#x27;s the point of deep learning</p>
    <p class="cue"><span class="time">[64:32]</span>is you have to create a loss function that</p>
    <p class="cue"><span class="time">[64:36]</span>will modify your parameters in a way that</p>
    <p class="cue"><span class="time">[64:39]</span>forces it to learn features.</p>
    <p class="cue"><span class="time">[64:41]</span>But I can&#x27;t tell you that a dimension number 3 is for eyes,</p>
    <p class="cue"><span class="time">[64:45]</span>and dimension number 6 is for ears.</p>
    <p class="cue"><span class="time">[64:47]</span>I can make a study, we&#x27;ll study it later this quarter,</p>
    <p class="cue"><span class="time">[64:50]</span>and tell you that this neuron is actually</p>
    <p class="cue"><span class="time">[64:54]</span>good at detecting certain types of features,</p>
    <p class="cue"><span class="time">[64:56]</span>but right now, I can&#x27;t tell you unless I do that study.</p>
    <p class="cue"><span class="time">[65:00]</span>Yeah.</p>
    <p class="cue"><span class="time">[65:01]</span>OK.</p>
    <p class="cue"><span class="time">[65:01]</span>So question for you.</p>
    <p class="cue"><span class="time">[65:02]</span>How would you build a training and a loss function</p>
    <p class="cue"><span class="time">[65:06]</span>to make that possible to train that network?</p>
    <p class="cue"><span class="time">[65:08]</span>Do you have ideas?</p>
    <p class="cue"><span class="time">[65:13]</span>It&#x27;s not an easy question.</p>
    <p class="cue"><span class="time">[65:15]</span>OK.</p>
    <p class="cue"><span class="time">[65:16]</span>Try.</p>
    <p class="cue"><span class="time">[65:21]</span>Where to start?</p>
    <p class="cue"><span class="time">[65:24]</span>Yes.</p>
    <p class="cue"><span class="time">[65:25]</span>Supporting two vectors I think mean square error</p>
    <p class="cue"><span class="time">[65:29]</span>is going to be better.</p>
    <p class="cue"><span class="time">[65:30]</span>So mean squared error between what?</p>
    <p class="cue"><span class="time">[65:31]</span>You&#x27;re right, actually.</p>
    <p class="cue"><span class="time">[65:32]</span>Two vectors mean squared error because it&#x27;s a-- yeah.</p>
    <p class="cue"><span class="time">[65:35]</span>But--</p>
    <p class="cue"><span class="time">[65:35]</span>[INAUDIBLE] the loss function probably--</p>
    <p class="cue"><span class="time">[65:42]</span>the cost function probably cross entropy [INAUDIBLE].</p>
    <p class="cue"><span class="time">[65:46]</span>So are you saying we would take pairs of pictures.</p>
    <p class="cue"><span class="time">[65:50]</span>We would run it through the network.</p>
    <p class="cue"><span class="time">[65:52]</span>We will then take the two vectors that we get</p>
    <p class="cue"><span class="time">[65:54]</span>and apply the loss function some distance L1 distance,</p>
    <p class="cue"><span class="time">[65:59]</span>L2 distance, and then trace it back</p>
    <p class="cue"><span class="time">[66:02]</span>and say these were the same people should have been closer.</p>
    <p class="cue"><span class="time">[66:06]</span>That&#x27;s what you mean?</p>
    <p class="cue"><span class="time">[66:07]</span>Yeah it&#x27;s a good idea.</p>
    <p class="cue"><span class="time">[66:08]</span>Yeah someone else wanted to say something.</p>
    <p class="cue"><span class="time">[66:10]</span>I was gonna say it&#x27;s like the cosine similarity of two</p>
    <p class="cue"><span class="time">[66:12]</span>vectors or any other distance [INAUDIBLE].</p>
    <p class="cue"><span class="time">[66:14]</span>Yeah, that&#x27;s another one.</p>
    <p class="cue"><span class="time">[66:15]</span>Cosine similarity.</p>
    <p class="cue"><span class="time">[66:16]</span>That could also be our loss function.</p>
    <p class="cue"><span class="time">[66:18]</span>We could also add some data manipulation.</p>
    <p class="cue"><span class="time">[66:20]</span>OK like what?</p>
    <p class="cue"><span class="time">[66:22]</span>Like create a data set from the original picture</p>
    <p class="cue"><span class="time">[66:25]</span>and play around with the [INAUDIBLE].</p>
    <p class="cue"><span class="time">[66:29]</span>Great idea.</p>
    <p class="cue"><span class="time">[66:30]</span>Data augmentation.</p>
    <p class="cue"><span class="time">[66:31]</span>So you say I can take the picture of Bertrand</p>
    <p class="cue"><span class="time">[66:34]</span>and probably mirror it, flip it, rotate it, crop it</p>
    <p class="cue"><span class="time">[66:40]</span>and I would use more data that way.</p>
    <p class="cue"><span class="time">[66:41]</span>Yeah, absolutely.</p>
    <p class="cue"><span class="time">[66:42]</span>That would help a lot actually.</p>
    <p class="cue"><span class="time">[66:44]</span>So all of these are good ones.</p>
    <p class="cue"><span class="time">[66:47]</span>If I summarize your point though,</p>
    <p class="cue"><span class="time">[66:49]</span>because that&#x27;s really the key to the designing</p>
    <p class="cue"><span class="time">[66:52]</span>a good loss function, what we really</p>
    <p class="cue"><span class="time">[66:55]</span>want is that similar picture of the same person,</p>
    <p class="cue"><span class="time">[66:58]</span>end up with similar vectors and picture of different people</p>
    <p class="cue"><span class="time">[67:02]</span>end up with different vectors if we rephrase it in plain English.</p>
    <p class="cue"><span class="time">[67:07]</span>So what we&#x27;ll do is that we&#x27;ll build a data set of triplets.</p>
    <p class="cue"><span class="time">[67:11]</span>The triplets includes a picture that</p>
    <p class="cue"><span class="time">[67:14]</span>is the anchor, a picture that is the positive.</p>
    <p class="cue"><span class="time">[67:18]</span>The reason it&#x27;s called positive is because it&#x27;s</p>
    <p class="cue"><span class="time">[67:20]</span>the same person as the anchor.</p>
    <p class="cue"><span class="time">[67:21]</span>And a picture that is called the negative because it&#x27;s</p>
    <p class="cue"><span class="time">[67:24]</span>a different person than the anchor, and by definition</p>
    <p class="cue"><span class="time">[67:27]</span>also a different person than the positive.</p>
    <p class="cue"><span class="time">[67:30]</span>Well, what if we only have one picture of a person.</p>
    <p class="cue"><span class="time">[67:32]</span>One picture of a person.</p>
    <p class="cue"><span class="time">[67:34]</span>Great question.</p>
    <p class="cue"><span class="time">[67:35]</span>If you have one picture of a person, then you can&#x27;t do that.</p>
    <p class="cue"><span class="time">[67:39]</span>We&#x27;ll actually see another method</p>
    <p class="cue"><span class="time">[67:41]</span>that would allow us to do it even</p>
    <p class="cue"><span class="time">[67:42]</span>with one picture of a person.</p>
    <p class="cue"><span class="time">[67:44]</span>Yeah.</p>
    <p class="cue"><span class="time">[67:45]</span>You could kind of rotate.</p>
    <p class="cue"><span class="time">[67:47]</span>You can rotate it.</p>
    <p class="cue"><span class="time">[67:47]</span>That&#x27;s true.</p>
    <p class="cue"><span class="time">[67:48]</span>You could actually do some data augmentation,</p>
    <p class="cue"><span class="time">[67:50]</span>as he was mentioning, and build a data</p>
    <p class="cue"><span class="time">[67:52]</span>set starting with one picture.</p>
    <p class="cue"><span class="time">[67:54]</span>But this approach will not be the best one.</p>
    <p class="cue"><span class="time">[67:56]</span>We&#x27;ll see another approach right after that would work better.</p>
    <p class="cue"><span class="time">[67:59]</span>Yeah.</p>
    <p class="cue"><span class="time">[68:00]</span>Why are we comparing like a vector from the model</p>
    <p class="cue"><span class="time">[68:04]</span>from the vector from another model</p>
    <p class="cue"><span class="time">[68:06]</span>instead of just comparing the output to the output?</p>
    <p class="cue"><span class="time">[68:11]</span>A good question.</p>
    <p class="cue"><span class="time">[68:12]</span>Why do we compare a vector rather</p>
    <p class="cue"><span class="time">[68:13]</span>than the output of the model?</p>
    <p class="cue"><span class="time">[68:17]</span>So what&#x27;s the output of the model.</p>
    <p class="cue"><span class="time">[68:19]</span>We actually haven&#x27;t talked about the architecture,</p>
    <p class="cue"><span class="time">[68:21]</span>but I&#x27;m assuming you&#x27;re saying it&#x27;s a binary number.</p>
    <p class="cue"><span class="time">[68:23]</span>It&#x27;s between 0 and 1.</p>
    <p class="cue"><span class="time">[68:24]</span>Because it&#x27;s a single dimension, it cannot hold meaningful</p>
    <p class="cue"><span class="time">[68:28]</span>information.</p>
    <p class="cue"><span class="time">[68:29]</span>So you probably want to have a vector that is big enough,</p>
    <p class="cue"><span class="time">[68:33]</span>where you believe it has enough flexibility</p>
    <p class="cue"><span class="time">[68:36]</span>to hold information that can allow</p>
    <p class="cue"><span class="time">[68:39]</span>us to verify if the same person is on the picture.</p>
    <p class="cue"><span class="time">[68:42]</span>Yeah, essentially.</p>
    <p class="cue"><span class="time">[68:43]</span>OK, I&#x27;m going to move on.</p>
    <p class="cue"><span class="time">[68:44]</span>And if there&#x27;s-- so what we want is to minimize the encoding</p>
    <p class="cue"><span class="time">[68:47]</span>distance between the anchor and the positive,</p>
    <p class="cue"><span class="time">[68:50]</span>and we want to maximize the encoding distance between</p>
    <p class="cue"><span class="time">[68:52]</span>the anchor and the negative.</p>
    <p class="cue"><span class="time">[68:54]</span>So question for you.</p>
    <p class="cue"><span class="time">[68:56]</span>What I&#x27;m going to ask you is to take 10, 15 seconds,</p>
    <p class="cue"><span class="time">[68:59]</span>look at the slide.</p>
    <p class="cue"><span class="time">[69:01]</span>And you&#x27;re going to start voting for A, B or C. By the way enc</p>
    <p class="cue"><span class="time">[69:10]</span>is encoding.</p>
    <p class="cue"><span class="time">[69:11]</span>Is just how I call the vector that we get out of the network.</p>
    <p class="cue"><span class="time">[69:21]</span>A is the anchor, n is the negative and p is the positive.</p>
    <p class="cue"><span class="time">[69:43]</span>So A is the anchor picture N is the negative picture, which</p>
    <p class="cue"><span class="time">[69:49]</span>is different from the anchor, the different person,</p>
    <p class="cue"><span class="time">[69:51]</span>and P is the positive picture, which is the same as the anchor.</p>
    <p class="cue"><span class="time">[69:57]</span>And anchor of A is when you run A through the network,</p>
    <p class="cue"><span class="time">[70:01]</span>you get the vector anchor of A. OK, let&#x27;s look at the results.</p>
    <p class="cue"><span class="time">[70:14]</span>A 47 for A 23 for B 3 for C. So someone who said good job first.</p>
    <p class="cue"><span class="time">[70:25]</span>That is correct.</p>
    <p class="cue"><span class="time">[70:27]</span>Someone who selected A wants to tell us why?</p>
    <p class="cue"><span class="time">[70:30]</span>Yeah.</p>
    <p class="cue"><span class="time">[70:31]</span>Because we&#x27;re to minimize the distance between the anchor</p>
    <p class="cue"><span class="time">[70:35]</span>and the positive.</p>
    <p class="cue"><span class="time">[70:36]</span>You just minimize the distance right there.</p>
    <p class="cue"><span class="time">[70:38]</span>But to maximize the distance between the anchor</p>
    <p class="cue"><span class="time">[70:40]</span>and the negative that&#x27;s the same as minimizing the negative.</p>
    <p class="cue"><span class="time">[70:44]</span>Correct Correct.</p>
    <p class="cue"><span class="time">[70:45]</span>Great so actually the keyword here is minimize.</p>
    <p class="cue"><span class="time">[70:48]</span>If I had said maximize the answer indeed,</p>
    <p class="cue"><span class="time">[70:50]</span>as you say would have been different because here,</p>
    <p class="cue"><span class="time">[70:52]</span>we&#x27;re looking at minimizing the distance between the anchor</p>
    <p class="cue"><span class="time">[70:55]</span>and the positive.</p>
    <p class="cue"><span class="time">[70:56]</span>And in fact minimizing this or maximizing the opposite of it</p>
    <p class="cue"><span class="time">[71:02]</span>that&#x27;s why the answer is a.</p>
    <p class="cue"><span class="time">[71:05]</span>OK good stuff.</p>
    <p class="cue"><span class="time">[71:08]</span>Let&#x27;s keep going.</p>
    <p class="cue"><span class="time">[71:10]</span>So going back to the initial setup, we had a cat</p>
    <p class="cue"><span class="time">[71:15]</span>and we were predicting a binary number.</p>
    <p class="cue"><span class="time">[71:17]</span>Here instead, we have three pictures</p>
    <p class="cue"><span class="time">[71:19]</span>going through the network in parallel</p>
    <p class="cue"><span class="time">[71:21]</span>so you can imagine it&#x27;s batch processing.</p>
    <p class="cue"><span class="time">[71:23]</span>It&#x27;s like the three are going in the same network</p>
    <p class="cue"><span class="time">[71:25]</span>at the same time.</p>
    <p class="cue"><span class="time">[71:26]</span>And then you&#x27;re getting three vectors.</p>
    <p class="cue"><span class="time">[71:28]</span>You&#x27;re computing the loss function OK.</p>
    <p class="cue"><span class="time">[71:32]</span>You&#x27;re doing this loss function we talked about.</p>
    <p class="cue"><span class="time">[71:34]</span>I&#x27;m not going to talk here about the alpha number</p>
    <p class="cue"><span class="time">[71:37]</span>but you&#x27;re going to learn when you build it</p>
    <p class="cue"><span class="time">[71:38]</span>why the alpha number matters.</p>
    <p class="cue"><span class="time">[71:41]</span>Hint is maybe 0 would have been a correct answer if you didn&#x27;t</p>
    <p class="cue"><span class="time">[71:45]</span>have the alpha number, so it would have created</p>
    <p class="cue"><span class="time">[71:48]</span>instability in the model.</p>
    <p class="cue"><span class="time">[71:49]</span>But you do that many, many times.</p>
    <p class="cue"><span class="time">[71:51]</span>You push the parameters to the right or the left.</p>
    <p class="cue"><span class="time">[71:54]</span>And because of the way you created your loss function</p>
    <p class="cue"><span class="time">[71:56]</span>and your data labeling, the way you structured</p>
    <p class="cue"><span class="time">[71:59]</span>your data and the loss function, essentially the model</p>
    <p class="cue"><span class="time">[72:03]</span>is going to learn by itself to create similar encoding</p>
    <p class="cue"><span class="time">[72:07]</span>for pictures that are of the same person,</p>
    <p class="cue"><span class="time">[72:09]</span>and separate encodings for pictures that</p>
    <p class="cue"><span class="time">[72:11]</span>are not from the same person.</p>
    <p class="cue"><span class="time">[72:13]</span>And you didn&#x27;t need to do feature engineering,</p>
    <p class="cue"><span class="time">[72:15]</span>you didn&#x27;t need to talk about eyes and ears</p>
    <p class="cue"><span class="time">[72:16]</span>and whatever because it will figure it out.</p>
    <p class="cue"><span class="time">[72:18]</span>You know that you created the learning environment</p>
    <p class="cue"><span class="time">[72:21]</span>to allow that to happen.</p>
    <p class="cue"><span class="time">[72:25]</span>So congratulations, you designed your first loss function,</p>
    <p class="cue"><span class="time">[72:29]</span>and we&#x27;re going to design many more in this course.</p>
    <p class="cue"><span class="time">[72:33]</span>This, by the way, is from facenet.</p>
    <p class="cue"><span class="time">[72:35]</span>It&#x27;s a paper from 2015 from Schrott et al.</p>
    <p class="cue"><span class="time">[72:38]</span>And you&#x27;ll see in the slides I used,</p>
    <p class="cue"><span class="time">[72:40]</span>I always put the reference to the papers in case</p>
    <p class="cue"><span class="time">[72:44]</span>you want to go back and study the actual paper.</p>
    <p class="cue"><span class="time">[72:46]</span>Many students do it for their projects.</p>
    <p class="cue"><span class="time">[72:47]</span>This is a great one.</p>
    <p class="cue"><span class="time">[72:48]</span>Great, great paper to look.</p>
    <p class="cue"><span class="time">[72:50]</span>A lot of citations as well.</p>
    <p class="cue"><span class="time">[72:53]</span>Let me make it slightly complicated-- more complicated.</p>
    <p class="cue"><span class="time">[72:56]</span>We learned face verification, now</p>
    <p class="cue"><span class="time">[72:58]</span>we want to do face identification.</p>
    <p class="cue"><span class="time">[73:00]</span>How is that different?</p>
    <p class="cue"><span class="time">[73:02]</span>Identification is a school wants to use--</p>
    <p class="cue"><span class="time">[73:07]</span>to recognize students in facilities.</p>
    <p class="cue"><span class="time">[73:09]</span>So imagine face verification is you swipe your card</p>
    <p class="cue"><span class="time">[73:12]</span>and then that picture was compared</p>
    <p class="cue"><span class="time">[73:15]</span>to the picture of the camera.</p>
    <p class="cue"><span class="time">[73:16]</span>That&#x27;s verification.</p>
    <p class="cue"><span class="time">[73:17]</span>The two are they the same or not?</p>
    <p class="cue"><span class="time">[73:19]</span>Identification is you have this picture</p>
    <p class="cue"><span class="time">[73:23]</span>in the database somewhere.</p>
    <p class="cue"><span class="time">[73:25]</span>The person enters immediately you can identify them.</p>
    <p class="cue"><span class="time">[73:28]</span>So the difference for those of you who fly in the US</p>
    <p class="cue"><span class="time">[73:32]</span>is when you go through global entry,</p>
    <p class="cue"><span class="time">[73:37]</span>many people don&#x27;t even need to put their passport or anything.</p>
    <p class="cue"><span class="time">[73:40]</span>They just want to look at the camera and they move on.</p>
    <p class="cue"><span class="time">[73:42]</span>That&#x27;s identification.</p>
    <p class="cue"><span class="time">[73:44]</span>But actually, when you&#x27;re in Europe, for example,</p>
    <p class="cue"><span class="time">[73:47]</span>you put your passport in, then you walk in,</p>
    <p class="cue"><span class="time">[73:50]</span>then it takes a picture, that&#x27;s verification.</p>
    <p class="cue"><span class="time">[73:53]</span>You see the difference or no.</p>
    <p class="cue"><span class="time">[73:54]</span>Yeah.</p>
    <p class="cue"><span class="time">[73:55]</span>You choosing for verification the negative, for the input, how</p>
    <p class="cue"><span class="time">[73:59]</span>would you [INAUDIBLE]?</p>
    <p class="cue"><span class="time">[74:01]</span>The negative or how do you create those triplets</p>
    <p class="cue"><span class="time">[74:04]</span>essentially?</p>
    <p class="cue"><span class="time">[74:05]</span>When you have to do it in real time?</p>
    <p class="cue"><span class="time">[74:06]</span>No in real time-- that&#x27;s a great question.</p>
    <p class="cue"><span class="time">[74:08]</span>I didn&#x27;t talk about it.</p>
    <p class="cue"><span class="time">[74:09]</span>So at train time you have a databases</p>
    <p class="cue"><span class="time">[74:12]</span>and you create the triplets automatically.</p>
    <p class="cue"><span class="time">[74:14]</span>Like you pick pictures from the same person,</p>
    <p class="cue"><span class="time">[74:16]</span>or you use data augmentation, and you add a random picture</p>
    <p class="cue"><span class="time">[74:19]</span>from someone else.</p>
    <p class="cue"><span class="time">[74:20]</span>You create millions of triplets like that,</p>
    <p class="cue"><span class="time">[74:22]</span>or billions of triplets.</p>
    <p class="cue"><span class="time">[74:23]</span>At test time, you only take the picture from the camera,</p>
    <p class="cue"><span class="time">[74:28]</span>run it-- you don&#x27;t use the negative.</p>
    <p class="cue"><span class="time">[74:30]</span>You just take the picture from the camera.</p>
    <p class="cue"><span class="time">[74:32]</span>You run it through the network.</p>
    <p class="cue"><span class="time">[74:34]</span>The person swipes.</p>
    <p class="cue"><span class="time">[74:35]</span>You, take the picture from the swipe,</p>
    <p class="cue"><span class="time">[74:37]</span>run it through the network.</p>
    <p class="cue"><span class="time">[74:38]</span>You do the comparison, you let them in or not.</p>
    <p class="cue"><span class="time">[74:40]</span>So there&#x27;s no more negative at test time in practice.</p>
    <p class="cue"><span class="time">[74:44]</span>It&#x27;s just a trick to train the model.</p>
    <p class="cue"><span class="time">[74:47]</span>OK, so how would you do face identification</p>
    <p class="cue"><span class="time">[74:49]</span>using what we learned for face verification.</p>
    <p class="cue"><span class="time">[74:52]</span>Is there any small tweak you can make</p>
    <p class="cue"><span class="time">[74:54]</span>that would make this network work for identification?</p>
    <p class="cue"><span class="time">[75:03]</span>Yes.</p>
    <p class="cue"><span class="time">[75:04]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[75:11]</span>Correct.</p>
    <p class="cue"><span class="time">[75:12]</span>Correct.</p>
    <p class="cue"><span class="time">[75:13]</span>What is it called in machine learning?</p>
    <p class="cue"><span class="time">[75:18]</span>There&#x27;s a machine learning algorithm</p>
    <p class="cue"><span class="time">[75:20]</span>that we can stack on top of what we just did.</p>
    <p class="cue"><span class="time">[75:23]</span>He said you can compare--</p>
    <p class="cue"><span class="time">[75:25]</span>so because we don&#x27;t have two pictures anymore,</p>
    <p class="cue"><span class="time">[75:27]</span>we just have one from the camera.</p>
    <p class="cue"><span class="time">[75:29]</span>You just compare the vector of-- you run this by the network,</p>
    <p class="cue"><span class="time">[75:33]</span>you get the vector.</p>
    <p class="cue"><span class="time">[75:34]</span>And then you compare it to the database--</p>
    <p class="cue"><span class="time">[75:36]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[75:38]</span>No, but you try.</p>
    <p class="cue"><span class="time">[75:40]</span>You have a database of all the student pictures.</p>
    <p class="cue"><span class="time">[75:42]</span>You run everything through the network.</p>
    <p class="cue"><span class="time">[75:44]</span>Instead of storing the image, you</p>
    <p class="cue"><span class="time">[75:45]</span>store the vectors and then someone shows up</p>
    <p class="cue"><span class="time">[75:49]</span>and you&#x27;re looking in the database.</p>
    <p class="cue"><span class="time">[75:50]</span>Is there any vector that is super close to this one.</p>
    <p class="cue"><span class="time">[75:54]</span>That&#x27;s identification.</p>
    <p class="cue"><span class="time">[75:55]</span>What is this algorithm called in machine learning?</p>
    <p class="cue"><span class="time">[76:00]</span>It&#x27;s pretty simple algorithm.</p>
    <p class="cue"><span class="time">[76:03]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[76:04]</span>No.</p>
    <p class="cue"><span class="time">[76:08]</span>OK I&#x27;m going to make it easier.</p>
    <p class="cue"><span class="time">[76:09]</span>What if instead of having one picture</p>
    <p class="cue"><span class="time">[76:11]</span>of a student in the database, you had three of each student.</p>
    <p class="cue"><span class="time">[76:15]</span>You have three vectors for each person.</p>
    <p class="cue"><span class="time">[76:18]</span>And then you&#x27;re trying to find the nearest</p>
    <p class="cue"><span class="time">[76:20]</span>vectors in the database from the one that the camera takes.</p>
    <p class="cue"><span class="time">[76:25]</span>I used the keyword.</p>
    <p class="cue"><span class="time">[76:26]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[76:27]</span>No.</p>
    <p class="cue"><span class="time">[76:28]</span>K-Nearest neighbor.</p>
    <p class="cue"><span class="time">[76:29]</span>Yeah.</p>
    <p class="cue"><span class="time">[76:31]</span>K-Nearest neighbors.</p>
    <p class="cue"><span class="time">[76:32]</span>That&#x27;s a K-Nearest neighbor algorithm.</p>
    <p class="cue"><span class="time">[76:34]</span>It&#x27;s essentially-- you want to explain what you meant.</p>
    <p class="cue"><span class="time">[76:39]</span>Why is it K-Nearest neighbor?</p>
    <p class="cue"><span class="time">[76:42]</span>Well, I was just thinking about what&#x27;s the nearest vector--</p>
    <p class="cue"><span class="time">[76:45]</span>Yeah.</p>
    <p class="cue"><span class="time">[76:46]</span>--closest to the vector [INAUDIBLE].</p>
    <p class="cue"><span class="time">[76:49]</span>Yeah.</p>
    <p class="cue"><span class="time">[76:49]</span>It&#x27;s K-Nearest neighbor for high dimensional vectors.</p>
    <p class="cue"><span class="time">[76:53]</span>So here is a simple example of K-Nearest neighbor</p>
    <p class="cue"><span class="time">[76:55]</span>per two dimensions.</p>
    <p class="cue"><span class="time">[76:56]</span>In practice, it&#x27;s 128 dimensions so I can&#x27;t put it</p>
    <p class="cue"><span class="time">[76:59]</span>on a slide, of course.</p>
    <p class="cue"><span class="time">[77:00]</span>But let&#x27;s say in green, you have the query point.</p>
    <p class="cue"><span class="time">[77:03]</span>The query point is the camera picture, OK.</p>
    <p class="cue"><span class="time">[77:07]</span>And then you run a nearest neighbor algorithm,</p>
    <p class="cue"><span class="time">[77:11]</span>and you say, are there three vectors in the database that</p>
    <p class="cue"><span class="time">[77:15]</span>are close to this vector.</p>
    <p class="cue"><span class="time">[77:17]</span>And you can add additional checks are these three</p>
    <p class="cue"><span class="time">[77:20]</span>vectors from the same person.</p>
    <p class="cue"><span class="time">[77:22]</span>If they are, then it&#x27;s very likely the person is correct</p>
    <p class="cue"><span class="time">[77:24]</span>because you just prove that the three closest</p>
    <p class="cue"><span class="time">[77:28]</span>vectors in the database are from three the same person</p>
    <p class="cue"><span class="time">[77:31]</span>three times, so it&#x27;s higher likelihood.</p>
    <p class="cue"><span class="time">[77:33]</span>You could even do it for 10 nearest neighbor</p>
    <p class="cue"><span class="time">[77:35]</span>if you want to be really secure.</p>
    <p class="cue"><span class="time">[77:37]</span>Let&#x27;s say you go to the airport every time.</p>
    <p class="cue"><span class="time">[77:39]</span>And every time they take a picture of you,</p>
    <p class="cue"><span class="time">[77:41]</span>and now they can do a 10 nearest neighbor on that search.</p>
    <p class="cue"><span class="time">[77:44]</span>Does that make sense?</p>
    <p class="cue"><span class="time">[77:46]</span>Now it&#x27;s slightly more complicated,</p>
    <p class="cue"><span class="time">[77:48]</span>you want to do face clustering.</p>
    <p class="cue"><span class="time">[77:49]</span>So in your phone, sometimes, it says</p>
    <p class="cue"><span class="time">[77:52]</span>it put automatically all the pictures from your mom</p>
    <p class="cue"><span class="time">[77:55]</span>in one folder and from your dad in another folder.</p>
    <p class="cue"><span class="time">[77:59]</span>How does it do it?</p>
    <p class="cue"><span class="time">[78:01]</span>How could you make a tweak to again, what we created</p>
    <p class="cue"><span class="time">[78:03]</span>or encoding network?</p>
    <p class="cue"><span class="time">[78:05]</span>How can you use that to create that?</p>
    <p class="cue"><span class="time">[78:07]</span>K-Means.</p>
    <p class="cue"><span class="time">[78:09]</span>K-Means yeah, exactly.</p>
    <p class="cue"><span class="time">[78:10]</span>K-Means algorithm, which is an unsupervised learning</p>
    <p class="cue"><span class="time">[78:13]</span>algorithm clustering.</p>
    <p class="cue"><span class="time">[78:14]</span>So you have a bunch of pictures.</p>
    <p class="cue"><span class="time">[78:17]</span>You have vectorized all of them with the network you trained.</p>
    <p class="cue"><span class="time">[78:19]</span>And normally the vectors that are from the same person</p>
    <p class="cue"><span class="time">[78:22]</span>should be clustered around the same place.</p>
    <p class="cue"><span class="time">[78:24]</span>And that&#x27;s very simply how big companies do it on your phone.</p>
    <p class="cue"><span class="time">[78:32]</span>Yes.</p>
    <p class="cue"><span class="time">[78:33]</span>What happens if the person&#x27;s not on the database?</p>
    <p class="cue"><span class="time">[78:35]</span>Yeah so if the person is not in the database,</p>
    <p class="cue"><span class="time">[78:39]</span>then you shouldn&#x27;t find any vector that</p>
    <p class="cue"><span class="time">[78:42]</span>is close to the vector you&#x27;re taking a picture of.</p>
    <p class="cue"><span class="time">[78:44]</span>The closest vector might be above your threshold</p>
    <p class="cue"><span class="time">[78:47]</span>in terms of distance, and you wouldn&#x27;t let that person in.</p>
    <p class="cue"><span class="time">[78:50]</span>Yeah.</p>
    <p class="cue"><span class="time">[78:51]</span>I think that&#x27;s why they make you, sign up for global entry.</p>
    <p class="cue"><span class="time">[78:54]</span>Yeah for sure.</p>
    <p class="cue"><span class="time">[78:55]</span>Yeah, they make you sign up.</p>
    <p class="cue"><span class="time">[78:56]</span>So it&#x27;s interesting because companies know that they</p>
    <p class="cue"><span class="time">[78:59]</span>need to build these algorithms.</p>
    <p class="cue"><span class="time">[79:01]</span>And then some-- the admission process the sign up</p>
    <p class="cue"><span class="time">[79:04]</span>process might include certain data points.</p>
    <p class="cue"><span class="time">[79:06]</span>And now you&#x27;re starting to understand how</p>
    <p class="cue"><span class="time">[79:08]</span>it&#x27;s used in the background.</p>
    <p class="cue"><span class="time">[79:10]</span>OK Let&#x27;s move on.</p>
    <p class="cue"><span class="time">[79:12]</span>Yeah one question.</p>
    <p class="cue"><span class="time">[79:13]</span>Are you comparing every new example with each vector</p>
    <p class="cue"><span class="time">[79:16]</span>or with a centroid?</p>
    <p class="cue"><span class="time">[79:19]</span>Oh, a good question.</p>
    <p class="cue"><span class="time">[79:20]</span>So are you comparing each new picture?</p>
    <p class="cue"><span class="time">[79:23]</span>So I take a picture of my mom with my phone.</p>
    <p class="cue"><span class="time">[79:28]</span>What&#x27;s going to happen?</p>
    <p class="cue"><span class="time">[79:29]</span>This picture is going to likely--</p>
    <p class="cue"><span class="time">[79:30]</span>if you&#x27;re doing clustering is going to be compared</p>
    <p class="cue"><span class="time">[79:33]</span>to the centroid of my mom.</p>
    <p class="cue"><span class="time">[79:34]</span>So the phone keeps probably a centroid of my mom.</p>
    <p class="cue"><span class="time">[79:37]</span>And if it&#x27;s close enough to the centroid over another centroid</p>
    <p class="cue"><span class="time">[79:40]</span>it&#x27;s going to probably put it in that folder, essentially.</p>
    <p class="cue"><span class="time">[79:44]</span>Yeah one more question, and then we&#x27;ll move on.</p>
    <p class="cue"><span class="time">[79:46]</span>Yeah what algorithm do you use to determine</p>
    <p class="cue"><span class="time">[79:48]</span>how many centroids you want?</p>
    <p class="cue"><span class="time">[79:51]</span>Yeah there is-- how do you figure out</p>
    <p class="cue"><span class="time">[79:53]</span>how many centroids you want?</p>
    <p class="cue"><span class="time">[79:54]</span>There is an algorithm.</p>
    <p class="cue"><span class="time">[79:56]</span>You&#x27;ll study it in CS230 not in CS230.</p>
    <p class="cue"><span class="time">[80:04]</span>What we learned here is what an encoder network is.</p>
    <p class="cue"><span class="time">[80:08]</span>We learned about positive anchor negative for the triplet loss.</p>
    <p class="cue"><span class="time">[80:12]</span>The loss I showed you is called the triplet loss because</p>
    <p class="cue"><span class="time">[80:15]</span>of the triplets.</p>
    <p class="cue"><span class="time">[80:16]</span>And then we learned the different variations</p>
    <p class="cue"><span class="time">[80:18]</span>of face verification identification and clustering.</p>
    <p class="cue"><span class="time">[80:20]</span>Now we&#x27;re going to get to an interesting section, which</p>
    <p class="cue"><span class="time">[80:24]</span>is brand new around self-supervised learning.</p>
    <p class="cue"><span class="time">[80:27]</span>So note that everything we did so far, the day and night</p>
    <p class="cue"><span class="time">[80:31]</span>classification, the trigger word detection, and the triplet loss</p>
    <p class="cue"><span class="time">[80:36]</span>were supervised learning.</p>
    <p class="cue"><span class="time">[80:39]</span>We had labels, essentially.</p>
    <p class="cue"><span class="time">[80:41]</span>Day and night is very classic supervised learning.</p>
    <p class="cue"><span class="time">[80:43]</span>You label data with 0 and 1.</p>
    <p class="cue"><span class="time">[80:45]</span>Same for trigger word detection, face verification.</p>
    <p class="cue"><span class="time">[80:47]</span>You can debate-- it can be different.</p>
    <p class="cue"><span class="time">[80:50]</span>But anyway, we focused on supervised learning.</p>
    <p class="cue"><span class="time">[80:53]</span>Now we&#x27;re going to talk about self-supervised learning.</p>
    <p class="cue"><span class="time">[80:57]</span>And my question for you is the following.</p>
    <p class="cue"><span class="time">[81:01]</span>Labeling is expensive.</p>
    <p class="cue"><span class="time">[81:02]</span>We know that.</p>
    <p class="cue"><span class="time">[81:03]</span>So how would you redo what we did with a different approach</p>
    <p class="cue"><span class="time">[81:09]</span>that does not require labels?</p>
    <p class="cue"><span class="time">[81:12]</span>Meaning you remember even in face verification,</p>
    <p class="cue"><span class="time">[81:15]</span>we had the name of the students in the database with their face,</p>
    <p class="cue"><span class="time">[81:20]</span>and we might have multiple pictures of them.</p>
    <p class="cue"><span class="time">[81:22]</span>Let&#x27;s say you don&#x27;t even have that.</p>
    <p class="cue"><span class="time">[81:24]</span>You just have faces in the wild unlabeled.</p>
    <p class="cue"><span class="time">[81:29]</span>How would you do things differently?</p>
    <p class="cue"><span class="time">[81:34]</span>Any idea?</p>
    <p class="cue"><span class="time">[81:34]</span>Yeah.</p>
    <p class="cue"><span class="time">[81:35]</span>So that&#x27;s neural network to decide</p>
    <p class="cue"><span class="time">[81:37]</span>like to point out which images are close to each other</p>
    <p class="cue"><span class="time">[81:40]</span>and [INAUDIBLE].</p>
    <p class="cue"><span class="time">[81:41]</span>OK.</p>
    <p class="cue"><span class="time">[81:42]</span>Let the neural network find the pictures that</p>
    <p class="cue"><span class="time">[81:44]</span>are close to each other, but how would you train that network?</p>
    <p class="cue"><span class="time">[81:47]</span>You&#x27;re starting with a network that doesn&#x27;t do anything</p>
    <p class="cue"><span class="time">[81:49]</span>and you give it an image, it gives you</p>
    <p class="cue"><span class="time">[81:51]</span>a random vector at first.</p>
    <p class="cue"><span class="time">[81:52]</span>So how would you train it?</p>
    <p class="cue"><span class="time">[81:54]</span>Yeah.</p>
    <p class="cue"><span class="time">[81:55]</span>Do some kind of clustering offline,</p>
    <p class="cue"><span class="time">[81:57]</span>and then use that as-- use those centroids as your labels.</p>
    <p class="cue"><span class="time">[82:02]</span>Do some clustering offline.</p>
    <p class="cue"><span class="time">[82:03]</span>But again, my question is the clustering algorithm.</p>
    <p class="cue"><span class="time">[82:05]</span>How is it trained?</p>
    <p class="cue"><span class="time">[82:06]</span>How do you cluster if you don&#x27;t have any encoder network?</p>
    <p class="cue"><span class="time">[82:10]</span>Because the clustering came after we trained</p>
    <p class="cue"><span class="time">[82:12]</span>the encoder network.</p>
    <p class="cue"><span class="time">[82:14]</span>The clustering only worked because we had a good encoder</p>
    <p class="cue"><span class="time">[82:16]</span>network.</p>
    <p class="cue"><span class="time">[82:17]</span>But if you have for example, a bunch</p>
    <p class="cue"><span class="time">[82:20]</span>of pictures of the same person, and then you</p>
    <p class="cue"><span class="time">[82:23]</span>run that through the network.</p>
    <p class="cue"><span class="time">[82:24]</span>But you don&#x27;t know if it&#x27;s the same person.</p>
    <p class="cue"><span class="time">[82:26]</span>That&#x27;s what I&#x27;m saying is like--</p>
    <p class="cue"><span class="time">[82:27]</span>Vectors would be similar.</p>
    <p class="cue"><span class="time">[82:28]</span>No, because that&#x27;s the network you&#x27;re training.</p>
    <p class="cue"><span class="time">[82:30]</span>The vectors are not similar because that&#x27;s</p>
    <p class="cue"><span class="time">[82:32]</span>the network we want to train.</p>
    <p class="cue"><span class="time">[82:33]</span>Right now I gave you a network.</p>
    <p class="cue"><span class="time">[82:36]</span>It&#x27;s completely random.</p>
    <p class="cue"><span class="time">[82:37]</span>You give it my picture on Saturday</p>
    <p class="cue"><span class="time">[82:39]</span>and on Sunday the vectors are completely off.</p>
    <p class="cue"><span class="time">[82:43]</span>So how do you start?</p>
    <p class="cue"><span class="time">[82:44]</span>Yeah, yeah.</p>
    <p class="cue"><span class="time">[82:45]</span>You want it.</p>
    <p class="cue"><span class="time">[82:46]</span>You could do a carbon copy.</p>
    <p class="cue"><span class="time">[82:48]</span>OK, tell me more.</p>
    <p class="cue"><span class="time">[82:50]</span>You could just train the model to [INAUDIBLE] image.</p>
    <p class="cue"><span class="time">[82:55]</span>First create a layer of representation of that.</p>
    <p class="cue"><span class="time">[82:57]</span>And then given the label representation,</p>
    <p class="cue"><span class="time">[83:01]</span>and then another model that given the label representation.</p>
    <p class="cue"><span class="time">[83:04]</span>OK, OK.</p>
    <p class="cue"><span class="time">[83:05]</span>Yeah you&#x27;re ahead.</p>
    <p class="cue"><span class="time">[83:06]</span>But we&#x27;ll study that in two weeks actually.</p>
    <p class="cue"><span class="time">[83:09]</span>So we&#x27;ll do a two encoders two weeks from now in class.</p>
    <p class="cue"><span class="time">[83:14]</span>Anyone else has an idea?</p>
    <p class="cue"><span class="time">[83:15]</span>Yeah.</p>
    <p class="cue"><span class="time">[83:16]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[83:28]</span>It&#x27;s actually similar to what he was saying.</p>
    <p class="cue"><span class="time">[83:31]</span>To reconstruct the original image.</p>
    <p class="cue"><span class="time">[83:33]</span>Yeah, that&#x27;s what we learned.</p>
    <p class="cue"><span class="time">[83:34]</span>There&#x27;s a lot of methods.</p>
    <p class="cue"><span class="time">[83:36]</span>Diffusion models work like that.</p>
    <p class="cue"><span class="time">[83:38]</span>Autoencoders we learn about that in two weeks</p>
    <p class="cue"><span class="time">[83:42]</span>when we focus on generative AI.</p>
    <p class="cue"><span class="time">[83:43]</span>Generative modeling.</p>
    <p class="cue"><span class="time">[83:45]</span>Here I want to present also a generative method,</p>
    <p class="cue"><span class="time">[83:51]</span>but it&#x27;s really interesting because it will be your Foray</p>
    <p class="cue"><span class="time">[83:54]</span>into self-supervised learning.</p>
    <p class="cue"><span class="time">[83:56]</span>Here is the idea.</p>
    <p class="cue"><span class="time">[83:57]</span>If we have pictures in the wild.</p>
    <p class="cue"><span class="time">[83:59]</span>Going with the methods you had mentioned around data</p>
    <p class="cue"><span class="time">[84:01]</span>augmentation, you can actually force the network</p>
    <p class="cue"><span class="time">[84:05]</span>to learn from the data itself.</p>
    <p class="cue"><span class="time">[84:07]</span>So let me give you an example.</p>
    <p class="cue"><span class="time">[84:09]</span>I take-- you look at the picture of this dog</p>
    <p class="cue"><span class="time">[84:12]</span>and you rotate it by 90 degrees.</p>
    <p class="cue"><span class="time">[84:15]</span>It&#x27;s still the same dog.</p>
    <p class="cue"><span class="time">[84:17]</span>A human would say it&#x27;s the same dog.</p>
    <p class="cue"><span class="time">[84:20]</span>What are we using in our brain?</p>
    <p class="cue"><span class="time">[84:21]</span>We&#x27;re using the ability to understand rotation invariance</p>
    <p class="cue"><span class="time">[84:24]</span>and to understand the semantics of the dog.</p>
    <p class="cue"><span class="time">[84:26]</span>And so technically, if you gave those two images to the network,</p>
    <p class="cue"><span class="time">[84:32]</span>you could create a loss function that compares those two pairs</p>
    <p class="cue"><span class="time">[84:35]</span>and has to have vectors that are close to each other.</p>
    <p class="cue"><span class="time">[84:39]</span>Does that make sense?</p>
    <p class="cue"><span class="time">[84:40]</span>The other thing you can do you can do a patch.</p>
    <p class="cue"><span class="time">[84:43]</span>You can literally take an image of a face</p>
    <p class="cue"><span class="time">[84:45]</span>and put a patch on half of the image.</p>
    <p class="cue"><span class="time">[84:49]</span>And then you say--</p>
    <p class="cue"><span class="time">[84:52]</span>and then you do the same thing on the other image.</p>
    <p class="cue"><span class="time">[84:54]</span>You put the other patch, the other half.</p>
    <p class="cue"><span class="time">[84:56]</span>And now you tell the network, these two</p>
    <p class="cue"><span class="time">[84:58]</span>should have the same vector, pretty much.</p>
    <p class="cue"><span class="time">[85:00]</span>So you use your data augmentation</p>
    <p class="cue"><span class="time">[85:04]</span>scheme on massive data sets online to force the network</p>
    <p class="cue"><span class="time">[85:08]</span>to learn from the data itself.</p>
    <p class="cue"><span class="time">[85:12]</span>OK no need to forge triplets, per se.</p>
    <p class="cue"><span class="time">[85:16]</span>You just take a picture.</p>
    <p class="cue"><span class="time">[85:17]</span>You make a variance of it with noise, with rotation,</p>
    <p class="cue"><span class="time">[85:21]</span>with cropping, with translation, with whatever you want.</p>
    <p class="cue"><span class="time">[85:24]</span>And then you put these two in the data set and you say,</p>
    <p class="cue"><span class="time">[85:27]</span>these are two the same person.</p>
    <p class="cue"><span class="time">[85:28]</span>It should have the same vector.</p>
    <p class="cue"><span class="time">[85:30]</span>Does that make sense?</p>
    <p class="cue"><span class="time">[85:31]</span>That&#x27;s why it&#x27;s called self-supervised learning.</p>
    <p class="cue"><span class="time">[85:34]</span>Because you don&#x27;t have labels, you just</p>
    <p class="cue"><span class="time">[85:37]</span>create a learning environment that</p>
    <p class="cue"><span class="time">[85:40]</span>makes the network learn from the patterns of the data directly</p>
    <p class="cue"><span class="time">[85:45]</span>itself.</p>
    <p class="cue"><span class="time">[85:48]</span>So this is an example called SimCLR.</p>
    <p class="cue"><span class="time">[85:51]</span>Again the paper is right there.</p>
    <p class="cue"><span class="time">[85:53]</span>And this shift from supervised triplets,</p>
    <p class="cue"><span class="time">[85:58]</span>facenet which was a paper from 2015 to self-supervised pairs.</p>
    <p class="cue"><span class="time">[86:04]</span>That is why modern models are trained on billions</p>
    <p class="cue"><span class="time">[86:08]</span>of unlabeled images.</p>
    <p class="cue"><span class="time">[86:11]</span>That&#x27;s how we create-- it&#x27;s much simpler when you think about it.</p>
    <p class="cue"><span class="time">[86:14]</span>You can literally write a script and scrape</p>
    <p class="cue"><span class="time">[86:18]</span>and it will label-- auto label the images</p>
    <p class="cue"><span class="time">[86:20]</span>and put them in pairs, do variations,</p>
    <p class="cue"><span class="time">[86:22]</span>and then you&#x27;ll end up with a very powerful pre-trained model,</p>
    <p class="cue"><span class="time">[86:26]</span>much simpler than people think.</p>
    <p class="cue"><span class="time">[86:29]</span>It&#x27;s not that hard at the end of the day.</p>
    <p class="cue"><span class="time">[86:31]</span>Most of the complexities are going to come from compute.</p>
    <p class="cue"><span class="time">[86:36]</span>So this method is called contrastive learning.</p>
    <p class="cue"><span class="time">[86:39]</span>We&#x27;re going to talk about it a little more in two weeks.</p>
    <p class="cue"><span class="time">[86:44]</span>Self-supervision is not only an image thing,</p>
    <p class="cue"><span class="time">[86:47]</span>it&#x27;s also used in other modalities.</p>
    <p class="cue"><span class="time">[86:49]</span>For example in texts the principle is the same.</p>
    <p class="cue"><span class="time">[86:53]</span>You predict what belongs together and you push away what</p>
    <p class="cue"><span class="time">[86:56]</span>doesn&#x27;t.</p>
    <p class="cue"><span class="time">[86:57]</span>That&#x27;s for images.</p>
    <p class="cue"><span class="time">[86:58]</span>And here.</p>
    <p class="cue"><span class="time">[87:00]</span>What the core of GPT, some of you</p>
    <p class="cue"><span class="time">[87:03]</span>probably have heard of that is a method called</p>
    <p class="cue"><span class="time">[87:06]</span>next token prediction.</p>
    <p class="cue"><span class="time">[87:08]</span>We&#x27;re going to learn later about tokens in the class.</p>
    <p class="cue"><span class="time">[87:10]</span>But today, forget about tokens, just think the words.</p>
    <p class="cue"><span class="time">[87:13]</span>We&#x27;re trying to look at a sentence</p>
    <p class="cue"><span class="time">[87:15]</span>and predict the next word.</p>
    <p class="cue"><span class="time">[87:17]</span>Why is this self-supervised learning?</p>
    <p class="cue"><span class="time">[87:20]</span>Because you don&#x27;t label data.</p>
    <p class="cue"><span class="time">[87:21]</span>You just literally grab data from online,</p>
    <p class="cue"><span class="time">[87:24]</span>and you create a scheme that forces the model</p>
    <p class="cue"><span class="time">[87:26]</span>to learn from the patterns of the data</p>
    <p class="cue"><span class="time">[87:28]</span>using self-supervised learning.</p>
    <p class="cue"><span class="time">[87:30]</span>But the self comes from the fact that you didn&#x27;t</p>
    <p class="cue"><span class="time">[87:33]</span>label it manually yourself.</p>
    <p class="cue"><span class="time">[87:36]</span>So let&#x27;s do a few examples.</p>
    <p class="cue"><span class="time">[87:38]</span>And the reason I want to do the examples is because we want</p>
    <p class="cue"><span class="time">[87:41]</span>to talk about emerging emergent behaviors that</p>
    <p class="cue"><span class="time">[87:45]</span>stem from the tasks we defined.</p>
    <p class="cue"><span class="time">[87:47]</span>So give me the word you&#x27;re thinking of.</p>
    <p class="cue"><span class="time">[87:50]</span>I poured myself a cup of--</p>
    <p class="cue"><span class="time">[87:54]</span>some people said tea, coffee.</p>
    <p class="cue"><span class="time">[87:56]</span>Anybody said anything else?</p>
    <p class="cue"><span class="time">[87:58]</span>Water.</p>
    <p class="cue"><span class="time">[87:59]</span>Water, a cup of water.</p>
    <p class="cue"><span class="time">[88:00]</span>Healthy people said water.</p>
    <p class="cue"><span class="time">[88:02]</span>OK yeah.</p>
    <p class="cue"><span class="time">[88:03]</span>So what&#x27;s the emergent behavior that you</p>
    <p class="cue"><span class="time">[88:07]</span>can expect the model is going to learn</p>
    <p class="cue"><span class="time">[88:09]</span>based on just that example?</p>
    <p class="cue"><span class="time">[88:16]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[88:18]</span>A good point, because you that whatever is here first</p>
    <p class="cue"><span class="time">[88:22]</span>fits in a cup, so it understands that.</p>
    <p class="cue"><span class="time">[88:25]</span>The second reason is poured.</p>
    <p class="cue"><span class="time">[88:26]</span>So it&#x27;s a liquid.</p>
    <p class="cue"><span class="time">[88:28]</span>So just this sentence, without even labeling,</p>
    <p class="cue"><span class="time">[88:30]</span>is going to generate emergent behaviors that we&#x27;ve never</p>
    <p class="cue"><span class="time">[88:33]</span>trained the model for.</p>
    <p class="cue"><span class="time">[88:35]</span>That&#x27;s what&#x27;s interesting about modern AI, if you will.</p>
    <p class="cue"><span class="time">[88:40]</span>You don&#x27;t need to define the tasks.</p>
    <p class="cue"><span class="time">[88:45]</span>Frankly, the same way.</p>
    <p class="cue"><span class="time">[88:47]</span>Think about face verification.</p>
    <p class="cue"><span class="time">[88:49]</span>Back in the days, we used to do what I showed you,</p>
    <p class="cue"><span class="time">[88:51]</span>where we would create triplets, and we</p>
    <p class="cue"><span class="time">[88:53]</span>would be very specific about this is for face verification.</p>
    <p class="cue"><span class="time">[88:56]</span>You could actually scrape all the images online</p>
    <p class="cue"><span class="time">[88:59]</span>and do the contrastive learning that I showed you next</p>
    <p class="cue"><span class="time">[89:02]</span>and it will still be good at detecting faces</p>
    <p class="cue"><span class="time">[89:04]</span>without you having even defined that task in the first place,</p>
    <p class="cue"><span class="time">[89:07]</span>just by doing the contrastive prediction.</p>
    <p class="cue"><span class="time">[89:10]</span>OK.</p>
    <p class="cue"><span class="time">[89:11]</span>First example.</p>
    <p class="cue"><span class="time">[89:11]</span>Also, again, I was trying to predict,</p>
    <p class="cue"><span class="time">[89:13]</span>but people usually say different things.</p>
    <p class="cue"><span class="time">[89:15]</span>I think the majority of people think coffee.</p>
    <p class="cue"><span class="time">[89:17]</span>It&#x27;s very cultural.</p>
    <p class="cue"><span class="time">[89:18]</span>You go in another country, it&#x27;s going to be tea for sure.</p>
    <p class="cue"><span class="time">[89:21]</span>And that forces the model to really think</p>
    <p class="cue"><span class="time">[89:26]</span>about everyday co-occurrence pattern like them, being liquid,</p>
    <p class="cue"><span class="time">[89:32]</span>being of a certain size occurring together.</p>
    <p class="cue"><span class="time">[89:34]</span>So for example, there&#x27;s probably a lot of sentences online that</p>
    <p class="cue"><span class="time">[89:37]</span>says pouring a cup of tea and there&#x27;s a lot</p>
    <p class="cue"><span class="time">[89:40]</span>saying pouring a cup of coffee.</p>
    <p class="cue"><span class="time">[89:42]</span>Because of that, the model should</p>
    <p class="cue"><span class="time">[89:43]</span>understand that these two things are probably close to each other</p>
    <p class="cue"><span class="time">[89:46]</span>because their context is similar.</p>
    <p class="cue"><span class="time">[89:48]</span>Second example, the capital of France is--</p>
    <p class="cue"><span class="time">[89:55]</span>Paris.</p>
    <p class="cue"><span class="time">[89:58]</span>Paris, OK.</p>
    <p class="cue"><span class="time">[89:59]</span>What&#x27;s the emergent behavior you can expect the model to learn?</p>
    <p class="cue"><span class="time">[90:05]</span>[INAUDIBLE]</p>
    <p class="cue"><span class="time">[90:06]</span>Yeah.</p>
    <p class="cue"><span class="time">[90:06]</span>Learn about facts, exactly.</p>
    <p class="cue"><span class="time">[90:08]</span>So this is really predicting the next token forces</p>
    <p class="cue"><span class="time">[90:11]</span>the model to encode real-world facts, such as Paris</p>
    <p class="cue"><span class="time">[90:13]</span>being the capital of France.</p>
    <p class="cue"><span class="time">[90:15]</span>Oops, sorry.</p>
    <p class="cue"><span class="time">[90:16]</span>What about the third example?</p>
    <p class="cue"><span class="time">[90:18]</span>She unlocked her phone using her body parts.</p>
    <p class="cue"><span class="time">[90:25]</span>I don&#x27;t what your phone--</p>
    <p class="cue"><span class="time">[90:27]</span>type of phone you have, but wait, what would you say.</p>
    <p class="cue"><span class="time">[90:31]</span>Face.</p>
    <p class="cue"><span class="time">[90:31]</span>Face.</p>
    <p class="cue"><span class="time">[90:32]</span>Password.</p>
    <p class="cue"><span class="time">[90:33]</span>Password.</p>
    <p class="cue"><span class="time">[90:34]</span>Fingerprint.</p>
    <p class="cue"><span class="time">[90:36]</span>Yeah, all of them are possible.</p>
    <p class="cue"><span class="time">[90:38]</span>So again, the network will learn probably</p>
    <p class="cue"><span class="time">[90:40]</span>that password, fingerprint, and face</p>
    <p class="cue"><span class="time">[90:42]</span>can be used to unlock stuff.</p>
    <p class="cue"><span class="time">[90:45]</span>And, in fact, here probably fingerprint or face</p>
    <p class="cue"><span class="time">[90:49]</span>might nowadays be the more common because</p>
    <p class="cue"><span class="time">[90:51]</span>of how the world has evolved.</p>
    <p class="cue"><span class="time">[90:53]</span>But back in the days, it would be password for sure.</p>
    <p class="cue"><span class="time">[90:56]</span>And so these forces semantic understanding</p>
    <p class="cue"><span class="time">[90:59]</span>that these things are probably all meant to unlock information.</p>
    <p class="cue"><span class="time">[91:04]</span>The next one the cat chased the dog or mouse or ball.</p>
    <p class="cue"><span class="time">[91:11]</span>And again, the model we learn probabilistic reasoning.</p>
    <p class="cue"><span class="time">[91:16]</span>Meaning because in the data set it</p>
    <p class="cue"><span class="time">[91:18]</span>will find variations of the sentence</p>
    <p class="cue"><span class="time">[91:20]</span>with different actually conclusion</p>
    <p class="cue"><span class="time">[91:23]</span>it would say that there&#x27;s a lot of things</p>
    <p class="cue"><span class="time">[91:25]</span>that the cat can chase.</p>
    <p class="cue"><span class="time">[91:27]</span>And so that&#x27;s probabilistic reasoning.</p>
    <p class="cue"><span class="time">[91:29]</span>What about the last one?</p>
    <p class="cue"><span class="time">[91:30]</span>If it&#x27;s raining, I should bring an umbrella.</p>
    <p class="cue"><span class="time">[91:35]</span>What&#x27;s the emergent behavior.</p>
    <p class="cue"><span class="time">[91:37]</span>It&#x27;s reasoning and inference is the model</p>
    <p class="cue"><span class="time">[91:40]</span>we learn to connect conditions.</p>
    <p class="cue"><span class="time">[91:42]</span>So for example raining requires you to be protecting yourself</p>
    <p class="cue"><span class="time">[91:46]</span>from the rain with an umbrella.</p>
    <p class="cue"><span class="time">[91:48]</span>That&#x27;s reasoning.</p>
    <p class="cue"><span class="time">[91:51]</span>So long story short emergent behaviors</p>
    <p class="cue"><span class="time">[91:54]</span>are an expected capabilities that</p>
    <p class="cue"><span class="time">[91:57]</span>arise from simple training objectives at scale</p>
    <p class="cue"><span class="time">[91:59]</span>without being explicitly taught or labeled.</p>
    <p class="cue"><span class="time">[92:04]</span>Later in this class, we&#x27;re going to have a full lecture</p>
    <p class="cue"><span class="time">[92:06]</span>on deep reinforcement learning, where we&#x27;re</p>
    <p class="cue"><span class="time">[92:08]</span>going to talk about emergent behaviors in robotics</p>
    <p class="cue"><span class="time">[92:12]</span>or in gaming, where turns out, the agent you&#x27;re training</p>
    <p class="cue"><span class="time">[92:17]</span>learns to do certain strategies that you</p>
    <p class="cue"><span class="time">[92:20]</span>didn&#x27;t expect they would do.</p>
    <p class="cue"><span class="time">[92:21]</span>AlphaGo is a good example, if you&#x27;ve watched the documentary.</p>
    <p class="cue"><span class="time">[92:26]</span>OK.</p>
    <p class="cue"><span class="time">[92:30]</span>Self-supervision is not just about text and images.</p>
    <p class="cue"><span class="time">[92:33]</span>We&#x27;ve seen the next token prediction for GPT,</p>
    <p class="cue"><span class="time">[92:35]</span>and we&#x27;ve also seen contrastive learning for images.</p>
    <p class="cue"><span class="time">[92:38]</span>My question here is the following.</p>
    <p class="cue"><span class="time">[92:41]</span>What other examples of modalities can you think of?</p>
    <p class="cue"><span class="time">[92:48]</span>And tell me the task that you would define.</p>
    <p class="cue"><span class="time">[92:50]</span>Audio.</p>
    <p class="cue"><span class="time">[92:50]</span>So for audio what would you do?</p>
    <p class="cue"><span class="time">[92:57]</span>Audio.</p>
    <p class="cue"><span class="time">[92:58]</span>How would you do a self-supervision in audio?</p>
    <p class="cue"><span class="time">[93:01]</span>Mask out portions of the audio.</p>
    <p class="cue"><span class="time">[93:03]</span>Exactly.</p>
    <p class="cue"><span class="time">[93:04]</span>Mask out portion-- so mask out 20 times steps.</p>
    <p class="cue"><span class="time">[93:08]</span>And because you know what the data was, you have a label.</p>
    <p class="cue"><span class="time">[93:11]</span>You knew what the truth was.</p>
    <p class="cue"><span class="time">[93:12]</span>You can do a self-supervision task.</p>
    <p class="cue"><span class="time">[93:14]</span>It would work great.</p>
    <p class="cue"><span class="time">[93:15]</span>Again, the only limitation is compute and scale.</p>
    <p class="cue"><span class="time">[93:19]</span>What other modalities?</p>
    <p class="cue"><span class="time">[93:22]</span>Maybe self-driving trains with conditions of the--</p>
    <p class="cue"><span class="time">[93:27]</span>Yeah so self-driving is a good example.</p>
    <p class="cue"><span class="time">[93:29]</span>It&#x27;s very multimodal.</p>
    <p class="cue"><span class="time">[93:31]</span>There&#x27;s a lot of different things</p>
    <p class="cue"><span class="time">[93:32]</span>happening in self-driving.</p>
    <p class="cue"><span class="time">[93:33]</span>We&#x27;ll talk about it in a future lecture.</p>
    <p class="cue"><span class="time">[93:35]</span>What else.</p>
    <p class="cue"><span class="time">[93:36]</span>What other modalities can you think of?</p>
    <p class="cue"><span class="time">[93:38]</span>Videos.</p>
    <p class="cue"><span class="time">[93:39]</span>Videos.</p>
    <p class="cue"><span class="time">[93:39]</span>What would you do video?</p>
    <p class="cue"><span class="time">[93:41]</span>Take some portions.</p>
    <p class="cue"><span class="time">[93:42]</span>Take frames out.</p>
    <p class="cue"><span class="time">[93:44]</span>You can take some frames out.</p>
    <p class="cue"><span class="time">[93:45]</span>Same principle as audio.</p>
    <p class="cue"><span class="time">[93:47]</span>Biology.</p>
    <p class="cue"><span class="time">[93:49]</span>Some people work in health biology here.</p>
    <p class="cue"><span class="time">[93:52]</span>Yeah a couple.</p>
    <p class="cue"><span class="time">[93:54]</span>Well you know about amino acids and protein structure.</p>
    <p class="cue"><span class="time">[93:58]</span>You can actually mask portion of the inputs,</p>
    <p class="cue"><span class="time">[94:02]</span>such as the protein structure or DNA, and then complete it,</p>
    <p class="cue"><span class="time">[94:08]</span>and it will force the model to understand those patterns.</p>
    <p class="cue"><span class="time">[94:12]</span>So great stuff in there.</p>
    <p class="cue"><span class="time">[94:14]</span>But the world is very multimodal.</p>
    <p class="cue"><span class="time">[94:16]</span>We experience words, images, sounds, and actions together.</p>
    <p class="cue"><span class="time">[94:19]</span>How can we connect them?</p>
    <p class="cue"><span class="time">[94:21]</span>When you think about multi-modality,</p>
    <p class="cue"><span class="time">[94:25]</span>you want to connect texts and images, let&#x27;s say.</p>
    <p class="cue"><span class="time">[94:29]</span>What do you need to do those?</p>
    <p class="cue"><span class="time">[94:31]</span>You actually need labeled data.</p>
    <p class="cue"><span class="time">[94:33]</span>You need image captions.</p>
    <p class="cue"><span class="time">[94:35]</span>So for example, you have a bunch of pictures</p>
    <p class="cue"><span class="time">[94:37]</span>online of the cat is looking at the camera.</p>
    <p class="cue"><span class="time">[94:40]</span>So there&#x27;s a picture and there&#x27;s a label underneath just like</p>
    <p class="cue"><span class="time">[94:43]</span>on Instagram.</p>
    <p class="cue"><span class="time">[94:43]</span>Let&#x27;s say people put captions, right.</p>
    <p class="cue"><span class="time">[94:46]</span>And the reason you can connect those modalities</p>
    <p class="cue"><span class="time">[94:50]</span>is because of that data set, because you have a lot of that.</p>
    <p class="cue"><span class="time">[94:53]</span>Now this is not typically called supervised learning,</p>
    <p class="cue"><span class="time">[94:56]</span>it will be called weakly supervised</p>
    <p class="cue"><span class="time">[94:58]</span>learning because you&#x27;re not actually labeling images</p>
    <p class="cue"><span class="time">[95:01]</span>with captions, you are benefiting</p>
    <p class="cue"><span class="time">[95:04]</span>from naturally occurring pairings in the world.</p>
    <p class="cue"><span class="time">[95:08]</span>There is naturally occurring pairings of images and texts.</p>
    <p class="cue"><span class="time">[95:12]</span>OK, so now what I want you to do is</p>
    <p class="cue"><span class="time">[95:14]</span>to find other examples that are not just images</p>
    <p class="cue"><span class="time">[95:18]</span>captioned, but naturally-occurring examples</p>
    <p class="cue"><span class="time">[95:21]</span>of different modalities that appear in the wild</p>
    <p class="cue"><span class="time">[95:25]</span>together that we could use to connect modalities.</p>
    <p class="cue"><span class="time">[95:28]</span>The whole point of connecting modalities</p>
    <p class="cue"><span class="time">[95:30]</span>is that our vectors now can represent different modalities</p>
    <p class="cue"><span class="time">[95:35]</span>close to each other in space.</p>
    <p class="cue"><span class="time">[95:39]</span>So think about that.</p>
    <p class="cue"><span class="time">[95:50]</span>Please continue.</p>
    <p class="cue"><span class="time">[95:51]</span>I&#x27;m going to read some of the answers.</p>
    <p class="cue"><span class="time">[95:52]</span>But we keep going.</p>
    <p class="cue"><span class="time">[95:58]</span>So stock price sequence is the single modality.</p>
    <p class="cue"><span class="time">[96:01]</span>You would look at stock price and you can mask and then</p>
    <p class="cue"><span class="time">[96:06]</span>predict.</p>
    <p class="cue"><span class="time">[96:06]</span>But I think maybe what you mean is you would put additional data</p>
    <p class="cue"><span class="time">[96:09]</span>points in there as well.</p>
    <p class="cue"><span class="time">[96:11]</span>Let me see if I have something.</p>
    <p class="cue"><span class="time">[96:12]</span>Audio paired with video.</p>
    <p class="cue"><span class="time">[96:13]</span>Audio and video is a great one.</p>
    <p class="cue"><span class="time">[96:14]</span>Audio and video is naturally paired.</p>
    <p class="cue"><span class="time">[96:16]</span>You take a YouTube video, it has the audio and the video.</p>
    <p class="cue"><span class="time">[96:20]</span>And so when a dog is barking, you</p>
    <p class="cue"><span class="time">[96:24]</span>have the audio of the dog barking</p>
    <p class="cue"><span class="time">[96:25]</span>and the video of the dog barking.</p>
    <p class="cue"><span class="time">[96:26]</span>And so you can create a pairing between those two modalities.</p>
    <p class="cue"><span class="time">[96:31]</span>Transcription so a lot of movies have subtitles.</p>
    <p class="cue"><span class="time">[96:34]</span>And so, by definition, a video stream or a stream of images</p>
    <p class="cue"><span class="time">[96:37]</span>will be naturally connected to text, which would also be</p>
    <p class="cue"><span class="time">[96:41]</span>naturally connected to audio.</p>
    <p class="cue"><span class="time">[96:54]</span>Music and song title.</p>
    <p class="cue"><span class="time">[96:56]</span>Again, that&#x27;s a great one.</p>
    <p class="cue"><span class="time">[96:57]</span>Audio and text are connected.</p>
    <p class="cue"><span class="time">[97:01]</span>Genotype and phenotype.</p>
    <p class="cue"><span class="time">[97:03]</span>Good one as well.</p>
    <p class="cue"><span class="time">[97:08]</span>Medical imaging with ultrasound.</p>
    <p class="cue"><span class="time">[97:11]</span>That&#x27;s a great one.</p>
    <p class="cue"><span class="time">[97:12]</span>Naturally occurring.</p>
    <p class="cue"><span class="time">[97:13]</span>You usually if you go to an ultrasound</p>
    <p class="cue"><span class="time">[97:15]</span>you&#x27;ll have the different types of images</p>
    <p class="cue"><span class="time">[97:17]</span>that occur together naturally.</p>
    <p class="cue"><span class="time">[97:21]</span>Game footage and keyboard action.</p>
    <p class="cue"><span class="time">[97:23]</span>Again another great one.</p>
    <p class="cue"><span class="time">[97:25]</span>So price and area code.</p>
    <p class="cue"><span class="time">[97:27]</span>Good one.</p>
    <p class="cue"><span class="time">[97:30]</span>Great examples.</p>
    <p class="cue"><span class="time">[97:31]</span>Facial expression.</p>
    <p class="cue"><span class="time">[97:32]</span>So TLDR is we have ways to connect modalities.</p>
    <p class="cue"><span class="time">[97:39]</span>Oftentimes some modalities are going to connect very naturally.</p>
    <p class="cue"><span class="time">[97:43]</span>Most things connect to text.</p>
    <p class="cue"><span class="time">[97:46]</span>So that&#x27;s what you want to use as your shared space typically.</p>
    <p class="cue"><span class="time">[97:50]</span>But here it is an example of a paper called image bind.</p>
    <p class="cue"><span class="time">[97:54]</span>And the interesting thing about image bind</p>
    <p class="cue"><span class="time">[97:56]</span>is it says that most things connect</p>
    <p class="cue"><span class="time">[98:02]</span>through a single modality.</p>
    <p class="cue"><span class="time">[98:03]</span>So, for example, thermal data connects to imaging.</p>
    <p class="cue"><span class="time">[98:08]</span>Imaging connects to text.</p>
    <p class="cue"><span class="time">[98:09]</span>So text is going to connect through images</p>
    <p class="cue"><span class="time">[98:12]</span>with thermal data.</p>
    <p class="cue"><span class="time">[98:14]</span>And what&#x27;s the consequence of that</p>
    <p class="cue"><span class="time">[98:16]</span>if I may show you a little example.</p>
    <p class="cue"><span class="time">[98:18]</span>Is that you can--</p>
    <p class="cue"><span class="time">[98:20]</span>this is a demo from Meta called image bind.</p>
    <p class="cue"><span class="time">[98:23]</span>It&#x27;s a cool one.</p>
    <p class="cue"><span class="time">[98:24]</span>You can actually see things occurring together.</p>
    <p class="cue"><span class="time">[98:27]</span>So, for example, you put a text drums.</p>
    <p class="cue"><span class="time">[98:29]</span>And of course, you can get an audio of a drum.</p>
    <p class="cue"><span class="time">[98:33]</span>But you can also see what the closest image in the vector</p>
    <p class="cue"><span class="time">[98:36]</span>space is to that concept.</p>
    <p class="cue"><span class="time">[98:38]</span>So all the spaces are now bound together.</p>
    <p class="cue"><span class="time">[98:41]</span>You can also do audio and image.</p>
    <p class="cue"><span class="time">[98:44]</span>So you give it a dog barking and a picture.</p>
    <p class="cue"><span class="time">[98:49]</span>What can you expect?</p>
    <p class="cue"><span class="time">[98:51]</span>A dog on the beach.</p>
    <p class="cue"><span class="time">[98:52]</span>That&#x27;s the multi-modal embedding.</p>
    <p class="cue"><span class="time">[98:55]</span>The connecting tissue between those different modalities.</p>
    <p class="cue"><span class="time">[98:58]</span>And that&#x27;s probably one of the biggest innovation</p>
    <p class="cue"><span class="time">[99:01]</span>of the last few years connecting those shared spaces.</p>
    <p class="cue"><span class="time">[99:04]</span>OK.</p>
    <p class="cue"><span class="time">[99:05]</span>I&#x27;m not going to cover the full paper,</p>
    <p class="cue"><span class="time">[99:06]</span>but the core insight is that there are shared spaces.</p>
    <p class="cue"><span class="time">[99:11]</span>There are spaces like text and image</p>
    <p class="cue"><span class="time">[99:12]</span>that connect to most modalities that can allow us to connect</p>
    <p class="cue"><span class="time">[99:15]</span>those modalities together.</p>
    <p class="cue"><span class="time">[99:17]</span>We learned a lot of things here embeddings,</p>
    <p class="cue"><span class="time">[99:19]</span>self-supervised learning, contrastive learning, data</p>
    <p class="cue"><span class="time">[99:22]</span>augmentation, next token prediction,</p>
    <p class="cue"><span class="time">[99:24]</span>weakly supervised learning.</p>
    <p class="cue"><span class="time">[99:26]</span>And then the shared embedding stays with the central pivot</p>
    <p class="cue"><span class="time">[99:29]</span>usually being text.</p>
    <p class="cue"><span class="time">[99:30]</span>OK.</p>
    <p class="cue"><span class="time">[99:33]</span>That&#x27;s all for today.</p>
    <p class="cue"><span class="time">[99:34]</span>We&#x27;re not going to have time to cover the adversarial example,</p>
    <p class="cue"><span class="time">[99:36]</span>but we&#x27;re going to cover it in two weeks together.</p>
    <p class="cue"><span class="time">[99:39]</span>You&#x27;re going to have more neural network baggage.</p>
  </section>
</article>
</body>
</html>
